{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 0. Mounting the GoogleDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5Rhv5Rk8Ttc",
        "outputId": "21a384ad-28b1-469a-df9f-dde83cccbcad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Installing Open Grounding Dino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Izpgm_dp8r73"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/Open-GroundingDino.zip -d /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE-j-mriN_qo",
        "outputId": "3283a6a1-c618-46b9-90b9-00543192a6cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'Open-GroundingDino'...\n",
            "remote: Enumerating objects: 181, done.\u001b[K\n",
            "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
            "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
            "remote: Total 181 (delta 41), reused 28 (delta 28), pack-reused 101\u001b[K\n",
            "Receiving objects: 100% (181/181), 8.72 MiB | 12.07 MiB/s, done.\n",
            "Resolving deltas: 100% (66/66), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone \"https://github.com/longzw1997/Open-GroundingDino\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed2ivbNnOxyr",
        "outputId": "22545e4a-d56c-4211-ee70-3c95675ac2b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Open-GroundingDino\n"
          ]
        }
      ],
      "source": [
        "%cd Open-GroundingDino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VvrAKrL0O0iT",
        "outputId": "fbed7219-5e25-49e4-ef2c-986154165f21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully installed addict-2.4.0 colorlog-6.8.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 submitit-1.5.1 supervision-0.6.0 timm-1.0.3 yapf-0.40.1\n"
          ]
        }
      ],
      "source": [
        "#install requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P24TKBIgO4-s",
        "outputId": "3bd418e2-b9c5-4f46-aa73-e9f12fdb3b9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Open-GroundingDino/models/GroundingDINO/ops\n"
          ]
        }
      ],
      "source": [
        "%cd models/GroundingDINO/ops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b93PRIJAPItA"
      },
      "outputs": [],
      "source": [
        "!python setup.py install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd4hT8OkPIcx",
        "outputId": "feb28e7f-5173-4098-bd87-f4a0f4f3e545"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* True check_forward_equal_with_pytorch_double: max_abs_err 8.67e-19 max_rel_err 2.35e-16\n",
            "* True check_forward_equal_with_pytorch_float: max_abs_err 4.66e-10 max_rel_err 1.13e-07\n",
            "* True check_gradient_numerical(D=30)\n",
            "* True check_gradient_numerical(D=32)\n",
            "* True check_gradient_numerical(D=64)\n",
            "* True check_gradient_numerical(D=71)\n"
          ]
        }
      ],
      "source": [
        "!python test.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnK7VxEmVoN-",
        "outputId": "ca684cbb-ccb1-455d-f14a-130d9ff28a54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Converting training data from  COCO to odvg, Val data remains in COCO format only"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- modifying `line 11` and `line 16` of `Open-GroundingDino/tools/coco2odvg.py` according to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#chenge path of input file to your input Coco json file\n",
        "!python /content/Open-GroundingDino/tools/coco2odvg.py --input \"/content/Yolo-to-COCO-format-converter/output/jun13_200ing.json\"  --output \"output.jsonl\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Make a folder with contents\n",
        "- Folder name `anno` with 'label.json' and `output.jsonl`\n",
        "- Folder name `train` with all train images\n",
        "- Folder name `val` with all val images\n",
        "- file name `val.json` with annotations in coco format\n",
        "- zip the folder and upload it to drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cnnx2uQXPIYC"
      },
      "outputs": [],
      "source": [
        "#unzip the uploaded folder in colab\n",
        "!unzip /content/drive/MyDrive/open_img.zip -d /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTNyQGqmVdSp",
        "outputId": "d5169c54-7380-4b67-865d-0dc7c9563a28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data has been written to /content/Open-GroundingDino/config/datasets_mixed_odvg.json\n"
          ]
        }
      ],
      "source": [
        "#change the paths according to unziped folder\n",
        "import json\n",
        "\n",
        "# Define the data\n",
        "data = {\n",
        "    \"train\": [\n",
        "        {\n",
        "            \"root\": \"/content/open_img/train/\",\n",
        "            \"anno\": \"/content/open_img/anno/out.jsonl\",\n",
        "            \"label_map\": \"/content/open_img/anno/label.json\",\n",
        "            \"dataset_mode\": \"odvg\"\n",
        "        }\n",
        "    ],\n",
        "    \"val\": [\n",
        "        {\n",
        "            \"root\": \"/content/open_img/val\",\n",
        "            \"anno\": \"/content/open_img/val.json\",\n",
        "            \"label_map\": None,\n",
        "            \"dataset_mode\": \"coco\"\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "file_path = '/content/Open-GroundingDino/config/datasets_mixed_odvg.json'\n",
        "\n",
        "with open(file_path, 'w') as file:\n",
        "    json.dump(data, file, indent=2)\n",
        "\n",
        "print(f\"Data has been written to {file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Modify some file according you dataset\n",
        "-  modify `Open-GroundingDino/config/cfg_coco.py` and `Open-GroundingDino/config/cfg_odvg.py` with your own label list\n",
        "\n",
        "```python\n",
        "use_coco_eval = False\n",
        "label_list=[\"bolt\",\"wrong direction\",\"1\",\"2\",\"3\",\"4\",\"5\"] #use your own labels\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1oSfABZZmxn"
      },
      "outputs": [],
      "source": [
        "#make a output directory to store the checkpoints of trained model\n",
        "import os\n",
        "os.makedirs(\"/content/output\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 4. Download `groundingdino_swint_ogc.pth` and `bert`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth&ved=2ahUKEwjLqsnAstiGAxVPbGwGHbcOADAQFnoECBYQAQ&usg=AOvVaw1HJopClO4_-MXLi9Ae6-le"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.makedirs(\"/content/bert\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content/bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(tokenizer.save_pretrained(\".\"))\n",
        "print(model.save_pretrained(\".\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5. Replace whole code of `train_dist.sh` with code below to run on single gpu\n",
        "- Add path of groundingdino_swint_ogc.pth and bert folder\n",
        "```python\n",
        "CFG=$1\n",
        "DATASETS=$2\n",
        "OUTPUT_DIR=$3\n",
        "\n",
        "# Set the environment variable for CUDA\n",
        "export CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "python main.py \\\n",
        "    --config_file ${CFG} \\\n",
        "    --datasets ${DATASETS} \\\n",
        "    --output_dir ${OUTPUT_DIR} \\\n",
        "    --pretrain_model_path /path/to/groundingdino_swint_ogc.pth \\ \n",
        "    --options text_encoder_type=/path/to/bert-base-uncased\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uajrUI73XzeH",
        "outputId": "e3b06e29-1b93-4e40-bc16-0e74b4b5e82e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not using distributed mode\n",
            "Loading config file from /content/Open-GroundingDino/config/cfg_odvg.py\n",
            "\u001b[32mINFO    \u001b[0m \u001b[32m2024-06-12 05:47:40,352 | \u001b[34mgit:\n",
            "  sha: a1f9128db6f6fee00c0552aab0a1d381d834dbe3, status: has uncommited changes, branch: main\n",
            "\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[32m2024-06-12 05:47:40,352 | \u001b[34mCommand: main.py --config_file /content/Open-GroundingDino/config/cfg_odvg.py --datasets /content/Open-GroundingDino/config/datasets_mixed_odvg.json --output_dir /content/output --pretrain_model_path /content/drive/MyDrive/groundingdino_swint_ogc(1).pth --options text_encoder_type=/content/bert\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[32m2024-06-12 05:47:40,353 | \u001b[34mFull config saved to /content/output/config_args_all.json\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[32m2024-06-12 05:47:40,353 | \u001b[34mworld size: 1\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[32m2024-06-12 05:47:40,353 | \u001b[34mrank: 0\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[32m2024-06-12 05:47:40,353 | \u001b[34mlocal_rank: 0\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[32m2024-06-12 05:47:40,353 | \u001b[34margs: Namespace(config_file='/content/Open-GroundingDino/config/cfg_odvg.py', options={'text_encoder_type': '/content/bert'}, datasets='/content/Open-GroundingDino/config/datasets_mixed_odvg.json', remove_difficult=False, fix_size=False, output_dir='/content/output', note='', device='cuda', seed=42, resume='', pretrain_model_path='/content/drive/MyDrive/groundingdino_swint_ogc(1).pth', finetune_ignore=None, start_epoch=0, eval=False, num_workers=8, test=False, debug=False, find_unused_params=False, save_results=False, save_log=False, world_size=1, dist_url='env://', rank=0, local_rank=0, amp=False, distributed=False, data_aug_scales=[480, 512, 544, 576, 608, 640, 672, 704, 736, 768, 800], data_aug_max_size=1333, data_aug_scales2_resize=[400, 500, 600], data_aug_scales2_crop=[384, 600], data_aug_scale_overlap=None, batch_size=4, modelname='groundingdino', backbone='swin_T_224_1k', position_embedding='sine', pe_temperatureH=20, pe_temperatureW=20, return_interm_indices=[1, 2, 3], enc_layers=6, dec_layers=6, pre_norm=False, dim_feedforward=2048, hidden_dim=256, dropout=0.0, nheads=8, num_queries=900, query_dim=4, num_patterns=0, num_feature_levels=4, enc_n_points=4, dec_n_points=4, two_stage_type='standard', two_stage_bbox_embed_share=False, two_stage_class_embed_share=False, transformer_activation='relu', dec_pred_bbox_embed_share=True, dn_box_noise_scale=1.0, dn_label_noise_ratio=0.5, dn_label_coef=1.0, dn_bbox_coef=1.0, embed_init_tgt=True, dn_labelbook_size=91, max_text_len=256, text_encoder_type='/content/bert', use_text_enhancer=True, use_fusion_layer=True, use_checkpoint=True, use_transformer_ckpt=True, use_text_cross_attention=True, text_dropout=0.0, fusion_dropout=0.0, fusion_droppath=0.1, sub_sentence_present=True, max_labels=50, lr=0.0001, backbone_freeze_keywords=None, freeze_keywords=['bert'], lr_backbone=1e-05, lr_backbone_names=['backbone.0', 'bert'], lr_linear_proj_mult=1e-05, lr_linear_proj_names=['ref_point_head', 'sampling_offsets'], weight_decay=0.0001, param_dict_type='ddetr_in_mmdet', ddetr_lr_param=False, epochs=15, lr_drop=4, save_checkpoint_interval=1, clip_max_norm=0.1, onecyclelr=False, multi_step_lr=False, lr_drop_list=[4, 8], frozen_weights=None, dilation=False, pdetr3_bbox_embed_diff_each_layer=False, pdetr3_refHW=-1, random_refpoints_xy=False, fix_refpoints_hw=-1, dabdetr_yolo_like_anchor_update=False, dabdetr_deformable_encoder=False, dabdetr_deformable_decoder=False, use_deformable_box_attn=False, box_attn_type='roi_align', dec_layer_number=None, decoder_layer_noise=False, dln_xy_noise=0.2, dln_hw_noise=0.2, add_channel_attention=False, add_pos_value=False, two_stage_pat_embed=0, two_stage_add_query_num=0, two_stage_learn_wh=False, two_stage_default_hw=0.05, two_stage_keep_all_tokens=False, num_select=300, batch_norm_type='FrozenBatchNorm2d', masks=False, aux_loss=True, set_cost_class=1.0, set_cost_bbox=5.0, set_cost_giou=2.0, cls_loss_coef=2.0, bbox_loss_coef=5.0, giou_loss_coef=2.0, enc_loss_coef=1.0, interm_loss_coef=1.0, no_interm_box_loss=False, mask_loss_coef=1.0, dice_loss_coef=1.0, focal_alpha=0.25, focal_gamma=2.0, decoder_sa_type='sa', matcher_type='HungarianMatcher', decoder_module_seq=['sa', 'ca', 'ffn'], nms_iou_threshold=-1, dec_pred_class_embed_share=True, match_unstable_error=True, use_ema=False, ema_decay=0.9997, ema_epoch=0, use_detached_boxes_dec_out=False, use_coco_eval=False, label_list=['bolt', 'wrong direction', '1', '2', '3', '4', '5'], dn_scalar=100)\n",
            "\u001b[0m\n",
            "\u001b[36mDEBUG   \u001b[0m \u001b[36m2024-06-12 05:47:40,357 | \u001b[34mbuild model ... ...\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "final text_encoder_type: /content/bert\n",
            "load tokenizer done.\n",
            "final text_encoder_type: /content/bert\n",
            "load tokenizer done.\n",
            "\u001b[36mDEBUG   \u001b[0m \u001b[36m2024-06-12 05:47:42,849 | \u001b[34mbuild model, done.\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[32m2024-06-12 05:47:42,852 | \u001b[34mnumber of params:172249090\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[32m2024-06-12 05:47:42,856 | \u001b[34mparams before freezing:\n",
            "{\n",
            "  \"transformer.level_embed\": 1024,\n",
            "  \"transformer.encoder.layers.0.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.0.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.0.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.0.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.0.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.0.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.0.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.0.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.0.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.0.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.0.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.1.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.1.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.1.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.1.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.1.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.1.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.1.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.1.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.1.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.1.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.2.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.2.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.2.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.2.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.2.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.2.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.2.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.2.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.2.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.2.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.3.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.3.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.3.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.3.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.3.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.3.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.3.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.3.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.3.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.3.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.4.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.4.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.4.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.4.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.4.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.4.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.4.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.4.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.4.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.4.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.5.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.5.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.5.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.5.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.5.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.5.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.5.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.5.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.5.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.5.norm2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.0.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.encoder.text_layers.0.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.encoder.text_layers.0.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.encoder.text_layers.0.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.0.linear1.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.0.linear1.bias\": 1024,\n",
            "  \"transformer.encoder.text_layers.0.linear2.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.0.linear2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.0.norm1.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.0.norm1.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.0.norm2.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.0.norm2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.1.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.encoder.text_layers.1.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.encoder.text_layers.1.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.encoder.text_layers.1.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.1.linear1.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.1.linear1.bias\": 1024,\n",
            "  \"transformer.encoder.text_layers.1.linear2.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.1.linear2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.1.norm1.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.1.norm1.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.1.norm2.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.1.norm2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.2.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.encoder.text_layers.2.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.encoder.text_layers.2.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.encoder.text_layers.2.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.2.linear1.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.2.linear1.bias\": 1024,\n",
            "  \"transformer.encoder.text_layers.2.linear2.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.2.linear2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.2.norm1.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.2.norm1.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.2.norm2.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.2.norm2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.3.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.encoder.text_layers.3.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.encoder.text_layers.3.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.encoder.text_layers.3.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.3.linear1.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.3.linear1.bias\": 1024,\n",
            "  \"transformer.encoder.text_layers.3.linear2.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.3.linear2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.3.norm1.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.3.norm1.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.3.norm2.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.3.norm2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.4.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.encoder.text_layers.4.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.encoder.text_layers.4.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.encoder.text_layers.4.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.4.linear1.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.4.linear1.bias\": 1024,\n",
            "  \"transformer.encoder.text_layers.4.linear2.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.4.linear2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.4.norm1.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.4.norm1.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.4.norm2.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.4.norm2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.5.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.encoder.text_layers.5.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.encoder.text_layers.5.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.encoder.text_layers.5.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.5.linear1.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.5.linear1.bias\": 1024,\n",
            "  \"transformer.encoder.text_layers.5.linear2.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.5.linear2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.5.norm1.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.5.norm1.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.5.norm2.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.5.norm2.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.gamma_v\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.gamma_l\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.layer_norm_v.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.layer_norm_v.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.layer_norm_l.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.layer_norm_l.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.values_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.values_v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.values_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.values_l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.out_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.out_v_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.out_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.out_l_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.gamma_v\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.gamma_l\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.layer_norm_v.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.layer_norm_v.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.layer_norm_l.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.layer_norm_l.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.values_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.values_v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.values_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.values_l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.out_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.out_v_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.out_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.out_l_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.gamma_v\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.gamma_l\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.layer_norm_v.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.layer_norm_v.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.layer_norm_l.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.layer_norm_l.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.values_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.values_v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.values_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.values_l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.out_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.out_v_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.out_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.out_l_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.gamma_v\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.gamma_l\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.layer_norm_v.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.layer_norm_v.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.layer_norm_l.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.layer_norm_l.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.values_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.values_v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.values_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.values_l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.out_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.out_v_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.out_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.out_l_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.gamma_v\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.gamma_l\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.layer_norm_v.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.layer_norm_v.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.layer_norm_l.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.layer_norm_l.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.values_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.values_v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.values_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.values_l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.out_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.out_v_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.out_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.out_l_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.gamma_v\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.gamma_l\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.layer_norm_v.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.layer_norm_v.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.layer_norm_l.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.layer_norm_l.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.values_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.values_v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.values_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.values_l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.out_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.out_v_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.out_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.out_l_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.0.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.0.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.0.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.ca_text.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.0.ca_text.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.0.ca_text.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.ca_text.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.catext_norm.weight\": 256,\n",
            "  \"transformer.decoder.layers.0.catext_norm.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.0.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.0.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.0.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.0.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.0.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.0.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.0.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.1.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.1.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.1.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.ca_text.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.1.ca_text.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.1.ca_text.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.ca_text.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.catext_norm.weight\": 256,\n",
            "  \"transformer.decoder.layers.1.catext_norm.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.1.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.1.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.1.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.1.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.1.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.1.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.1.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.2.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.2.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.2.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.ca_text.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.2.ca_text.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.2.ca_text.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.ca_text.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.catext_norm.weight\": 256,\n",
            "  \"transformer.decoder.layers.2.catext_norm.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.2.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.2.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.2.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.2.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.2.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.2.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.2.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.3.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.3.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.3.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.ca_text.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.3.ca_text.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.3.ca_text.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.ca_text.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.catext_norm.weight\": 256,\n",
            "  \"transformer.decoder.layers.3.catext_norm.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.3.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.3.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.3.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.3.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.3.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.3.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.3.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.4.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.4.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.4.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.ca_text.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.4.ca_text.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.4.ca_text.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.ca_text.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.catext_norm.weight\": 256,\n",
            "  \"transformer.decoder.layers.4.catext_norm.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.4.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.4.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.4.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.4.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.4.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.4.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.4.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.5.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.5.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.5.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.ca_text.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.5.ca_text.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.5.ca_text.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.ca_text.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.catext_norm.weight\": 256,\n",
            "  \"transformer.decoder.layers.5.catext_norm.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.5.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.5.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.5.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.5.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.5.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.5.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.5.norm3.bias\": 256,\n",
            "  \"transformer.decoder.norm.weight\": 256,\n",
            "  \"transformer.decoder.norm.bias\": 256,\n",
            "  \"transformer.decoder.ref_point_head.layers.0.weight\": 131072,\n",
            "  \"transformer.decoder.ref_point_head.layers.0.bias\": 256,\n",
            "  \"transformer.decoder.ref_point_head.layers.1.weight\": 65536,\n",
            "  \"transformer.decoder.ref_point_head.layers.1.bias\": 256,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.0.weight\": 65536,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.0.bias\": 256,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.1.weight\": 65536,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.1.bias\": 256,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.2.weight\": 1024,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.2.bias\": 4,\n",
            "  \"transformer.tgt_embed.weight\": 230400,\n",
            "  \"transformer.enc_output.weight\": 65536,\n",
            "  \"transformer.enc_output.bias\": 256,\n",
            "  \"transformer.enc_output_norm.weight\": 256,\n",
            "  \"transformer.enc_output_norm.bias\": 256,\n",
            "  \"transformer.enc_out_bbox_embed.layers.0.weight\": 65536,\n",
            "  \"transformer.enc_out_bbox_embed.layers.0.bias\": 256,\n",
            "  \"transformer.enc_out_bbox_embed.layers.1.weight\": 65536,\n",
            "  \"transformer.enc_out_bbox_embed.layers.1.bias\": 256,\n",
            "  \"transformer.enc_out_bbox_embed.layers.2.weight\": 1024,\n",
            "  \"transformer.enc_out_bbox_embed.layers.2.bias\": 4,\n",
            "  \"bert.embeddings.word_embeddings.weight\": 23440896,\n",
            "  \"bert.embeddings.position_embeddings.weight\": 393216,\n",
            "  \"bert.embeddings.token_type_embeddings.weight\": 1536,\n",
            "  \"bert.embeddings.LayerNorm.weight\": 768,\n",
            "  \"bert.embeddings.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.0.attention.self.query.weight\": 589824,\n",
            "  \"bert.encoder.layer.0.attention.self.query.bias\": 768,\n",
            "  \"bert.encoder.layer.0.attention.self.key.weight\": 589824,\n",
            "  \"bert.encoder.layer.0.attention.self.key.bias\": 768,\n",
            "  \"bert.encoder.layer.0.attention.self.value.weight\": 589824,\n",
            "  \"bert.encoder.layer.0.attention.self.value.bias\": 768,\n",
            "  \"bert.encoder.layer.0.attention.output.dense.weight\": 589824,\n",
            "  \"bert.encoder.layer.0.attention.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.0.attention.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.0.attention.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.0.intermediate.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.0.intermediate.dense.bias\": 3072,\n",
            "  \"bert.encoder.layer.0.output.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.0.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.0.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.0.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.1.attention.self.query.weight\": 589824,\n",
            "  \"bert.encoder.layer.1.attention.self.query.bias\": 768,\n",
            "  \"bert.encoder.layer.1.attention.self.key.weight\": 589824,\n",
            "  \"bert.encoder.layer.1.attention.self.key.bias\": 768,\n",
            "  \"bert.encoder.layer.1.attention.self.value.weight\": 589824,\n",
            "  \"bert.encoder.layer.1.attention.self.value.bias\": 768,\n",
            "  \"bert.encoder.layer.1.attention.output.dense.weight\": 589824,\n",
            "  \"bert.encoder.layer.1.attention.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.1.attention.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.1.attention.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.1.intermediate.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.1.intermediate.dense.bias\": 3072,\n",
            "  \"bert.encoder.layer.1.output.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.1.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.1.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.1.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.2.attention.self.query.weight\": 589824,\n",
            "  \"bert.encoder.layer.2.attention.self.query.bias\": 768,\n",
            "  \"bert.encoder.layer.2.attention.self.key.weight\": 589824,\n",
            "  \"bert.encoder.layer.2.attention.self.key.bias\": 768,\n",
            "  \"bert.encoder.layer.2.attention.self.value.weight\": 589824,\n",
            "  \"bert.encoder.layer.2.attention.self.value.bias\": 768,\n",
            "  \"bert.encoder.layer.2.attention.output.dense.weight\": 589824,\n",
            "  \"bert.encoder.layer.2.attention.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.2.attention.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.2.attention.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.2.intermediate.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.2.intermediate.dense.bias\": 3072,\n",
            "  \"bert.encoder.layer.2.output.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.2.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.2.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.2.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.3.attention.self.query.weight\": 589824,\n",
            "  \"bert.encoder.layer.3.attention.self.query.bias\": 768,\n",
            "  \"bert.encoder.layer.3.attention.self.key.weight\": 589824,\n",
            "  \"bert.encoder.layer.3.attention.self.key.bias\": 768,\n",
            "  \"bert.encoder.layer.3.attention.self.value.weight\": 589824,\n",
            "  \"bert.encoder.layer.3.attention.self.value.bias\": 768,\n",
            "  \"bert.encoder.layer.3.attention.output.dense.weight\": 589824,\n",
            "  \"bert.encoder.layer.3.attention.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.3.attention.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.3.attention.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.3.intermediate.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.3.intermediate.dense.bias\": 3072,\n",
            "  \"bert.encoder.layer.3.output.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.3.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.3.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.3.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.4.attention.self.query.weight\": 589824,\n",
            "  \"bert.encoder.layer.4.attention.self.query.bias\": 768,\n",
            "  \"bert.encoder.layer.4.attention.self.key.weight\": 589824,\n",
            "  \"bert.encoder.layer.4.attention.self.key.bias\": 768,\n",
            "  \"bert.encoder.layer.4.attention.self.value.weight\": 589824,\n",
            "  \"bert.encoder.layer.4.attention.self.value.bias\": 768,\n",
            "  \"bert.encoder.layer.4.attention.output.dense.weight\": 589824,\n",
            "  \"bert.encoder.layer.4.attention.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.4.attention.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.4.attention.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.4.intermediate.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.4.intermediate.dense.bias\": 3072,\n",
            "  \"bert.encoder.layer.4.output.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.4.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.4.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.4.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.5.attention.self.query.weight\": 589824,\n",
            "  \"bert.encoder.layer.5.attention.self.query.bias\": 768,\n",
            "  \"bert.encoder.layer.5.attention.self.key.weight\": 589824,\n",
            "  \"bert.encoder.layer.5.attention.self.key.bias\": 768,\n",
            "  \"bert.encoder.layer.5.attention.self.value.weight\": 589824,\n",
            "  \"bert.encoder.layer.5.attention.self.value.bias\": 768,\n",
            "  \"bert.encoder.layer.5.attention.output.dense.weight\": 589824,\n",
            "  \"bert.encoder.layer.5.attention.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.5.attention.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.5.attention.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.5.intermediate.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.5.intermediate.dense.bias\": 3072,\n",
            "  \"bert.encoder.layer.5.output.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.5.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.5.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.5.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.6.attention.self.query.weight\": 589824,\n",
            "  \"bert.encoder.layer.6.attention.self.query.bias\": 768,\n",
            "  \"bert.encoder.layer.6.attention.self.key.weight\": 589824,\n",
            "  \"bert.encoder.layer.6.attention.self.key.bias\": 768,\n",
            "  \"bert.encoder.layer.6.attention.self.value.weight\": 589824,\n",
            "  \"bert.encoder.layer.6.attention.self.value.bias\": 768,\n",
            "  \"bert.encoder.layer.6.attention.output.dense.weight\": 589824,\n",
            "  \"bert.encoder.layer.6.attention.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.6.attention.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.6.attention.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.6.intermediate.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.6.intermediate.dense.bias\": 3072,\n",
            "  \"bert.encoder.layer.6.output.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.6.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.6.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.6.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.7.attention.self.query.weight\": 589824,\n",
            "  \"bert.encoder.layer.7.attention.self.query.bias\": 768,\n",
            "  \"bert.encoder.layer.7.attention.self.key.weight\": 589824,\n",
            "  \"bert.encoder.layer.7.attention.self.key.bias\": 768,\n",
            "  \"bert.encoder.layer.7.attention.self.value.weight\": 589824,\n",
            "  \"bert.encoder.layer.7.attention.self.value.bias\": 768,\n",
            "  \"bert.encoder.layer.7.attention.output.dense.weight\": 589824,\n",
            "  \"bert.encoder.layer.7.attention.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.7.attention.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.7.attention.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.7.intermediate.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.7.intermediate.dense.bias\": 3072,\n",
            "  \"bert.encoder.layer.7.output.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.7.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.7.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.7.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.8.attention.self.query.weight\": 589824,\n",
            "  \"bert.encoder.layer.8.attention.self.query.bias\": 768,\n",
            "  \"bert.encoder.layer.8.attention.self.key.weight\": 589824,\n",
            "  \"bert.encoder.layer.8.attention.self.key.bias\": 768,\n",
            "  \"bert.encoder.layer.8.attention.self.value.weight\": 589824,\n",
            "  \"bert.encoder.layer.8.attention.self.value.bias\": 768,\n",
            "  \"bert.encoder.layer.8.attention.output.dense.weight\": 589824,\n",
            "  \"bert.encoder.layer.8.attention.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.8.attention.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.8.attention.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.8.intermediate.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.8.intermediate.dense.bias\": 3072,\n",
            "  \"bert.encoder.layer.8.output.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.8.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.8.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.8.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.9.attention.self.query.weight\": 589824,\n",
            "  \"bert.encoder.layer.9.attention.self.query.bias\": 768,\n",
            "  \"bert.encoder.layer.9.attention.self.key.weight\": 589824,\n",
            "  \"bert.encoder.layer.9.attention.self.key.bias\": 768,\n",
            "  \"bert.encoder.layer.9.attention.self.value.weight\": 589824,\n",
            "  \"bert.encoder.layer.9.attention.self.value.bias\": 768,\n",
            "  \"bert.encoder.layer.9.attention.output.dense.weight\": 589824,\n",
            "  \"bert.encoder.layer.9.attention.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.9.attention.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.9.attention.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.9.intermediate.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.9.intermediate.dense.bias\": 3072,\n",
            "  \"bert.encoder.layer.9.output.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.9.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.9.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.9.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.10.attention.self.query.weight\": 589824,\n",
            "  \"bert.encoder.layer.10.attention.self.query.bias\": 768,\n",
            "  \"bert.encoder.layer.10.attention.self.key.weight\": 589824,\n",
            "  \"bert.encoder.layer.10.attention.self.key.bias\": 768,\n",
            "  \"bert.encoder.layer.10.attention.self.value.weight\": 589824,\n",
            "  \"bert.encoder.layer.10.attention.self.value.bias\": 768,\n",
            "  \"bert.encoder.layer.10.attention.output.dense.weight\": 589824,\n",
            "  \"bert.encoder.layer.10.attention.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.10.attention.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.10.attention.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.10.intermediate.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.10.intermediate.dense.bias\": 3072,\n",
            "  \"bert.encoder.layer.10.output.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.10.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.10.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.10.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.11.attention.self.query.weight\": 589824,\n",
            "  \"bert.encoder.layer.11.attention.self.query.bias\": 768,\n",
            "  \"bert.encoder.layer.11.attention.self.key.weight\": 589824,\n",
            "  \"bert.encoder.layer.11.attention.self.key.bias\": 768,\n",
            "  \"bert.encoder.layer.11.attention.self.value.weight\": 589824,\n",
            "  \"bert.encoder.layer.11.attention.self.value.bias\": 768,\n",
            "  \"bert.encoder.layer.11.attention.output.dense.weight\": 589824,\n",
            "  \"bert.encoder.layer.11.attention.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.11.attention.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.11.attention.output.LayerNorm.bias\": 768,\n",
            "  \"bert.encoder.layer.11.intermediate.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.11.intermediate.dense.bias\": 3072,\n",
            "  \"bert.encoder.layer.11.output.dense.weight\": 2359296,\n",
            "  \"bert.encoder.layer.11.output.dense.bias\": 768,\n",
            "  \"bert.encoder.layer.11.output.LayerNorm.weight\": 768,\n",
            "  \"bert.encoder.layer.11.output.LayerNorm.bias\": 768,\n",
            "  \"feat_map.weight\": 196608,\n",
            "  \"feat_map.bias\": 256,\n",
            "  \"input_proj.0.0.weight\": 49152,\n",
            "  \"input_proj.0.0.bias\": 256,\n",
            "  \"input_proj.0.1.weight\": 256,\n",
            "  \"input_proj.0.1.bias\": 256,\n",
            "  \"input_proj.1.0.weight\": 98304,\n",
            "  \"input_proj.1.0.bias\": 256,\n",
            "  \"input_proj.1.1.weight\": 256,\n",
            "  \"input_proj.1.1.bias\": 256,\n",
            "  \"input_proj.2.0.weight\": 196608,\n",
            "  \"input_proj.2.0.bias\": 256,\n",
            "  \"input_proj.2.1.weight\": 256,\n",
            "  \"input_proj.2.1.bias\": 256,\n",
            "  \"input_proj.3.0.weight\": 1769472,\n",
            "  \"input_proj.3.0.bias\": 256,\n",
            "  \"input_proj.3.1.weight\": 256,\n",
            "  \"input_proj.3.1.bias\": 256,\n",
            "  \"backbone.0.patch_embed.proj.weight\": 4608,\n",
            "  \"backbone.0.patch_embed.proj.bias\": 96,\n",
            "  \"backbone.0.patch_embed.norm.weight\": 96,\n",
            "  \"backbone.0.patch_embed.norm.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.0.norm1.weight\": 96,\n",
            "  \"backbone.0.layers.0.blocks.0.norm1.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.0.attn.relative_position_bias_table\": 507,\n",
            "  \"backbone.0.layers.0.blocks.0.attn.qkv.weight\": 27648,\n",
            "  \"backbone.0.layers.0.blocks.0.attn.qkv.bias\": 288,\n",
            "  \"backbone.0.layers.0.blocks.0.attn.proj.weight\": 9216,\n",
            "  \"backbone.0.layers.0.blocks.0.attn.proj.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.0.norm2.weight\": 96,\n",
            "  \"backbone.0.layers.0.blocks.0.norm2.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.0.mlp.fc1.weight\": 36864,\n",
            "  \"backbone.0.layers.0.blocks.0.mlp.fc1.bias\": 384,\n",
            "  \"backbone.0.layers.0.blocks.0.mlp.fc2.weight\": 36864,\n",
            "  \"backbone.0.layers.0.blocks.0.mlp.fc2.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.1.norm1.weight\": 96,\n",
            "  \"backbone.0.layers.0.blocks.1.norm1.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.1.attn.relative_position_bias_table\": 507,\n",
            "  \"backbone.0.layers.0.blocks.1.attn.qkv.weight\": 27648,\n",
            "  \"backbone.0.layers.0.blocks.1.attn.qkv.bias\": 288,\n",
            "  \"backbone.0.layers.0.blocks.1.attn.proj.weight\": 9216,\n",
            "  \"backbone.0.layers.0.blocks.1.attn.proj.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.1.norm2.weight\": 96,\n",
            "  \"backbone.0.layers.0.blocks.1.norm2.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.1.mlp.fc1.weight\": 36864,\n",
            "  \"backbone.0.layers.0.blocks.1.mlp.fc1.bias\": 384,\n",
            "  \"backbone.0.layers.0.blocks.1.mlp.fc2.weight\": 36864,\n",
            "  \"backbone.0.layers.0.blocks.1.mlp.fc2.bias\": 96,\n",
            "  \"backbone.0.layers.0.downsample.reduction.weight\": 73728,\n",
            "  \"backbone.0.layers.0.downsample.norm.weight\": 384,\n",
            "  \"backbone.0.layers.0.downsample.norm.bias\": 384,\n",
            "  \"backbone.0.layers.1.blocks.0.norm1.weight\": 192,\n",
            "  \"backbone.0.layers.1.blocks.0.norm1.bias\": 192,\n",
            "  \"backbone.0.layers.1.blocks.0.attn.relative_position_bias_table\": 1014,\n",
            "  \"backbone.0.layers.1.blocks.0.attn.qkv.weight\": 110592,\n",
            "  \"backbone.0.layers.1.blocks.0.attn.qkv.bias\": 576,\n",
            "  \"backbone.0.layers.1.blocks.0.attn.proj.weight\": 36864,\n",
            "  \"backbone.0.layers.1.blocks.0.attn.proj.bias\": 192,\n",
            "  \"backbone.0.layers.1.blocks.0.norm2.weight\": 192,\n",
            "  \"backbone.0.layers.1.blocks.0.norm2.bias\": 192,\n",
            "  \"backbone.0.layers.1.blocks.0.mlp.fc1.weight\": 147456,\n",
            "  \"backbone.0.layers.1.blocks.0.mlp.fc1.bias\": 768,\n",
            "  \"backbone.0.layers.1.blocks.0.mlp.fc2.weight\": 147456,\n",
            "  \"backbone.0.layers.1.blocks.0.mlp.fc2.bias\": 192,\n",
            "  \"backbone.0.layers.1.blocks.1.norm1.weight\": 192,\n",
            "  \"backbone.0.layers.1.blocks.1.norm1.bias\": 192,\n",
            "  \"backbone.0.layers.1.blocks.1.attn.relative_position_bias_table\": 1014,\n",
            "  \"backbone.0.layers.1.blocks.1.attn.qkv.weight\": 110592,\n",
            "  \"backbone.0.layers.1.blocks.1.attn.qkv.bias\": 576,\n",
            "  \"backbone.0.layers.1.blocks.1.attn.proj.weight\": 36864,\n",
            "  \"backbone.0.layers.1.blocks.1.attn.proj.bias\": 192,\n",
            "  \"backbone.0.layers.1.blocks.1.norm2.weight\": 192,\n",
            "  \"backbone.0.layers.1.blocks.1.norm2.bias\": 192,\n",
            "  \"backbone.0.layers.1.blocks.1.mlp.fc1.weight\": 147456,\n",
            "  \"backbone.0.layers.1.blocks.1.mlp.fc1.bias\": 768,\n",
            "  \"backbone.0.layers.1.blocks.1.mlp.fc2.weight\": 147456,\n",
            "  \"backbone.0.layers.1.blocks.1.mlp.fc2.bias\": 192,\n",
            "  \"backbone.0.layers.1.downsample.reduction.weight\": 294912,\n",
            "  \"backbone.0.layers.1.downsample.norm.weight\": 768,\n",
            "  \"backbone.0.layers.1.downsample.norm.bias\": 768,\n",
            "  \"backbone.0.layers.2.blocks.0.norm1.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.0.norm1.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.0.attn.relative_position_bias_table\": 2028,\n",
            "  \"backbone.0.layers.2.blocks.0.attn.qkv.weight\": 442368,\n",
            "  \"backbone.0.layers.2.blocks.0.attn.qkv.bias\": 1152,\n",
            "  \"backbone.0.layers.2.blocks.0.attn.proj.weight\": 147456,\n",
            "  \"backbone.0.layers.2.blocks.0.attn.proj.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.0.norm2.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.0.norm2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.0.mlp.fc1.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.0.mlp.fc1.bias\": 1536,\n",
            "  \"backbone.0.layers.2.blocks.0.mlp.fc2.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.0.mlp.fc2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.1.norm1.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.1.norm1.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.1.attn.relative_position_bias_table\": 2028,\n",
            "  \"backbone.0.layers.2.blocks.1.attn.qkv.weight\": 442368,\n",
            "  \"backbone.0.layers.2.blocks.1.attn.qkv.bias\": 1152,\n",
            "  \"backbone.0.layers.2.blocks.1.attn.proj.weight\": 147456,\n",
            "  \"backbone.0.layers.2.blocks.1.attn.proj.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.1.norm2.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.1.norm2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.1.mlp.fc1.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.1.mlp.fc1.bias\": 1536,\n",
            "  \"backbone.0.layers.2.blocks.1.mlp.fc2.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.1.mlp.fc2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.2.norm1.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.2.norm1.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.2.attn.relative_position_bias_table\": 2028,\n",
            "  \"backbone.0.layers.2.blocks.2.attn.qkv.weight\": 442368,\n",
            "  \"backbone.0.layers.2.blocks.2.attn.qkv.bias\": 1152,\n",
            "  \"backbone.0.layers.2.blocks.2.attn.proj.weight\": 147456,\n",
            "  \"backbone.0.layers.2.blocks.2.attn.proj.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.2.norm2.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.2.norm2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.2.mlp.fc1.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.2.mlp.fc1.bias\": 1536,\n",
            "  \"backbone.0.layers.2.blocks.2.mlp.fc2.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.2.mlp.fc2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.3.norm1.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.3.norm1.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.3.attn.relative_position_bias_table\": 2028,\n",
            "  \"backbone.0.layers.2.blocks.3.attn.qkv.weight\": 442368,\n",
            "  \"backbone.0.layers.2.blocks.3.attn.qkv.bias\": 1152,\n",
            "  \"backbone.0.layers.2.blocks.3.attn.proj.weight\": 147456,\n",
            "  \"backbone.0.layers.2.blocks.3.attn.proj.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.3.norm2.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.3.norm2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.3.mlp.fc1.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.3.mlp.fc1.bias\": 1536,\n",
            "  \"backbone.0.layers.2.blocks.3.mlp.fc2.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.3.mlp.fc2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.4.norm1.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.4.norm1.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.4.attn.relative_position_bias_table\": 2028,\n",
            "  \"backbone.0.layers.2.blocks.4.attn.qkv.weight\": 442368,\n",
            "  \"backbone.0.layers.2.blocks.4.attn.qkv.bias\": 1152,\n",
            "  \"backbone.0.layers.2.blocks.4.attn.proj.weight\": 147456,\n",
            "  \"backbone.0.layers.2.blocks.4.attn.proj.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.4.norm2.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.4.norm2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.4.mlp.fc1.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.4.mlp.fc1.bias\": 1536,\n",
            "  \"backbone.0.layers.2.blocks.4.mlp.fc2.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.4.mlp.fc2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.5.norm1.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.5.norm1.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.5.attn.relative_position_bias_table\": 2028,\n",
            "  \"backbone.0.layers.2.blocks.5.attn.qkv.weight\": 442368,\n",
            "  \"backbone.0.layers.2.blocks.5.attn.qkv.bias\": 1152,\n",
            "  \"backbone.0.layers.2.blocks.5.attn.proj.weight\": 147456,\n",
            "  \"backbone.0.layers.2.blocks.5.attn.proj.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.5.norm2.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.5.norm2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.5.mlp.fc1.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.5.mlp.fc1.bias\": 1536,\n",
            "  \"backbone.0.layers.2.blocks.5.mlp.fc2.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.5.mlp.fc2.bias\": 384,\n",
            "  \"backbone.0.layers.2.downsample.reduction.weight\": 1179648,\n",
            "  \"backbone.0.layers.2.downsample.norm.weight\": 1536,\n",
            "  \"backbone.0.layers.2.downsample.norm.bias\": 1536,\n",
            "  \"backbone.0.layers.3.blocks.0.norm1.weight\": 768,\n",
            "  \"backbone.0.layers.3.blocks.0.norm1.bias\": 768,\n",
            "  \"backbone.0.layers.3.blocks.0.attn.relative_position_bias_table\": 4056,\n",
            "  \"backbone.0.layers.3.blocks.0.attn.qkv.weight\": 1769472,\n",
            "  \"backbone.0.layers.3.blocks.0.attn.qkv.bias\": 2304,\n",
            "  \"backbone.0.layers.3.blocks.0.attn.proj.weight\": 589824,\n",
            "  \"backbone.0.layers.3.blocks.0.attn.proj.bias\": 768,\n",
            "  \"backbone.0.layers.3.blocks.0.norm2.weight\": 768,\n",
            "  \"backbone.0.layers.3.blocks.0.norm2.bias\": 768,\n",
            "  \"backbone.0.layers.3.blocks.0.mlp.fc1.weight\": 2359296,\n",
            "  \"backbone.0.layers.3.blocks.0.mlp.fc1.bias\": 3072,\n",
            "  \"backbone.0.layers.3.blocks.0.mlp.fc2.weight\": 2359296,\n",
            "  \"backbone.0.layers.3.blocks.0.mlp.fc2.bias\": 768,\n",
            "  \"backbone.0.layers.3.blocks.1.norm1.weight\": 768,\n",
            "  \"backbone.0.layers.3.blocks.1.norm1.bias\": 768,\n",
            "  \"backbone.0.layers.3.blocks.1.attn.relative_position_bias_table\": 4056,\n",
            "  \"backbone.0.layers.3.blocks.1.attn.qkv.weight\": 1769472,\n",
            "  \"backbone.0.layers.3.blocks.1.attn.qkv.bias\": 2304,\n",
            "  \"backbone.0.layers.3.blocks.1.attn.proj.weight\": 589824,\n",
            "  \"backbone.0.layers.3.blocks.1.attn.proj.bias\": 768,\n",
            "  \"backbone.0.layers.3.blocks.1.norm2.weight\": 768,\n",
            "  \"backbone.0.layers.3.blocks.1.norm2.bias\": 768,\n",
            "  \"backbone.0.layers.3.blocks.1.mlp.fc1.weight\": 2359296,\n",
            "  \"backbone.0.layers.3.blocks.1.mlp.fc1.bias\": 3072,\n",
            "  \"backbone.0.layers.3.blocks.1.mlp.fc2.weight\": 2359296,\n",
            "  \"backbone.0.layers.3.blocks.1.mlp.fc2.bias\": 768,\n",
            "  \"backbone.0.norm1.weight\": 192,\n",
            "  \"backbone.0.norm1.bias\": 192,\n",
            "  \"backbone.0.norm2.weight\": 384,\n",
            "  \"backbone.0.norm2.bias\": 384,\n",
            "  \"backbone.0.norm3.weight\": 768,\n",
            "  \"backbone.0.norm3.bias\": 768\n",
            "}\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[32m2024-06-12 05:47:42,948 | \u001b[34mparams after freezing:\n",
            "{\n",
            "  \"transformer.level_embed\": 1024,\n",
            "  \"transformer.encoder.layers.0.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.0.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.0.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.0.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.0.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.0.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.0.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.0.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.0.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.0.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.0.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.0.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.1.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.1.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.1.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.1.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.1.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.1.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.1.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.1.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.1.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.1.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.1.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.2.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.2.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.2.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.2.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.2.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.2.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.2.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.2.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.2.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.2.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.2.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.3.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.3.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.3.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.3.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.3.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.3.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.3.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.3.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.3.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.3.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.3.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.4.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.4.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.4.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.4.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.4.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.4.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.4.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.4.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.4.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.4.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.4.norm2.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.self_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.encoder.layers.5.self_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.self_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.encoder.layers.5.self_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.encoder.layers.5.self_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.5.self_attn.value_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.self_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.encoder.layers.5.self_attn.output_proj.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.norm1.weight\": 256,\n",
            "  \"transformer.encoder.layers.5.norm1.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.linear1.weight\": 524288,\n",
            "  \"transformer.encoder.layers.5.linear1.bias\": 2048,\n",
            "  \"transformer.encoder.layers.5.linear2.weight\": 524288,\n",
            "  \"transformer.encoder.layers.5.linear2.bias\": 256,\n",
            "  \"transformer.encoder.layers.5.norm2.weight\": 256,\n",
            "  \"transformer.encoder.layers.5.norm2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.0.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.encoder.text_layers.0.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.encoder.text_layers.0.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.encoder.text_layers.0.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.0.linear1.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.0.linear1.bias\": 1024,\n",
            "  \"transformer.encoder.text_layers.0.linear2.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.0.linear2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.0.norm1.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.0.norm1.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.0.norm2.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.0.norm2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.1.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.encoder.text_layers.1.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.encoder.text_layers.1.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.encoder.text_layers.1.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.1.linear1.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.1.linear1.bias\": 1024,\n",
            "  \"transformer.encoder.text_layers.1.linear2.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.1.linear2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.1.norm1.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.1.norm1.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.1.norm2.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.1.norm2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.2.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.encoder.text_layers.2.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.encoder.text_layers.2.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.encoder.text_layers.2.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.2.linear1.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.2.linear1.bias\": 1024,\n",
            "  \"transformer.encoder.text_layers.2.linear2.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.2.linear2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.2.norm1.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.2.norm1.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.2.norm2.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.2.norm2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.3.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.encoder.text_layers.3.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.encoder.text_layers.3.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.encoder.text_layers.3.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.3.linear1.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.3.linear1.bias\": 1024,\n",
            "  \"transformer.encoder.text_layers.3.linear2.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.3.linear2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.3.norm1.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.3.norm1.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.3.norm2.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.3.norm2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.4.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.encoder.text_layers.4.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.encoder.text_layers.4.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.encoder.text_layers.4.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.4.linear1.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.4.linear1.bias\": 1024,\n",
            "  \"transformer.encoder.text_layers.4.linear2.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.4.linear2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.4.norm1.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.4.norm1.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.4.norm2.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.4.norm2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.5.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.encoder.text_layers.5.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.encoder.text_layers.5.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.encoder.text_layers.5.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.5.linear1.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.5.linear1.bias\": 1024,\n",
            "  \"transformer.encoder.text_layers.5.linear2.weight\": 262144,\n",
            "  \"transformer.encoder.text_layers.5.linear2.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.5.norm1.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.5.norm1.bias\": 256,\n",
            "  \"transformer.encoder.text_layers.5.norm2.weight\": 256,\n",
            "  \"transformer.encoder.text_layers.5.norm2.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.gamma_v\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.gamma_l\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.layer_norm_v.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.layer_norm_v.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.layer_norm_l.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.layer_norm_l.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.values_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.values_v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.values_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.values_l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.out_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.out_v_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.out_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.0.attn.out_l_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.gamma_v\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.gamma_l\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.layer_norm_v.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.layer_norm_v.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.layer_norm_l.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.layer_norm_l.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.values_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.values_v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.values_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.values_l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.out_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.out_v_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.out_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.1.attn.out_l_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.gamma_v\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.gamma_l\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.layer_norm_v.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.layer_norm_v.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.layer_norm_l.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.layer_norm_l.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.values_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.values_v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.values_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.values_l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.out_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.out_v_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.out_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.2.attn.out_l_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.gamma_v\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.gamma_l\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.layer_norm_v.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.layer_norm_v.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.layer_norm_l.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.layer_norm_l.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.values_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.values_v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.values_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.values_l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.out_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.out_v_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.out_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.3.attn.out_l_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.gamma_v\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.gamma_l\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.layer_norm_v.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.layer_norm_v.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.layer_norm_l.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.layer_norm_l.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.values_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.values_v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.values_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.values_l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.out_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.out_v_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.out_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.4.attn.out_l_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.gamma_v\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.gamma_l\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.layer_norm_v.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.layer_norm_v.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.layer_norm_l.weight\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.layer_norm_l.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.values_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.values_v_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.values_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.values_l_proj.bias\": 1024,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.out_v_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.out_v_proj.bias\": 256,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.out_l_proj.weight\": 262144,\n",
            "  \"transformer.encoder.fusion_layers.5.attn.out_l_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.0.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.0.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.0.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.ca_text.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.0.ca_text.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.0.ca_text.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.ca_text.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.catext_norm.weight\": 256,\n",
            "  \"transformer.decoder.layers.0.catext_norm.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.0.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.0.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.0.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.0.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.0.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.0.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.0.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.0.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.0.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.1.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.1.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.1.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.ca_text.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.1.ca_text.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.1.ca_text.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.ca_text.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.catext_norm.weight\": 256,\n",
            "  \"transformer.decoder.layers.1.catext_norm.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.1.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.1.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.1.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.1.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.1.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.1.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.1.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.1.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.1.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.2.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.2.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.2.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.ca_text.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.2.ca_text.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.2.ca_text.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.ca_text.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.catext_norm.weight\": 256,\n",
            "  \"transformer.decoder.layers.2.catext_norm.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.2.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.2.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.2.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.2.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.2.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.2.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.2.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.2.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.2.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.3.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.3.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.3.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.ca_text.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.3.ca_text.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.3.ca_text.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.ca_text.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.catext_norm.weight\": 256,\n",
            "  \"transformer.decoder.layers.3.catext_norm.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.3.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.3.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.3.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.3.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.3.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.3.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.3.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.3.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.3.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.4.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.4.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.4.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.ca_text.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.4.ca_text.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.4.ca_text.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.ca_text.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.catext_norm.weight\": 256,\n",
            "  \"transformer.decoder.layers.4.catext_norm.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.4.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.4.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.4.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.4.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.4.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.4.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.4.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.4.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.4.norm3.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.cross_attn.sampling_offsets.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.cross_attn.attention_weights.weight\": 32768,\n",
            "  \"transformer.decoder.layers.5.cross_attn.attention_weights.bias\": 128,\n",
            "  \"transformer.decoder.layers.5.cross_attn.value_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.cross_attn.value_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.cross_attn.output_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.cross_attn.output_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.norm1.weight\": 256,\n",
            "  \"transformer.decoder.layers.5.norm1.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.ca_text.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.5.ca_text.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.5.ca_text.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.ca_text.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.catext_norm.weight\": 256,\n",
            "  \"transformer.decoder.layers.5.catext_norm.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.self_attn.in_proj_weight\": 196608,\n",
            "  \"transformer.decoder.layers.5.self_attn.in_proj_bias\": 768,\n",
            "  \"transformer.decoder.layers.5.self_attn.out_proj.weight\": 65536,\n",
            "  \"transformer.decoder.layers.5.self_attn.out_proj.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.norm2.weight\": 256,\n",
            "  \"transformer.decoder.layers.5.norm2.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.linear1.weight\": 524288,\n",
            "  \"transformer.decoder.layers.5.linear1.bias\": 2048,\n",
            "  \"transformer.decoder.layers.5.linear2.weight\": 524288,\n",
            "  \"transformer.decoder.layers.5.linear2.bias\": 256,\n",
            "  \"transformer.decoder.layers.5.norm3.weight\": 256,\n",
            "  \"transformer.decoder.layers.5.norm3.bias\": 256,\n",
            "  \"transformer.decoder.norm.weight\": 256,\n",
            "  \"transformer.decoder.norm.bias\": 256,\n",
            "  \"transformer.decoder.ref_point_head.layers.0.weight\": 131072,\n",
            "  \"transformer.decoder.ref_point_head.layers.0.bias\": 256,\n",
            "  \"transformer.decoder.ref_point_head.layers.1.weight\": 65536,\n",
            "  \"transformer.decoder.ref_point_head.layers.1.bias\": 256,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.0.weight\": 65536,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.0.bias\": 256,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.1.weight\": 65536,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.1.bias\": 256,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.2.weight\": 1024,\n",
            "  \"transformer.decoder.bbox_embed.0.layers.2.bias\": 4,\n",
            "  \"transformer.tgt_embed.weight\": 230400,\n",
            "  \"transformer.enc_output.weight\": 65536,\n",
            "  \"transformer.enc_output.bias\": 256,\n",
            "  \"transformer.enc_output_norm.weight\": 256,\n",
            "  \"transformer.enc_output_norm.bias\": 256,\n",
            "  \"transformer.enc_out_bbox_embed.layers.0.weight\": 65536,\n",
            "  \"transformer.enc_out_bbox_embed.layers.0.bias\": 256,\n",
            "  \"transformer.enc_out_bbox_embed.layers.1.weight\": 65536,\n",
            "  \"transformer.enc_out_bbox_embed.layers.1.bias\": 256,\n",
            "  \"transformer.enc_out_bbox_embed.layers.2.weight\": 1024,\n",
            "  \"transformer.enc_out_bbox_embed.layers.2.bias\": 4,\n",
            "  \"feat_map.weight\": 196608,\n",
            "  \"feat_map.bias\": 256,\n",
            "  \"input_proj.0.0.weight\": 49152,\n",
            "  \"input_proj.0.0.bias\": 256,\n",
            "  \"input_proj.0.1.weight\": 256,\n",
            "  \"input_proj.0.1.bias\": 256,\n",
            "  \"input_proj.1.0.weight\": 98304,\n",
            "  \"input_proj.1.0.bias\": 256,\n",
            "  \"input_proj.1.1.weight\": 256,\n",
            "  \"input_proj.1.1.bias\": 256,\n",
            "  \"input_proj.2.0.weight\": 196608,\n",
            "  \"input_proj.2.0.bias\": 256,\n",
            "  \"input_proj.2.1.weight\": 256,\n",
            "  \"input_proj.2.1.bias\": 256,\n",
            "  \"input_proj.3.0.weight\": 1769472,\n",
            "  \"input_proj.3.0.bias\": 256,\n",
            "  \"input_proj.3.1.weight\": 256,\n",
            "  \"input_proj.3.1.bias\": 256,\n",
            "  \"backbone.0.patch_embed.proj.weight\": 4608,\n",
            "  \"backbone.0.patch_embed.proj.bias\": 96,\n",
            "  \"backbone.0.patch_embed.norm.weight\": 96,\n",
            "  \"backbone.0.patch_embed.norm.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.0.norm1.weight\": 96,\n",
            "  \"backbone.0.layers.0.blocks.0.norm1.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.0.attn.relative_position_bias_table\": 507,\n",
            "  \"backbone.0.layers.0.blocks.0.attn.qkv.weight\": 27648,\n",
            "  \"backbone.0.layers.0.blocks.0.attn.qkv.bias\": 288,\n",
            "  \"backbone.0.layers.0.blocks.0.attn.proj.weight\": 9216,\n",
            "  \"backbone.0.layers.0.blocks.0.attn.proj.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.0.norm2.weight\": 96,\n",
            "  \"backbone.0.layers.0.blocks.0.norm2.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.0.mlp.fc1.weight\": 36864,\n",
            "  \"backbone.0.layers.0.blocks.0.mlp.fc1.bias\": 384,\n",
            "  \"backbone.0.layers.0.blocks.0.mlp.fc2.weight\": 36864,\n",
            "  \"backbone.0.layers.0.blocks.0.mlp.fc2.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.1.norm1.weight\": 96,\n",
            "  \"backbone.0.layers.0.blocks.1.norm1.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.1.attn.relative_position_bias_table\": 507,\n",
            "  \"backbone.0.layers.0.blocks.1.attn.qkv.weight\": 27648,\n",
            "  \"backbone.0.layers.0.blocks.1.attn.qkv.bias\": 288,\n",
            "  \"backbone.0.layers.0.blocks.1.attn.proj.weight\": 9216,\n",
            "  \"backbone.0.layers.0.blocks.1.attn.proj.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.1.norm2.weight\": 96,\n",
            "  \"backbone.0.layers.0.blocks.1.norm2.bias\": 96,\n",
            "  \"backbone.0.layers.0.blocks.1.mlp.fc1.weight\": 36864,\n",
            "  \"backbone.0.layers.0.blocks.1.mlp.fc1.bias\": 384,\n",
            "  \"backbone.0.layers.0.blocks.1.mlp.fc2.weight\": 36864,\n",
            "  \"backbone.0.layers.0.blocks.1.mlp.fc2.bias\": 96,\n",
            "  \"backbone.0.layers.0.downsample.reduction.weight\": 73728,\n",
            "  \"backbone.0.layers.0.downsample.norm.weight\": 384,\n",
            "  \"backbone.0.layers.0.downsample.norm.bias\": 384,\n",
            "  \"backbone.0.layers.1.blocks.0.norm1.weight\": 192,\n",
            "  \"backbone.0.layers.1.blocks.0.norm1.bias\": 192,\n",
            "  \"backbone.0.layers.1.blocks.0.attn.relative_position_bias_table\": 1014,\n",
            "  \"backbone.0.layers.1.blocks.0.attn.qkv.weight\": 110592,\n",
            "  \"backbone.0.layers.1.blocks.0.attn.qkv.bias\": 576,\n",
            "  \"backbone.0.layers.1.blocks.0.attn.proj.weight\": 36864,\n",
            "  \"backbone.0.layers.1.blocks.0.attn.proj.bias\": 192,\n",
            "  \"backbone.0.layers.1.blocks.0.norm2.weight\": 192,\n",
            "  \"backbone.0.layers.1.blocks.0.norm2.bias\": 192,\n",
            "  \"backbone.0.layers.1.blocks.0.mlp.fc1.weight\": 147456,\n",
            "  \"backbone.0.layers.1.blocks.0.mlp.fc1.bias\": 768,\n",
            "  \"backbone.0.layers.1.blocks.0.mlp.fc2.weight\": 147456,\n",
            "  \"backbone.0.layers.1.blocks.0.mlp.fc2.bias\": 192,\n",
            "  \"backbone.0.layers.1.blocks.1.norm1.weight\": 192,\n",
            "  \"backbone.0.layers.1.blocks.1.norm1.bias\": 192,\n",
            "  \"backbone.0.layers.1.blocks.1.attn.relative_position_bias_table\": 1014,\n",
            "  \"backbone.0.layers.1.blocks.1.attn.qkv.weight\": 110592,\n",
            "  \"backbone.0.layers.1.blocks.1.attn.qkv.bias\": 576,\n",
            "  \"backbone.0.layers.1.blocks.1.attn.proj.weight\": 36864,\n",
            "  \"backbone.0.layers.1.blocks.1.attn.proj.bias\": 192,\n",
            "  \"backbone.0.layers.1.blocks.1.norm2.weight\": 192,\n",
            "  \"backbone.0.layers.1.blocks.1.norm2.bias\": 192,\n",
            "  \"backbone.0.layers.1.blocks.1.mlp.fc1.weight\": 147456,\n",
            "  \"backbone.0.layers.1.blocks.1.mlp.fc1.bias\": 768,\n",
            "  \"backbone.0.layers.1.blocks.1.mlp.fc2.weight\": 147456,\n",
            "  \"backbone.0.layers.1.blocks.1.mlp.fc2.bias\": 192,\n",
            "  \"backbone.0.layers.1.downsample.reduction.weight\": 294912,\n",
            "  \"backbone.0.layers.1.downsample.norm.weight\": 768,\n",
            "  \"backbone.0.layers.1.downsample.norm.bias\": 768,\n",
            "  \"backbone.0.layers.2.blocks.0.norm1.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.0.norm1.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.0.attn.relative_position_bias_table\": 2028,\n",
            "  \"backbone.0.layers.2.blocks.0.attn.qkv.weight\": 442368,\n",
            "  \"backbone.0.layers.2.blocks.0.attn.qkv.bias\": 1152,\n",
            "  \"backbone.0.layers.2.blocks.0.attn.proj.weight\": 147456,\n",
            "  \"backbone.0.layers.2.blocks.0.attn.proj.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.0.norm2.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.0.norm2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.0.mlp.fc1.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.0.mlp.fc1.bias\": 1536,\n",
            "  \"backbone.0.layers.2.blocks.0.mlp.fc2.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.0.mlp.fc2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.1.norm1.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.1.norm1.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.1.attn.relative_position_bias_table\": 2028,\n",
            "  \"backbone.0.layers.2.blocks.1.attn.qkv.weight\": 442368,\n",
            "  \"backbone.0.layers.2.blocks.1.attn.qkv.bias\": 1152,\n",
            "  \"backbone.0.layers.2.blocks.1.attn.proj.weight\": 147456,\n",
            "  \"backbone.0.layers.2.blocks.1.attn.proj.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.1.norm2.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.1.norm2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.1.mlp.fc1.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.1.mlp.fc1.bias\": 1536,\n",
            "  \"backbone.0.layers.2.blocks.1.mlp.fc2.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.1.mlp.fc2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.2.norm1.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.2.norm1.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.2.attn.relative_position_bias_table\": 2028,\n",
            "  \"backbone.0.layers.2.blocks.2.attn.qkv.weight\": 442368,\n",
            "  \"backbone.0.layers.2.blocks.2.attn.qkv.bias\": 1152,\n",
            "  \"backbone.0.layers.2.blocks.2.attn.proj.weight\": 147456,\n",
            "  \"backbone.0.layers.2.blocks.2.attn.proj.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.2.norm2.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.2.norm2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.2.mlp.fc1.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.2.mlp.fc1.bias\": 1536,\n",
            "  \"backbone.0.layers.2.blocks.2.mlp.fc2.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.2.mlp.fc2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.3.norm1.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.3.norm1.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.3.attn.relative_position_bias_table\": 2028,\n",
            "  \"backbone.0.layers.2.blocks.3.attn.qkv.weight\": 442368,\n",
            "  \"backbone.0.layers.2.blocks.3.attn.qkv.bias\": 1152,\n",
            "  \"backbone.0.layers.2.blocks.3.attn.proj.weight\": 147456,\n",
            "  \"backbone.0.layers.2.blocks.3.attn.proj.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.3.norm2.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.3.norm2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.3.mlp.fc1.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.3.mlp.fc1.bias\": 1536,\n",
            "  \"backbone.0.layers.2.blocks.3.mlp.fc2.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.3.mlp.fc2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.4.norm1.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.4.norm1.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.4.attn.relative_position_bias_table\": 2028,\n",
            "  \"backbone.0.layers.2.blocks.4.attn.qkv.weight\": 442368,\n",
            "  \"backbone.0.layers.2.blocks.4.attn.qkv.bias\": 1152,\n",
            "  \"backbone.0.layers.2.blocks.4.attn.proj.weight\": 147456,\n",
            "  \"backbone.0.layers.2.blocks.4.attn.proj.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.4.norm2.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.4.norm2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.4.mlp.fc1.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.4.mlp.fc1.bias\": 1536,\n",
            "  \"backbone.0.layers.2.blocks.4.mlp.fc2.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.4.mlp.fc2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.5.norm1.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.5.norm1.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.5.attn.relative_position_bias_table\": 2028,\n",
            "  \"backbone.0.layers.2.blocks.5.attn.qkv.weight\": 442368,\n",
            "  \"backbone.0.layers.2.blocks.5.attn.qkv.bias\": 1152,\n",
            "  \"backbone.0.layers.2.blocks.5.attn.proj.weight\": 147456,\n",
            "  \"backbone.0.layers.2.blocks.5.attn.proj.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.5.norm2.weight\": 384,\n",
            "  \"backbone.0.layers.2.blocks.5.norm2.bias\": 384,\n",
            "  \"backbone.0.layers.2.blocks.5.mlp.fc1.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.5.mlp.fc1.bias\": 1536,\n",
            "  \"backbone.0.layers.2.blocks.5.mlp.fc2.weight\": 589824,\n",
            "  \"backbone.0.layers.2.blocks.5.mlp.fc2.bias\": 384,\n",
            "  \"backbone.0.layers.2.downsample.reduction.weight\": 1179648,\n",
            "  \"backbone.0.layers.2.downsample.norm.weight\": 1536,\n",
            "  \"backbone.0.layers.2.downsample.norm.bias\": 1536,\n",
            "  \"backbone.0.layers.3.blocks.0.norm1.weight\": 768,\n",
            "  \"backbone.0.layers.3.blocks.0.norm1.bias\": 768,\n",
            "  \"backbone.0.layers.3.blocks.0.attn.relative_position_bias_table\": 4056,\n",
            "  \"backbone.0.layers.3.blocks.0.attn.qkv.weight\": 1769472,\n",
            "  \"backbone.0.layers.3.blocks.0.attn.qkv.bias\": 2304,\n",
            "  \"backbone.0.layers.3.blocks.0.attn.proj.weight\": 589824,\n",
            "  \"backbone.0.layers.3.blocks.0.attn.proj.bias\": 768,\n",
            "  \"backbone.0.layers.3.blocks.0.norm2.weight\": 768,\n",
            "  \"backbone.0.layers.3.blocks.0.norm2.bias\": 768,\n",
            "  \"backbone.0.layers.3.blocks.0.mlp.fc1.weight\": 2359296,\n",
            "  \"backbone.0.layers.3.blocks.0.mlp.fc1.bias\": 3072,\n",
            "  \"backbone.0.layers.3.blocks.0.mlp.fc2.weight\": 2359296,\n",
            "  \"backbone.0.layers.3.blocks.0.mlp.fc2.bias\": 768,\n",
            "  \"backbone.0.layers.3.blocks.1.norm1.weight\": 768,\n",
            "  \"backbone.0.layers.3.blocks.1.norm1.bias\": 768,\n",
            "  \"backbone.0.layers.3.blocks.1.attn.relative_position_bias_table\": 4056,\n",
            "  \"backbone.0.layers.3.blocks.1.attn.qkv.weight\": 1769472,\n",
            "  \"backbone.0.layers.3.blocks.1.attn.qkv.bias\": 2304,\n",
            "  \"backbone.0.layers.3.blocks.1.attn.proj.weight\": 589824,\n",
            "  \"backbone.0.layers.3.blocks.1.attn.proj.bias\": 768,\n",
            "  \"backbone.0.layers.3.blocks.1.norm2.weight\": 768,\n",
            "  \"backbone.0.layers.3.blocks.1.norm2.bias\": 768,\n",
            "  \"backbone.0.layers.3.blocks.1.mlp.fc1.weight\": 2359296,\n",
            "  \"backbone.0.layers.3.blocks.1.mlp.fc1.bias\": 3072,\n",
            "  \"backbone.0.layers.3.blocks.1.mlp.fc2.weight\": 2359296,\n",
            "  \"backbone.0.layers.3.blocks.1.mlp.fc2.bias\": 768,\n",
            "  \"backbone.0.norm1.weight\": 192,\n",
            "  \"backbone.0.norm1.bias\": 192,\n",
            "  \"backbone.0.norm2.weight\": 384,\n",
            "  \"backbone.0.norm2.bias\": 384,\n",
            "  \"backbone.0.norm3.weight\": 768,\n",
            "  \"backbone.0.norm3.bias\": 768\n",
            "}\u001b[0m\n",
            "\u001b[36mDEBUG   \u001b[0m \u001b[36m2024-06-12 05:47:43,040 | \u001b[34mbuild dataset ... ...\u001b[0m\n",
            "/content/open_img/train/ /content/open_img/anno/out.jsonl /content/open_img/anno/label.json\n",
            "  == total images: 360\n",
            "  == total labels: 7\n",
            "\u001b[36mDEBUG   \u001b[0m \u001b[36m2024-06-12 05:47:43,050 | \u001b[34mbuild dataset, done.\u001b[0m\n",
            "\u001b[36mDEBUG   \u001b[0m \u001b[36m2024-06-12 05:47:43,050 | \u001b[34mnumber of training dataset: 1, samples: 360\u001b[0m\n",
            "/content/open_img/val /content/open_img/val.json\n",
            "loading annotations into memory...\n",
            "Done (t=0.00s)\n",
            "creating index...\n",
            "index created!\n",
            "\u001b[32mINFO    \u001b[0m \u001b[32m2024-06-12 05:48:05,777 | \u001b[34mIgnore keys: []\u001b[0m\n",
            "\u001b[32mINFO    \u001b[0m \u001b[32m2024-06-12 05:48:05,990 | \u001b[34m_IncompatibleKeys(missing_keys=[], unexpected_keys=['label_enc.weight', 'bert.embeddings.position_ids'])\u001b[0m\n",
            "Start training\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:1052: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "Epoch: [0]  [ 0/90]  eta: 0:10:13  lr: 0.000100  loss: 17.5592 (17.5592)  loss_ce: 1.4126 (1.4126)  loss_bbox: 0.2542 (0.2542)  loss_giou: 0.8373 (0.8373)  loss_ce_0: 1.3664 (1.3664)  loss_bbox_0: 0.2638 (0.2638)  loss_giou_0: 0.8677 (0.8677)  loss_ce_1: 1.3717 (1.3717)  loss_bbox_1: 0.2556 (0.2556)  loss_giou_1: 0.8723 (0.8723)  loss_ce_2: 1.4074 (1.4074)  loss_bbox_2: 0.2662 (0.2662)  loss_giou_2: 0.8854 (0.8854)  loss_ce_3: 1.4136 (1.4136)  loss_bbox_3: 0.2479 (0.2479)  loss_giou_3: 0.8467 (0.8467)  loss_ce_4: 1.4044 (1.4044)  loss_bbox_4: 0.2555 (0.2555)  loss_giou_4: 0.8424 (0.8424)  loss_ce_interm: 1.3606 (1.3606)  loss_bbox_interm: 0.2622 (0.2622)  loss_giou_interm: 0.8654 (0.8654)  loss_ce_unscaled: 0.7063 (0.7063)  loss_bbox_unscaled: 0.0508 (0.0508)  loss_giou_unscaled: 0.4186 (0.4186)  loss_xy_unscaled: 0.0128 (0.0128)  loss_hw_unscaled: 0.0380 (0.0380)  loss_ce_0_unscaled: 0.6832 (0.6832)  loss_bbox_0_unscaled: 0.0528 (0.0528)  loss_giou_0_unscaled: 0.4339 (0.4339)  loss_xy_0_unscaled: 0.0139 (0.0139)  loss_hw_0_unscaled: 0.0389 (0.0389)  loss_ce_1_unscaled: 0.6859 (0.6859)  loss_bbox_1_unscaled: 0.0511 (0.0511)  loss_giou_1_unscaled: 0.4362 (0.4362)  loss_xy_1_unscaled: 0.0141 (0.0141)  loss_hw_1_unscaled: 0.0371 (0.0371)  loss_ce_2_unscaled: 0.7037 (0.7037)  loss_bbox_2_unscaled: 0.0532 (0.0532)  loss_giou_2_unscaled: 0.4427 (0.4427)  loss_xy_2_unscaled: 0.0140 (0.0140)  loss_hw_2_unscaled: 0.0392 (0.0392)  loss_ce_3_unscaled: 0.7068 (0.7068)  loss_bbox_3_unscaled: 0.0496 (0.0496)  loss_giou_3_unscaled: 0.4233 (0.4233)  loss_xy_3_unscaled: 0.0129 (0.0129)  loss_hw_3_unscaled: 0.0367 (0.0367)  loss_ce_4_unscaled: 0.7022 (0.7022)  loss_bbox_4_unscaled: 0.0511 (0.0511)  loss_giou_4_unscaled: 0.4212 (0.4212)  loss_xy_4_unscaled: 0.0130 (0.0130)  loss_hw_4_unscaled: 0.0381 (0.0381)  loss_ce_interm_unscaled: 0.6803 (0.6803)  loss_bbox_interm_unscaled: 0.0524 (0.0524)  loss_giou_interm_unscaled: 0.4327 (0.4327)  loss_xy_interm_unscaled: 0.0133 (0.0133)  loss_hw_interm_unscaled: 0.0391 (0.0391)  time: 6.8183  data: 0.8335  max mem: 7078\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
            "  return F.conv2d(input, weight, bias, self.stride,\n",
            "Epoch: [0]  [10/90]  eta: 0:04:04  lr: 0.000100  loss: 10.3850 (11.5036)  loss_ce: 0.7612 (0.8460)  loss_bbox: 0.0797 (0.1098)  loss_giou: 0.5897 (0.6361)  loss_ce_0: 0.8639 (0.9000)  loss_bbox_0: 0.0874 (0.1153)  loss_giou_0: 0.6564 (0.6628)  loss_ce_1: 0.8600 (0.8715)  loss_bbox_1: 0.0858 (0.1142)  loss_giou_1: 0.6367 (0.6584)  loss_ce_2: 0.7967 (0.8643)  loss_bbox_2: 0.0847 (0.1142)  loss_giou_2: 0.6299 (0.6545)  loss_ce_3: 0.7925 (0.8573)  loss_bbox_3: 0.0822 (0.1105)  loss_giou_3: 0.6109 (0.6392)  loss_ce_4: 0.7938 (0.8459)  loss_bbox_4: 0.0801 (0.1101)  loss_giou_4: 0.5917 (0.6365)  loss_ce_interm: 0.9430 (0.9362)  loss_bbox_interm: 0.0912 (0.1217)  loss_giou_interm: 0.6798 (0.6994)  loss_ce_unscaled: 0.3806 (0.4230)  loss_bbox_unscaled: 0.0159 (0.0220)  loss_giou_unscaled: 0.2949 (0.3181)  loss_xy_unscaled: 0.0048 (0.0063)  loss_hw_unscaled: 0.0110 (0.0157)  loss_ce_0_unscaled: 0.4319 (0.4500)  loss_bbox_0_unscaled: 0.0175 (0.0231)  loss_giou_0_unscaled: 0.3282 (0.3314)  loss_xy_0_unscaled: 0.0052 (0.0066)  loss_hw_0_unscaled: 0.0129 (0.0164)  loss_ce_1_unscaled: 0.4300 (0.4357)  loss_bbox_1_unscaled: 0.0172 (0.0228)  loss_giou_1_unscaled: 0.3184 (0.3292)  loss_xy_1_unscaled: 0.0048 (0.0067)  loss_hw_1_unscaled: 0.0135 (0.0162)  loss_ce_2_unscaled: 0.3984 (0.4321)  loss_bbox_2_unscaled: 0.0169 (0.0228)  loss_giou_2_unscaled: 0.3150 (0.3272)  loss_xy_2_unscaled: 0.0047 (0.0065)  loss_hw_2_unscaled: 0.0130 (0.0164)  loss_ce_3_unscaled: 0.3963 (0.4286)  loss_bbox_3_unscaled: 0.0164 (0.0221)  loss_giou_3_unscaled: 0.3055 (0.3196)  loss_xy_3_unscaled: 0.0045 (0.0063)  loss_hw_3_unscaled: 0.0124 (0.0158)  loss_ce_4_unscaled: 0.3969 (0.4229)  loss_bbox_4_unscaled: 0.0160 (0.0220)  loss_giou_4_unscaled: 0.2959 (0.3183)  loss_xy_4_unscaled: 0.0047 (0.0063)  loss_hw_4_unscaled: 0.0111 (0.0157)  loss_ce_interm_unscaled: 0.4715 (0.4681)  loss_bbox_interm_unscaled: 0.0182 (0.0243)  loss_giou_interm_unscaled: 0.3399 (0.3497)  loss_xy_interm_unscaled: 0.0054 (0.0070)  loss_hw_interm_unscaled: 0.0128 (0.0173)  time: 3.0574  data: 0.0886  max mem: 7467\n",
            "Epoch: [0]  [20/90]  eta: 0:03:21  lr: 0.000100  loss: 8.1545 (9.5096)  loss_ce: 0.5757 (0.6611)  loss_bbox: 0.0621 (0.0859)  loss_giou: 0.5244 (0.5650)  loss_ce_0: 0.5889 (0.7055)  loss_bbox_0: 0.0666 (0.0895)  loss_giou_0: 0.5388 (0.5918)  loss_ce_1: 0.6005 (0.6872)  loss_bbox_1: 0.0629 (0.0888)  loss_giou_1: 0.5495 (0.5931)  loss_ce_2: 0.5774 (0.6828)  loss_bbox_2: 0.0625 (0.0885)  loss_giou_2: 0.5347 (0.5838)  loss_ce_3: 0.5802 (0.6774)  loss_bbox_3: 0.0633 (0.0865)  loss_giou_3: 0.5255 (0.5715)  loss_ce_4: 0.5730 (0.6629)  loss_bbox_4: 0.0623 (0.0861)  loss_giou_4: 0.5239 (0.5670)  loss_ce_interm: 0.6062 (0.7310)  loss_bbox_interm: 0.0760 (0.0944)  loss_giou_interm: 0.5821 (0.6096)  loss_ce_unscaled: 0.2879 (0.3306)  loss_bbox_unscaled: 0.0124 (0.0172)  loss_giou_unscaled: 0.2622 (0.2825)  loss_xy_unscaled: 0.0043 (0.0051)  loss_hw_unscaled: 0.0084 (0.0121)  loss_ce_0_unscaled: 0.2944 (0.3528)  loss_bbox_0_unscaled: 0.0133 (0.0179)  loss_giou_0_unscaled: 0.2694 (0.2959)  loss_xy_0_unscaled: 0.0042 (0.0054)  loss_hw_0_unscaled: 0.0095 (0.0125)  loss_ce_1_unscaled: 0.3003 (0.3436)  loss_bbox_1_unscaled: 0.0126 (0.0178)  loss_giou_1_unscaled: 0.2748 (0.2965)  loss_xy_1_unscaled: 0.0041 (0.0054)  loss_hw_1_unscaled: 0.0084 (0.0124)  loss_ce_2_unscaled: 0.2887 (0.3414)  loss_bbox_2_unscaled: 0.0125 (0.0177)  loss_giou_2_unscaled: 0.2673 (0.2919)  loss_xy_2_unscaled: 0.0041 (0.0052)  loss_hw_2_unscaled: 0.0083 (0.0124)  loss_ce_3_unscaled: 0.2901 (0.3387)  loss_bbox_3_unscaled: 0.0127 (0.0173)  loss_giou_3_unscaled: 0.2628 (0.2858)  loss_xy_3_unscaled: 0.0042 (0.0052)  loss_hw_3_unscaled: 0.0083 (0.0122)  loss_ce_4_unscaled: 0.2865 (0.3315)  loss_bbox_4_unscaled: 0.0125 (0.0172)  loss_giou_4_unscaled: 0.2620 (0.2835)  loss_xy_4_unscaled: 0.0043 (0.0051)  loss_hw_4_unscaled: 0.0084 (0.0121)  loss_ce_interm_unscaled: 0.3031 (0.3655)  loss_bbox_interm_unscaled: 0.0152 (0.0189)  loss_giou_interm_unscaled: 0.2911 (0.3048)  loss_xy_interm_unscaled: 0.0047 (0.0056)  loss_hw_interm_unscaled: 0.0101 (0.0133)  time: 2.6851  data: 0.0130  max mem: 8014\n",
            "Epoch: [0]  [30/90]  eta: 0:02:49  lr: 0.000100  loss: 6.7035 (8.5805)  loss_ce: 0.4209 (0.5898)  loss_bbox: 0.0569 (0.0761)  loss_giou: 0.4778 (0.5327)  loss_ce_0: 0.3979 (0.6040)  loss_bbox_0: 0.0566 (0.0792)  loss_giou_0: 0.4942 (0.5585)  loss_ce_1: 0.3895 (0.5875)  loss_bbox_1: 0.0577 (0.0786)  loss_giou_1: 0.5006 (0.5595)  loss_ce_2: 0.3946 (0.5898)  loss_bbox_2: 0.0568 (0.0781)  loss_giou_2: 0.4939 (0.5496)  loss_ce_3: 0.4203 (0.5913)  loss_bbox_3: 0.0567 (0.0767)  loss_giou_3: 0.4899 (0.5395)  loss_ce_4: 0.4203 (0.5860)  loss_bbox_4: 0.0570 (0.0762)  loss_giou_4: 0.4777 (0.5343)  loss_ce_interm: 0.4366 (0.6270)  loss_bbox_interm: 0.0615 (0.0834)  loss_giou_interm: 0.5160 (0.5825)  loss_ce_unscaled: 0.2104 (0.2949)  loss_bbox_unscaled: 0.0114 (0.0152)  loss_giou_unscaled: 0.2389 (0.2663)  loss_xy_unscaled: 0.0033 (0.0045)  loss_hw_unscaled: 0.0078 (0.0108)  loss_ce_0_unscaled: 0.1990 (0.3020)  loss_bbox_0_unscaled: 0.0113 (0.0158)  loss_giou_0_unscaled: 0.2471 (0.2792)  loss_xy_0_unscaled: 0.0035 (0.0047)  loss_hw_0_unscaled: 0.0080 (0.0112)  loss_ce_1_unscaled: 0.1948 (0.2938)  loss_bbox_1_unscaled: 0.0115 (0.0157)  loss_giou_1_unscaled: 0.2503 (0.2797)  loss_xy_1_unscaled: 0.0035 (0.0047)  loss_hw_1_unscaled: 0.0081 (0.0111)  loss_ce_2_unscaled: 0.1973 (0.2949)  loss_bbox_2_unscaled: 0.0114 (0.0156)  loss_giou_2_unscaled: 0.2469 (0.2748)  loss_xy_2_unscaled: 0.0034 (0.0046)  loss_hw_2_unscaled: 0.0080 (0.0111)  loss_ce_3_unscaled: 0.2101 (0.2957)  loss_bbox_3_unscaled: 0.0113 (0.0153)  loss_giou_3_unscaled: 0.2449 (0.2698)  loss_xy_3_unscaled: 0.0033 (0.0045)  loss_hw_3_unscaled: 0.0080 (0.0109)  loss_ce_4_unscaled: 0.2101 (0.2930)  loss_bbox_4_unscaled: 0.0114 (0.0152)  loss_giou_4_unscaled: 0.2389 (0.2672)  loss_xy_4_unscaled: 0.0033 (0.0045)  loss_hw_4_unscaled: 0.0078 (0.0107)  loss_ce_interm_unscaled: 0.2183 (0.3135)  loss_bbox_interm_unscaled: 0.0123 (0.0167)  loss_giou_interm_unscaled: 0.2580 (0.2912)  loss_xy_interm_unscaled: 0.0036 (0.0048)  loss_hw_interm_unscaled: 0.0087 (0.0119)  time: 2.6848  data: 0.0135  max mem: 8014\n",
            "Epoch: [0]  [40/90]  eta: 0:02:21  lr: 0.000100  loss: 5.9331 (7.9152)  loss_ce: 0.3675 (0.5310)  loss_bbox: 0.0548 (0.0710)  loss_giou: 0.4357 (0.5100)  loss_ce_0: 0.3444 (0.5379)  loss_bbox_0: 0.0566 (0.0733)  loss_giou_0: 0.4578 (0.5302)  loss_ce_1: 0.3354 (0.5234)  loss_bbox_1: 0.0570 (0.0728)  loss_giou_1: 0.4474 (0.5310)  loss_ce_2: 0.3569 (0.5265)  loss_bbox_2: 0.0563 (0.0725)  loss_giou_2: 0.4461 (0.5228)  loss_ce_3: 0.3522 (0.5272)  loss_bbox_3: 0.0563 (0.0715)  loss_giou_3: 0.4528 (0.5155)  loss_ce_4: 0.3607 (0.5247)  loss_bbox_4: 0.0551 (0.0711)  loss_giou_4: 0.4378 (0.5119)  loss_ce_interm: 0.3686 (0.5605)  loss_bbox_interm: 0.0605 (0.0770)  loss_giou_interm: 0.4885 (0.5533)  loss_ce_unscaled: 0.1837 (0.2655)  loss_bbox_unscaled: 0.0110 (0.0142)  loss_giou_unscaled: 0.2178 (0.2550)  loss_xy_unscaled: 0.0031 (0.0041)  loss_hw_unscaled: 0.0077 (0.0101)  loss_ce_0_unscaled: 0.1722 (0.2690)  loss_bbox_0_unscaled: 0.0113 (0.0147)  loss_giou_0_unscaled: 0.2289 (0.2651)  loss_xy_0_unscaled: 0.0030 (0.0043)  loss_hw_0_unscaled: 0.0083 (0.0104)  loss_ce_1_unscaled: 0.1677 (0.2617)  loss_bbox_1_unscaled: 0.0114 (0.0146)  loss_giou_1_unscaled: 0.2237 (0.2655)  loss_xy_1_unscaled: 0.0031 (0.0043)  loss_hw_1_unscaled: 0.0082 (0.0103)  loss_ce_2_unscaled: 0.1785 (0.2632)  loss_bbox_2_unscaled: 0.0113 (0.0145)  loss_giou_2_unscaled: 0.2230 (0.2614)  loss_xy_2_unscaled: 0.0030 (0.0042)  loss_hw_2_unscaled: 0.0080 (0.0103)  loss_ce_3_unscaled: 0.1761 (0.2636)  loss_bbox_3_unscaled: 0.0113 (0.0143)  loss_giou_3_unscaled: 0.2264 (0.2578)  loss_xy_3_unscaled: 0.0030 (0.0041)  loss_hw_3_unscaled: 0.0080 (0.0102)  loss_ce_4_unscaled: 0.1804 (0.2623)  loss_bbox_4_unscaled: 0.0110 (0.0142)  loss_giou_4_unscaled: 0.2189 (0.2559)  loss_xy_4_unscaled: 0.0031 (0.0041)  loss_hw_4_unscaled: 0.0077 (0.0101)  loss_ce_interm_unscaled: 0.1843 (0.2803)  loss_bbox_interm_unscaled: 0.0121 (0.0154)  loss_giou_interm_unscaled: 0.2443 (0.2766)  loss_xy_interm_unscaled: 0.0032 (0.0044)  loss_hw_interm_unscaled: 0.0088 (0.0110)  time: 2.7935  data: 0.0156  max mem: 8014\n",
            "Epoch: [0]  [50/90]  eta: 0:01:55  lr: 0.000100  loss: 5.7517 (7.4946)  loss_ce: 0.3070 (0.4837)  loss_bbox: 0.0513 (0.0675)  loss_giou: 0.4465 (0.5034)  loss_ce_0: 0.3138 (0.4915)  loss_bbox_0: 0.0527 (0.0695)  loss_giou_0: 0.4583 (0.5204)  loss_ce_1: 0.2890 (0.4773)  loss_bbox_1: 0.0513 (0.0690)  loss_giou_1: 0.4406 (0.5199)  loss_ce_2: 0.2896 (0.4774)  loss_bbox_2: 0.0520 (0.0688)  loss_giou_2: 0.4548 (0.5138)  loss_ce_3: 0.2930 (0.4782)  loss_bbox_3: 0.0512 (0.0679)  loss_giou_3: 0.4588 (0.5086)  loss_ce_4: 0.2876 (0.4746)  loss_bbox_4: 0.0510 (0.0676)  loss_giou_4: 0.4648 (0.5062)  loss_ce_interm: 0.3402 (0.5152)  loss_bbox_interm: 0.0546 (0.0728)  loss_giou_interm: 0.4614 (0.5414)  loss_ce_unscaled: 0.1535 (0.2419)  loss_bbox_unscaled: 0.0103 (0.0135)  loss_giou_unscaled: 0.2233 (0.2517)  loss_xy_unscaled: 0.0031 (0.0039)  loss_hw_unscaled: 0.0074 (0.0096)  loss_ce_0_unscaled: 0.1569 (0.2457)  loss_bbox_0_unscaled: 0.0105 (0.0139)  loss_giou_0_unscaled: 0.2292 (0.2602)  loss_xy_0_unscaled: 0.0031 (0.0041)  loss_hw_0_unscaled: 0.0075 (0.0098)  loss_ce_1_unscaled: 0.1445 (0.2386)  loss_bbox_1_unscaled: 0.0103 (0.0138)  loss_giou_1_unscaled: 0.2203 (0.2599)  loss_xy_1_unscaled: 0.0031 (0.0041)  loss_hw_1_unscaled: 0.0074 (0.0097)  loss_ce_2_unscaled: 0.1448 (0.2387)  loss_bbox_2_unscaled: 0.0104 (0.0138)  loss_giou_2_unscaled: 0.2274 (0.2569)  loss_xy_2_unscaled: 0.0032 (0.0040)  loss_hw_2_unscaled: 0.0074 (0.0098)  loss_ce_3_unscaled: 0.1465 (0.2391)  loss_bbox_3_unscaled: 0.0102 (0.0136)  loss_giou_3_unscaled: 0.2294 (0.2543)  loss_xy_3_unscaled: 0.0031 (0.0039)  loss_hw_3_unscaled: 0.0074 (0.0096)  loss_ce_4_unscaled: 0.1438 (0.2373)  loss_bbox_4_unscaled: 0.0102 (0.0135)  loss_giou_4_unscaled: 0.2324 (0.2531)  loss_xy_4_unscaled: 0.0031 (0.0039)  loss_hw_4_unscaled: 0.0075 (0.0096)  loss_ce_interm_unscaled: 0.1701 (0.2576)  loss_bbox_interm_unscaled: 0.0109 (0.0146)  loss_giou_interm_unscaled: 0.2307 (0.2707)  loss_xy_interm_unscaled: 0.0032 (0.0042)  loss_hw_interm_unscaled: 0.0077 (0.0104)  time: 2.9693  data: 0.0172  max mem: 8014\n",
            "Epoch: [0]  [60/90]  eta: 0:01:26  lr: 0.000100  loss: 5.7094 (7.1514)  loss_ce: 0.2553 (0.4443)  loss_bbox: 0.0532 (0.0658)  loss_giou: 0.4590 (0.4953)  loss_ce_0: 0.2803 (0.4558)  loss_bbox_0: 0.0536 (0.0675)  loss_giou_0: 0.4664 (0.5088)  loss_ce_1: 0.2650 (0.4410)  loss_bbox_1: 0.0540 (0.0670)  loss_giou_1: 0.4640 (0.5090)  loss_ce_2: 0.2522 (0.4384)  loss_bbox_2: 0.0542 (0.0669)  loss_giou_2: 0.4597 (0.5045)  loss_ce_3: 0.2407 (0.4392)  loss_bbox_3: 0.0540 (0.0661)  loss_giou_3: 0.4588 (0.4998)  loss_ce_4: 0.2409 (0.4357)  loss_bbox_4: 0.0538 (0.0660)  loss_giou_4: 0.4648 (0.4980)  loss_ce_interm: 0.3065 (0.4810)  loss_bbox_interm: 0.0570 (0.0708)  loss_giou_interm: 0.4711 (0.5304)  loss_ce_unscaled: 0.1277 (0.2221)  loss_bbox_unscaled: 0.0106 (0.0132)  loss_giou_unscaled: 0.2295 (0.2477)  loss_xy_unscaled: 0.0033 (0.0038)  loss_hw_unscaled: 0.0076 (0.0093)  loss_ce_0_unscaled: 0.1401 (0.2279)  loss_bbox_0_unscaled: 0.0107 (0.0135)  loss_giou_0_unscaled: 0.2332 (0.2544)  loss_xy_0_unscaled: 0.0032 (0.0040)  loss_hw_0_unscaled: 0.0076 (0.0096)  loss_ce_1_unscaled: 0.1325 (0.2205)  loss_bbox_1_unscaled: 0.0108 (0.0134)  loss_giou_1_unscaled: 0.2320 (0.2545)  loss_xy_1_unscaled: 0.0033 (0.0039)  loss_hw_1_unscaled: 0.0075 (0.0095)  loss_ce_2_unscaled: 0.1261 (0.2192)  loss_bbox_2_unscaled: 0.0108 (0.0134)  loss_giou_2_unscaled: 0.2299 (0.2522)  loss_xy_2_unscaled: 0.0033 (0.0039)  loss_hw_2_unscaled: 0.0076 (0.0095)  loss_ce_3_unscaled: 0.1203 (0.2196)  loss_bbox_3_unscaled: 0.0108 (0.0132)  loss_giou_3_unscaled: 0.2294 (0.2499)  loss_xy_3_unscaled: 0.0033 (0.0038)  loss_hw_3_unscaled: 0.0076 (0.0094)  loss_ce_4_unscaled: 0.1205 (0.2179)  loss_bbox_4_unscaled: 0.0108 (0.0132)  loss_giou_4_unscaled: 0.2324 (0.2490)  loss_xy_4_unscaled: 0.0033 (0.0038)  loss_hw_4_unscaled: 0.0076 (0.0094)  loss_ce_interm_unscaled: 0.1532 (0.2405)  loss_bbox_interm_unscaled: 0.0114 (0.0142)  loss_giou_interm_unscaled: 0.2356 (0.2652)  loss_xy_interm_unscaled: 0.0033 (0.0041)  loss_hw_interm_unscaled: 0.0080 (0.0101)  time: 3.0037  data: 0.0173  max mem: 8014\n",
            "Epoch: [0]  [70/90]  eta: 0:00:57  lr: 0.000100  loss: 5.6503 (6.9486)  loss_ce: 0.2517 (0.4206)  loss_bbox: 0.0551 (0.0641)  loss_giou: 0.4590 (0.4911)  loss_ce_0: 0.2889 (0.4348)  loss_bbox_0: 0.0550 (0.0657)  loss_giou_0: 0.4626 (0.5038)  loss_ce_1: 0.2687 (0.4181)  loss_bbox_1: 0.0546 (0.0652)  loss_giou_1: 0.4603 (0.5044)  loss_ce_2: 0.2522 (0.4147)  loss_bbox_2: 0.0553 (0.0650)  loss_giou_2: 0.4641 (0.4997)  loss_ce_3: 0.2388 (0.4143)  loss_bbox_3: 0.0549 (0.0644)  loss_giou_3: 0.4605 (0.4959)  loss_ce_4: 0.2392 (0.4119)  loss_bbox_4: 0.0549 (0.0643)  loss_giou_4: 0.4622 (0.4943)  loss_ce_interm: 0.3007 (0.4620)  loss_bbox_interm: 0.0585 (0.0690)  loss_giou_interm: 0.4821 (0.5253)  loss_ce_unscaled: 0.1259 (0.2103)  loss_bbox_unscaled: 0.0110 (0.0128)  loss_giou_unscaled: 0.2295 (0.2455)  loss_xy_unscaled: 0.0033 (0.0037)  loss_hw_unscaled: 0.0076 (0.0091)  loss_ce_0_unscaled: 0.1445 (0.2174)  loss_bbox_0_unscaled: 0.0110 (0.0131)  loss_giou_0_unscaled: 0.2313 (0.2519)  loss_xy_0_unscaled: 0.0033 (0.0039)  loss_hw_0_unscaled: 0.0077 (0.0093)  loss_ce_1_unscaled: 0.1344 (0.2090)  loss_bbox_1_unscaled: 0.0109 (0.0130)  loss_giou_1_unscaled: 0.2302 (0.2522)  loss_xy_1_unscaled: 0.0033 (0.0038)  loss_hw_1_unscaled: 0.0077 (0.0092)  loss_ce_2_unscaled: 0.1261 (0.2074)  loss_bbox_2_unscaled: 0.0111 (0.0130)  loss_giou_2_unscaled: 0.2321 (0.2499)  loss_xy_2_unscaled: 0.0033 (0.0038)  loss_hw_2_unscaled: 0.0077 (0.0092)  loss_ce_3_unscaled: 0.1194 (0.2072)  loss_bbox_3_unscaled: 0.0110 (0.0129)  loss_giou_3_unscaled: 0.2303 (0.2480)  loss_xy_3_unscaled: 0.0033 (0.0037)  loss_hw_3_unscaled: 0.0076 (0.0091)  loss_ce_4_unscaled: 0.1196 (0.2060)  loss_bbox_4_unscaled: 0.0110 (0.0129)  loss_giou_4_unscaled: 0.2311 (0.2471)  loss_xy_4_unscaled: 0.0033 (0.0038)  loss_hw_4_unscaled: 0.0076 (0.0091)  loss_ce_interm_unscaled: 0.1504 (0.2310)  loss_bbox_interm_unscaled: 0.0117 (0.0138)  loss_giou_interm_unscaled: 0.2411 (0.2626)  loss_xy_interm_unscaled: 0.0034 (0.0040)  loss_hw_interm_unscaled: 0.0083 (0.0098)  time: 2.9310  data: 0.0164  max mem: 8014\n",
            "Epoch: [0]  [80/90]  eta: 0:00:29  lr: 0.000100  loss: 5.4665 (6.7880)  loss_ce: 0.2603 (0.4055)  loss_bbox: 0.0512 (0.0630)  loss_giou: 0.4337 (0.4860)  loss_ce_0: 0.2902 (0.4170)  loss_bbox_0: 0.0533 (0.0646)  loss_giou_0: 0.4508 (0.4983)  loss_ce_1: 0.2687 (0.4018)  loss_bbox_1: 0.0522 (0.0641)  loss_giou_1: 0.4566 (0.4986)  loss_ce_2: 0.2623 (0.3991)  loss_bbox_2: 0.0517 (0.0639)  loss_giou_2: 0.4599 (0.4948)  loss_ce_3: 0.2543 (0.3987)  loss_bbox_3: 0.0519 (0.0633)  loss_giou_3: 0.4538 (0.4908)  loss_ce_4: 0.2512 (0.3968)  loss_bbox_4: 0.0521 (0.0632)  loss_giou_4: 0.4443 (0.4890)  loss_ce_interm: 0.3007 (0.4424)  loss_bbox_interm: 0.0557 (0.0677)  loss_giou_interm: 0.4593 (0.5193)  loss_ce_unscaled: 0.1302 (0.2028)  loss_bbox_unscaled: 0.0102 (0.0126)  loss_giou_unscaled: 0.2169 (0.2430)  loss_xy_unscaled: 0.0032 (0.0037)  loss_hw_unscaled: 0.0069 (0.0089)  loss_ce_0_unscaled: 0.1451 (0.2085)  loss_bbox_0_unscaled: 0.0107 (0.0129)  loss_giou_0_unscaled: 0.2254 (0.2492)  loss_xy_0_unscaled: 0.0034 (0.0038)  loss_hw_0_unscaled: 0.0072 (0.0091)  loss_ce_1_unscaled: 0.1344 (0.2009)  loss_bbox_1_unscaled: 0.0104 (0.0128)  loss_giou_1_unscaled: 0.2283 (0.2493)  loss_xy_1_unscaled: 0.0033 (0.0038)  loss_hw_1_unscaled: 0.0071 (0.0090)  loss_ce_2_unscaled: 0.1312 (0.1995)  loss_bbox_2_unscaled: 0.0103 (0.0128)  loss_giou_2_unscaled: 0.2299 (0.2474)  loss_xy_2_unscaled: 0.0032 (0.0037)  loss_hw_2_unscaled: 0.0070 (0.0090)  loss_ce_3_unscaled: 0.1272 (0.1993)  loss_bbox_3_unscaled: 0.0104 (0.0127)  loss_giou_3_unscaled: 0.2269 (0.2454)  loss_xy_3_unscaled: 0.0032 (0.0037)  loss_hw_3_unscaled: 0.0071 (0.0090)  loss_ce_4_unscaled: 0.1256 (0.1984)  loss_bbox_4_unscaled: 0.0104 (0.0126)  loss_giou_4_unscaled: 0.2221 (0.2445)  loss_xy_4_unscaled: 0.0032 (0.0037)  loss_hw_4_unscaled: 0.0071 (0.0089)  loss_ce_interm_unscaled: 0.1504 (0.2212)  loss_bbox_interm_unscaled: 0.0111 (0.0135)  loss_giou_interm_unscaled: 0.2297 (0.2596)  loss_xy_interm_unscaled: 0.0034 (0.0039)  loss_hw_interm_unscaled: 0.0077 (0.0096)  time: 2.9329  data: 0.0161  max mem: 8014\n",
            "Epoch: [0]  [89/90]  eta: 0:00:02  lr: 0.000100  loss: 5.1443 (6.6427)  loss_ce: 0.2522 (0.3905)  loss_bbox: 0.0520 (0.0624)  loss_giou: 0.4249 (0.4807)  loss_ce_0: 0.2608 (0.4044)  loss_bbox_0: 0.0533 (0.0639)  loss_giou_0: 0.4267 (0.4912)  loss_ce_1: 0.2500 (0.3883)  loss_bbox_1: 0.0525 (0.0634)  loss_giou_1: 0.4347 (0.4927)  loss_ce_2: 0.2377 (0.3835)  loss_bbox_2: 0.0524 (0.0633)  loss_giou_2: 0.4300 (0.4892)  loss_ce_3: 0.2365 (0.3841)  loss_bbox_3: 0.0526 (0.0627)  loss_giou_3: 0.4292 (0.4851)  loss_ce_4: 0.2454 (0.3824)  loss_bbox_4: 0.0521 (0.0626)  loss_giou_4: 0.4273 (0.4833)  loss_ce_interm: 0.2836 (0.4296)  loss_bbox_interm: 0.0555 (0.0669)  loss_giou_interm: 0.4466 (0.5125)  loss_ce_unscaled: 0.1261 (0.1952)  loss_bbox_unscaled: 0.0104 (0.0125)  loss_giou_unscaled: 0.2125 (0.2404)  loss_xy_unscaled: 0.0032 (0.0036)  loss_hw_unscaled: 0.0071 (0.0088)  loss_ce_0_unscaled: 0.1304 (0.2022)  loss_bbox_0_unscaled: 0.0107 (0.0128)  loss_giou_0_unscaled: 0.2133 (0.2456)  loss_xy_0_unscaled: 0.0032 (0.0038)  loss_hw_0_unscaled: 0.0072 (0.0090)  loss_ce_1_unscaled: 0.1250 (0.1942)  loss_bbox_1_unscaled: 0.0105 (0.0127)  loss_giou_1_unscaled: 0.2173 (0.2463)  loss_xy_1_unscaled: 0.0032 (0.0037)  loss_hw_1_unscaled: 0.0073 (0.0090)  loss_ce_2_unscaled: 0.1189 (0.1918)  loss_bbox_2_unscaled: 0.0105 (0.0127)  loss_giou_2_unscaled: 0.2150 (0.2446)  loss_xy_2_unscaled: 0.0032 (0.0037)  loss_hw_2_unscaled: 0.0072 (0.0090)  loss_ce_3_unscaled: 0.1183 (0.1921)  loss_bbox_3_unscaled: 0.0105 (0.0125)  loss_giou_3_unscaled: 0.2146 (0.2425)  loss_xy_3_unscaled: 0.0032 (0.0037)  loss_hw_3_unscaled: 0.0072 (0.0089)  loss_ce_4_unscaled: 0.1227 (0.1912)  loss_bbox_4_unscaled: 0.0104 (0.0125)  loss_giou_4_unscaled: 0.2137 (0.2416)  loss_xy_4_unscaled: 0.0032 (0.0037)  loss_hw_4_unscaled: 0.0071 (0.0089)  loss_ce_interm_unscaled: 0.1418 (0.2148)  loss_bbox_interm_unscaled: 0.0111 (0.0134)  loss_giou_interm_unscaled: 0.2233 (0.2563)  loss_xy_interm_unscaled: 0.0035 (0.0039)  loss_hw_interm_unscaled: 0.0077 (0.0095)  time: 2.9733  data: 0.0155  max mem: 8014\n",
            "Epoch: [0] Total time: 0:04:22 (2.9189 s / it)\n",
            "Averaged stats: lr: 0.000100  loss: 5.1443 (6.6427)  loss_ce: 0.2522 (0.3905)  loss_bbox: 0.0520 (0.0624)  loss_giou: 0.4249 (0.4807)  loss_ce_0: 0.2608 (0.4044)  loss_bbox_0: 0.0533 (0.0639)  loss_giou_0: 0.4267 (0.4912)  loss_ce_1: 0.2500 (0.3883)  loss_bbox_1: 0.0525 (0.0634)  loss_giou_1: 0.4347 (0.4927)  loss_ce_2: 0.2377 (0.3835)  loss_bbox_2: 0.0524 (0.0633)  loss_giou_2: 0.4300 (0.4892)  loss_ce_3: 0.2365 (0.3841)  loss_bbox_3: 0.0526 (0.0627)  loss_giou_3: 0.4292 (0.4851)  loss_ce_4: 0.2454 (0.3824)  loss_bbox_4: 0.0521 (0.0626)  loss_giou_4: 0.4273 (0.4833)  loss_ce_interm: 0.2836 (0.4296)  loss_bbox_interm: 0.0555 (0.0669)  loss_giou_interm: 0.4466 (0.5125)  loss_ce_unscaled: 0.1261 (0.1952)  loss_bbox_unscaled: 0.0104 (0.0125)  loss_giou_unscaled: 0.2125 (0.2404)  loss_xy_unscaled: 0.0032 (0.0036)  loss_hw_unscaled: 0.0071 (0.0088)  loss_ce_0_unscaled: 0.1304 (0.2022)  loss_bbox_0_unscaled: 0.0107 (0.0128)  loss_giou_0_unscaled: 0.2133 (0.2456)  loss_xy_0_unscaled: 0.0032 (0.0038)  loss_hw_0_unscaled: 0.0072 (0.0090)  loss_ce_1_unscaled: 0.1250 (0.1942)  loss_bbox_1_unscaled: 0.0105 (0.0127)  loss_giou_1_unscaled: 0.2173 (0.2463)  loss_xy_1_unscaled: 0.0032 (0.0037)  loss_hw_1_unscaled: 0.0073 (0.0090)  loss_ce_2_unscaled: 0.1189 (0.1918)  loss_bbox_2_unscaled: 0.0105 (0.0127)  loss_giou_2_unscaled: 0.2150 (0.2446)  loss_xy_2_unscaled: 0.0032 (0.0037)  loss_hw_2_unscaled: 0.0072 (0.0090)  loss_ce_3_unscaled: 0.1183 (0.1921)  loss_bbox_3_unscaled: 0.0105 (0.0125)  loss_giou_3_unscaled: 0.2146 (0.2425)  loss_xy_3_unscaled: 0.0032 (0.0037)  loss_hw_3_unscaled: 0.0072 (0.0089)  loss_ce_4_unscaled: 0.1227 (0.1912)  loss_bbox_4_unscaled: 0.0104 (0.0125)  loss_giou_4_unscaled: 0.2137 (0.2416)  loss_xy_4_unscaled: 0.0032 (0.0037)  loss_hw_4_unscaled: 0.0071 (0.0089)  loss_ce_interm_unscaled: 0.1418 (0.2148)  loss_bbox_interm_unscaled: 0.0111 (0.0134)  loss_giou_interm_unscaled: 0.2233 (0.2563)  loss_xy_interm_unscaled: 0.0035 (0.0039)  loss_hw_interm_unscaled: 0.0077 (0.0095)\n",
            "Input text prompt: bolt . wrong direction . 1 . 2 . 3 . 4 . 5 .\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:91: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Test:  [ 0/10]  eta: 0:00:17    time: 1.7798  data: 0.8488  max mem: 8014\n",
            "Test:  [ 9/10]  eta: 0:00:01    time: 1.0012  data: 0.1024  max mem: 8014\n",
            "Test: Total time: 0:00:10 (1.0096 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.040\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.133\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.010\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.402\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.034\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.194\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.380\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.363\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.439\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.383\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch: [1]  [ 0/90]  eta: 0:04:26  lr: 0.000100  loss: 5.1799 (5.1799)  loss_ce: 0.2285 (0.2285)  loss_bbox: 0.0521 (0.0521)  loss_giou: 0.4349 (0.4349)  loss_ce_0: 0.2619 (0.2619)  loss_bbox_0: 0.0543 (0.0543)  loss_giou_0: 0.4440 (0.4440)  loss_ce_1: 0.2492 (0.2492)  loss_bbox_1: 0.0523 (0.0523)  loss_giou_1: 0.4301 (0.4301)  loss_ce_2: 0.2406 (0.2406)  loss_bbox_2: 0.0514 (0.0514)  loss_giou_2: 0.4293 (0.4293)  loss_ce_3: 0.2331 (0.2331)  loss_bbox_3: 0.0527 (0.0527)  loss_giou_3: 0.4396 (0.4396)  loss_ce_4: 0.2332 (0.2332)  loss_bbox_4: 0.0522 (0.0522)  loss_giou_4: 0.4341 (0.4341)  loss_ce_interm: 0.2741 (0.2741)  loss_bbox_interm: 0.0586 (0.0586)  loss_giou_interm: 0.4737 (0.4737)  loss_ce_unscaled: 0.1143 (0.1143)  loss_bbox_unscaled: 0.0104 (0.0104)  loss_giou_unscaled: 0.2174 (0.2174)  loss_xy_unscaled: 0.0027 (0.0027)  loss_hw_unscaled: 0.0077 (0.0077)  loss_ce_0_unscaled: 0.1310 (0.1310)  loss_bbox_0_unscaled: 0.0109 (0.0109)  loss_giou_0_unscaled: 0.2220 (0.2220)  loss_xy_0_unscaled: 0.0026 (0.0026)  loss_hw_0_unscaled: 0.0083 (0.0083)  loss_ce_1_unscaled: 0.1246 (0.1246)  loss_bbox_1_unscaled: 0.0105 (0.0105)  loss_giou_1_unscaled: 0.2151 (0.2151)  loss_xy_1_unscaled: 0.0027 (0.0027)  loss_hw_1_unscaled: 0.0078 (0.0078)  loss_ce_2_unscaled: 0.1203 (0.1203)  loss_bbox_2_unscaled: 0.0103 (0.0103)  loss_giou_2_unscaled: 0.2146 (0.2146)  loss_xy_2_unscaled: 0.0027 (0.0027)  loss_hw_2_unscaled: 0.0076 (0.0076)  loss_ce_3_unscaled: 0.1165 (0.1165)  loss_bbox_3_unscaled: 0.0105 (0.0105)  loss_giou_3_unscaled: 0.2198 (0.2198)  loss_xy_3_unscaled: 0.0027 (0.0027)  loss_hw_3_unscaled: 0.0079 (0.0079)  loss_ce_4_unscaled: 0.1166 (0.1166)  loss_bbox_4_unscaled: 0.0104 (0.0104)  loss_giou_4_unscaled: 0.2171 (0.2171)  loss_xy_4_unscaled: 0.0027 (0.0027)  loss_hw_4_unscaled: 0.0078 (0.0078)  loss_ce_interm_unscaled: 0.1370 (0.1370)  loss_bbox_interm_unscaled: 0.0117 (0.0117)  loss_giou_interm_unscaled: 0.2368 (0.2368)  loss_xy_interm_unscaled: 0.0027 (0.0027)  loss_hw_interm_unscaled: 0.0090 (0.0090)  time: 2.9591  data: 0.7637  max mem: 8014\n",
            "Epoch: [1]  [10/90]  eta: 0:03:46  lr: 0.000100  loss: 5.0920 (5.0437)  loss_ce: 0.2108 (0.2151)  loss_bbox: 0.0515 (0.0517)  loss_giou: 0.4349 (0.4363)  loss_ce_0: 0.2472 (0.2595)  loss_bbox_0: 0.0506 (0.0514)  loss_giou_0: 0.4228 (0.4254)  loss_ce_1: 0.2328 (0.2324)  loss_bbox_1: 0.0510 (0.0518)  loss_giou_1: 0.4301 (0.4348)  loss_ce_2: 0.2118 (0.2176)  loss_bbox_2: 0.0514 (0.0516)  loss_giou_2: 0.4326 (0.4370)  loss_ce_3: 0.2124 (0.2140)  loss_bbox_3: 0.0513 (0.0518)  loss_giou_3: 0.4427 (0.4374)  loss_ce_4: 0.2138 (0.2173)  loss_bbox_4: 0.0516 (0.0516)  loss_giou_4: 0.4341 (0.4346)  loss_ce_interm: 0.2643 (0.2714)  loss_bbox_interm: 0.0546 (0.0532)  loss_giou_interm: 0.4523 (0.4480)  loss_ce_unscaled: 0.1054 (0.1076)  loss_bbox_unscaled: 0.0103 (0.0103)  loss_giou_unscaled: 0.2174 (0.2181)  loss_xy_unscaled: 0.0031 (0.0031)  loss_hw_unscaled: 0.0071 (0.0073)  loss_ce_0_unscaled: 0.1236 (0.1297)  loss_bbox_0_unscaled: 0.0101 (0.0103)  loss_giou_0_unscaled: 0.2114 (0.2127)  loss_xy_0_unscaled: 0.0031 (0.0030)  loss_hw_0_unscaled: 0.0069 (0.0072)  loss_ce_1_unscaled: 0.1164 (0.1162)  loss_bbox_1_unscaled: 0.0102 (0.0104)  loss_giou_1_unscaled: 0.2151 (0.2174)  loss_xy_1_unscaled: 0.0031 (0.0031)  loss_hw_1_unscaled: 0.0070 (0.0073)  loss_ce_2_unscaled: 0.1059 (0.1088)  loss_bbox_2_unscaled: 0.0103 (0.0103)  loss_giou_2_unscaled: 0.2163 (0.2185)  loss_xy_2_unscaled: 0.0031 (0.0030)  loss_hw_2_unscaled: 0.0071 (0.0073)  loss_ce_3_unscaled: 0.1062 (0.1070)  loss_bbox_3_unscaled: 0.0103 (0.0104)  loss_giou_3_unscaled: 0.2213 (0.2187)  loss_xy_3_unscaled: 0.0031 (0.0031)  loss_hw_3_unscaled: 0.0071 (0.0073)  loss_ce_4_unscaled: 0.1069 (0.1086)  loss_bbox_4_unscaled: 0.0103 (0.0103)  loss_giou_4_unscaled: 0.2171 (0.2173)  loss_xy_4_unscaled: 0.0031 (0.0031)  loss_hw_4_unscaled: 0.0071 (0.0073)  loss_ce_interm_unscaled: 0.1322 (0.1357)  loss_bbox_interm_unscaled: 0.0109 (0.0106)  loss_giou_interm_unscaled: 0.2262 (0.2240)  loss_xy_interm_unscaled: 0.0031 (0.0032)  loss_hw_interm_unscaled: 0.0073 (0.0075)  time: 2.8304  data: 0.0841  max mem: 8014\n",
            "Epoch: [1]  [20/90]  eta: 0:03:16  lr: 0.000100  loss: 5.2054 (5.1500)  loss_ce: 0.2108 (0.2214)  loss_bbox: 0.0515 (0.0525)  loss_giou: 0.4363 (0.4439)  loss_ce_0: 0.2465 (0.2618)  loss_bbox_0: 0.0522 (0.0523)  loss_giou_0: 0.4323 (0.4382)  loss_ce_1: 0.2412 (0.2422)  loss_bbox_1: 0.0520 (0.0526)  loss_giou_1: 0.4369 (0.4415)  loss_ce_2: 0.2141 (0.2273)  loss_bbox_2: 0.0516 (0.0526)  loss_giou_2: 0.4340 (0.4436)  loss_ce_3: 0.2125 (0.2227)  loss_bbox_3: 0.0521 (0.0527)  loss_giou_3: 0.4427 (0.4445)  loss_ce_4: 0.2107 (0.2214)  loss_bbox_4: 0.0516 (0.0524)  loss_giou_4: 0.4346 (0.4428)  loss_ce_interm: 0.2820 (0.2780)  loss_bbox_interm: 0.0530 (0.0533)  loss_giou_interm: 0.4530 (0.4523)  loss_ce_unscaled: 0.1054 (0.1107)  loss_bbox_unscaled: 0.0103 (0.0105)  loss_giou_unscaled: 0.2182 (0.2219)  loss_xy_unscaled: 0.0030 (0.0030)  loss_hw_unscaled: 0.0073 (0.0074)  loss_ce_0_unscaled: 0.1233 (0.1309)  loss_bbox_0_unscaled: 0.0104 (0.0105)  loss_giou_0_unscaled: 0.2162 (0.2191)  loss_xy_0_unscaled: 0.0030 (0.0030)  loss_hw_0_unscaled: 0.0070 (0.0074)  loss_ce_1_unscaled: 0.1206 (0.1211)  loss_bbox_1_unscaled: 0.0104 (0.0105)  loss_giou_1_unscaled: 0.2185 (0.2207)  loss_xy_1_unscaled: 0.0030 (0.0030)  loss_hw_1_unscaled: 0.0070 (0.0075)  loss_ce_2_unscaled: 0.1071 (0.1137)  loss_bbox_2_unscaled: 0.0103 (0.0105)  loss_giou_2_unscaled: 0.2170 (0.2218)  loss_xy_2_unscaled: 0.0030 (0.0030)  loss_hw_2_unscaled: 0.0072 (0.0075)  loss_ce_3_unscaled: 0.1062 (0.1114)  loss_bbox_3_unscaled: 0.0104 (0.0105)  loss_giou_3_unscaled: 0.2213 (0.2223)  loss_xy_3_unscaled: 0.0030 (0.0030)  loss_hw_3_unscaled: 0.0071 (0.0075)  loss_ce_4_unscaled: 0.1053 (0.1107)  loss_bbox_4_unscaled: 0.0103 (0.0105)  loss_giou_4_unscaled: 0.2173 (0.2214)  loss_xy_4_unscaled: 0.0030 (0.0030)  loss_hw_4_unscaled: 0.0072 (0.0074)  loss_ce_interm_unscaled: 0.1410 (0.1390)  loss_bbox_interm_unscaled: 0.0106 (0.0107)  loss_giou_interm_unscaled: 0.2265 (0.2261)  loss_xy_interm_unscaled: 0.0031 (0.0032)  loss_hw_interm_unscaled: 0.0073 (0.0075)  time: 2.8028  data: 0.0145  max mem: 8014\n",
            "Epoch: [1]  [30/90]  eta: 0:02:48  lr: 0.000100  loss: 5.1699 (5.0657)  loss_ce: 0.2175 (0.2230)  loss_bbox: 0.0502 (0.0512)  loss_giou: 0.4196 (0.4328)  loss_ce_0: 0.2465 (0.2559)  loss_bbox_0: 0.0510 (0.0514)  loss_giou_0: 0.4269 (0.4304)  loss_ce_1: 0.2412 (0.2402)  loss_bbox_1: 0.0504 (0.0513)  loss_giou_1: 0.4230 (0.4310)  loss_ce_2: 0.2283 (0.2273)  loss_bbox_2: 0.0499 (0.0513)  loss_giou_2: 0.4238 (0.4323)  loss_ce_3: 0.2172 (0.2241)  loss_bbox_3: 0.0501 (0.0513)  loss_giou_3: 0.4313 (0.4327)  loss_ce_4: 0.2171 (0.2221)  loss_bbox_4: 0.0502 (0.0511)  loss_giou_4: 0.4216 (0.4322)  loss_ce_interm: 0.2820 (0.2765)  loss_bbox_interm: 0.0533 (0.0533)  loss_giou_interm: 0.4307 (0.4445)  loss_ce_unscaled: 0.1087 (0.1115)  loss_bbox_unscaled: 0.0100 (0.0102)  loss_giou_unscaled: 0.2098 (0.2164)  loss_xy_unscaled: 0.0028 (0.0030)  loss_hw_unscaled: 0.0071 (0.0073)  loss_ce_0_unscaled: 0.1233 (0.1279)  loss_bbox_0_unscaled: 0.0102 (0.0103)  loss_giou_0_unscaled: 0.2135 (0.2152)  loss_xy_0_unscaled: 0.0029 (0.0030)  loss_hw_0_unscaled: 0.0074 (0.0073)  loss_ce_1_unscaled: 0.1206 (0.1201)  loss_bbox_1_unscaled: 0.0101 (0.0103)  loss_giou_1_unscaled: 0.2115 (0.2155)  loss_xy_1_unscaled: 0.0028 (0.0030)  loss_hw_1_unscaled: 0.0071 (0.0073)  loss_ce_2_unscaled: 0.1142 (0.1137)  loss_bbox_2_unscaled: 0.0100 (0.0103)  loss_giou_2_unscaled: 0.2119 (0.2161)  loss_xy_2_unscaled: 0.0028 (0.0030)  loss_hw_2_unscaled: 0.0071 (0.0073)  loss_ce_3_unscaled: 0.1086 (0.1121)  loss_bbox_3_unscaled: 0.0100 (0.0103)  loss_giou_3_unscaled: 0.2157 (0.2163)  loss_xy_3_unscaled: 0.0028 (0.0030)  loss_hw_3_unscaled: 0.0070 (0.0073)  loss_ce_4_unscaled: 0.1086 (0.1110)  loss_bbox_4_unscaled: 0.0100 (0.0102)  loss_giou_4_unscaled: 0.2108 (0.2161)  loss_xy_4_unscaled: 0.0028 (0.0030)  loss_hw_4_unscaled: 0.0071 (0.0072)  loss_ce_interm_unscaled: 0.1410 (0.1382)  loss_bbox_interm_unscaled: 0.0107 (0.0107)  loss_giou_interm_unscaled: 0.2154 (0.2222)  loss_xy_interm_unscaled: 0.0031 (0.0031)  loss_hw_interm_unscaled: 0.0076 (0.0075)  time: 2.7852  data: 0.0143  max mem: 8014\n",
            "Epoch: [1]  [40/90]  eta: 0:02:20  lr: 0.000100  loss: 4.8168 (5.0257)  loss_ce: 0.2182 (0.2256)  loss_bbox: 0.0490 (0.0506)  loss_giou: 0.4025 (0.4270)  loss_ce_0: 0.2330 (0.2506)  loss_bbox_0: 0.0500 (0.0511)  loss_giou_0: 0.4137 (0.4280)  loss_ce_1: 0.2203 (0.2393)  loss_bbox_1: 0.0492 (0.0508)  loss_giou_1: 0.4076 (0.4266)  loss_ce_2: 0.2143 (0.2258)  loss_bbox_2: 0.0494 (0.0508)  loss_giou_2: 0.4080 (0.4281)  loss_ce_3: 0.2171 (0.2252)  loss_bbox_3: 0.0485 (0.0506)  loss_giou_3: 0.4043 (0.4269)  loss_ce_4: 0.2188 (0.2235)  loss_bbox_4: 0.0490 (0.0506)  loss_giou_4: 0.4054 (0.4269)  loss_ce_interm: 0.2630 (0.2750)  loss_bbox_interm: 0.0527 (0.0531)  loss_giou_interm: 0.4212 (0.4397)  loss_ce_unscaled: 0.1091 (0.1128)  loss_bbox_unscaled: 0.0098 (0.0101)  loss_giou_unscaled: 0.2013 (0.2135)  loss_xy_unscaled: 0.0029 (0.0030)  loss_hw_unscaled: 0.0069 (0.0072)  loss_ce_0_unscaled: 0.1165 (0.1253)  loss_bbox_0_unscaled: 0.0100 (0.0102)  loss_giou_0_unscaled: 0.2068 (0.2140)  loss_xy_0_unscaled: 0.0029 (0.0030)  loss_hw_0_unscaled: 0.0071 (0.0072)  loss_ce_1_unscaled: 0.1102 (0.1196)  loss_bbox_1_unscaled: 0.0098 (0.0102)  loss_giou_1_unscaled: 0.2038 (0.2133)  loss_xy_1_unscaled: 0.0028 (0.0030)  loss_hw_1_unscaled: 0.0070 (0.0072)  loss_ce_2_unscaled: 0.1072 (0.1129)  loss_bbox_2_unscaled: 0.0099 (0.0102)  loss_giou_2_unscaled: 0.2040 (0.2140)  loss_xy_2_unscaled: 0.0029 (0.0030)  loss_hw_2_unscaled: 0.0068 (0.0072)  loss_ce_3_unscaled: 0.1086 (0.1126)  loss_bbox_3_unscaled: 0.0097 (0.0101)  loss_giou_3_unscaled: 0.2022 (0.2135)  loss_xy_3_unscaled: 0.0029 (0.0030)  loss_hw_3_unscaled: 0.0070 (0.0072)  loss_ce_4_unscaled: 0.1094 (0.1118)  loss_bbox_4_unscaled: 0.0098 (0.0101)  loss_giou_4_unscaled: 0.2027 (0.2134)  loss_xy_4_unscaled: 0.0029 (0.0030)  loss_hw_4_unscaled: 0.0069 (0.0072)  loss_ce_interm_unscaled: 0.1315 (0.1375)  loss_bbox_interm_unscaled: 0.0105 (0.0106)  loss_giou_interm_unscaled: 0.2106 (0.2198)  loss_xy_interm_unscaled: 0.0031 (0.0031)  loss_hw_interm_unscaled: 0.0076 (0.0075)  time: 2.8171  data: 0.0149  max mem: 8948\n",
            "Epoch: [1]  [50/90]  eta: 0:01:51  lr: 0.000100  loss: 4.8477 (5.0559)  loss_ce: 0.2275 (0.2253)  loss_bbox: 0.0499 (0.0524)  loss_giou: 0.4158 (0.4307)  loss_ce_0: 0.2268 (0.2475)  loss_bbox_0: 0.0524 (0.0528)  loss_giou_0: 0.4282 (0.4313)  loss_ce_1: 0.2335 (0.2388)  loss_bbox_1: 0.0508 (0.0525)  loss_giou_1: 0.4219 (0.4292)  loss_ce_2: 0.2214 (0.2267)  loss_bbox_2: 0.0501 (0.0524)  loss_giou_2: 0.4142 (0.4305)  loss_ce_3: 0.2318 (0.2257)  loss_bbox_3: 0.0501 (0.0521)  loss_giou_3: 0.4058 (0.4297)  loss_ce_4: 0.2271 (0.2235)  loss_bbox_4: 0.0500 (0.0523)  loss_giou_4: 0.4111 (0.4300)  loss_ce_interm: 0.2712 (0.2767)  loss_bbox_interm: 0.0534 (0.0545)  loss_giou_interm: 0.4241 (0.4413)  loss_ce_unscaled: 0.1138 (0.1127)  loss_bbox_unscaled: 0.0100 (0.0105)  loss_giou_unscaled: 0.2079 (0.2154)  loss_xy_unscaled: 0.0031 (0.0031)  loss_hw_unscaled: 0.0071 (0.0074)  loss_ce_0_unscaled: 0.1134 (0.1237)  loss_bbox_0_unscaled: 0.0105 (0.0106)  loss_giou_0_unscaled: 0.2141 (0.2157)  loss_xy_0_unscaled: 0.0031 (0.0031)  loss_hw_0_unscaled: 0.0075 (0.0074)  loss_ce_1_unscaled: 0.1167 (0.1194)  loss_bbox_1_unscaled: 0.0102 (0.0105)  loss_giou_1_unscaled: 0.2110 (0.2146)  loss_xy_1_unscaled: 0.0030 (0.0031)  loss_hw_1_unscaled: 0.0073 (0.0074)  loss_ce_2_unscaled: 0.1107 (0.1133)  loss_bbox_2_unscaled: 0.0100 (0.0105)  loss_giou_2_unscaled: 0.2071 (0.2152)  loss_xy_2_unscaled: 0.0030 (0.0031)  loss_hw_2_unscaled: 0.0071 (0.0074)  loss_ce_3_unscaled: 0.1159 (0.1129)  loss_bbox_3_unscaled: 0.0100 (0.0104)  loss_giou_3_unscaled: 0.2029 (0.2148)  loss_xy_3_unscaled: 0.0030 (0.0031)  loss_hw_3_unscaled: 0.0070 (0.0073)  loss_ce_4_unscaled: 0.1135 (0.1117)  loss_bbox_4_unscaled: 0.0100 (0.0105)  loss_giou_4_unscaled: 0.2056 (0.2150)  loss_xy_4_unscaled: 0.0030 (0.0031)  loss_hw_4_unscaled: 0.0071 (0.0073)  loss_ce_interm_unscaled: 0.1356 (0.1383)  loss_bbox_interm_unscaled: 0.0107 (0.0109)  loss_giou_interm_unscaled: 0.2121 (0.2207)  loss_xy_interm_unscaled: 0.0032 (0.0033)  loss_hw_interm_unscaled: 0.0075 (0.0077)  time: 2.7444  data: 0.0147  max mem: 8948\n",
            "Epoch: [1]  [60/90]  eta: 0:01:23  lr: 0.000100  loss: 4.9928 (5.0473)  loss_ce: 0.2275 (0.2281)  loss_bbox: 0.0533 (0.0526)  loss_giou: 0.4167 (0.4272)  loss_ce_0: 0.2283 (0.2475)  loss_bbox_0: 0.0541 (0.0531)  loss_giou_0: 0.4303 (0.4294)  loss_ce_1: 0.2335 (0.2382)  loss_bbox_1: 0.0542 (0.0527)  loss_giou_1: 0.4217 (0.4267)  loss_ce_2: 0.2228 (0.2271)  loss_bbox_2: 0.0531 (0.0526)  loss_giou_2: 0.4194 (0.4279)  loss_ce_3: 0.2318 (0.2268)  loss_bbox_3: 0.0529 (0.0524)  loss_giou_3: 0.4203 (0.4276)  loss_ce_4: 0.2220 (0.2250)  loss_bbox_4: 0.0532 (0.0525)  loss_giou_4: 0.4198 (0.4273)  loss_ce_interm: 0.2764 (0.2760)  loss_bbox_interm: 0.0568 (0.0552)  loss_giou_interm: 0.4449 (0.4413)  loss_ce_unscaled: 0.1138 (0.1140)  loss_bbox_unscaled: 0.0107 (0.0105)  loss_giou_unscaled: 0.2084 (0.2136)  loss_xy_unscaled: 0.0033 (0.0031)  loss_hw_unscaled: 0.0075 (0.0074)  loss_ce_0_unscaled: 0.1141 (0.1237)  loss_bbox_0_unscaled: 0.0108 (0.0106)  loss_giou_0_unscaled: 0.2151 (0.2147)  loss_xy_0_unscaled: 0.0033 (0.0032)  loss_hw_0_unscaled: 0.0075 (0.0075)  loss_ce_1_unscaled: 0.1167 (0.1191)  loss_bbox_1_unscaled: 0.0108 (0.0105)  loss_giou_1_unscaled: 0.2108 (0.2134)  loss_xy_1_unscaled: 0.0033 (0.0031)  loss_hw_1_unscaled: 0.0075 (0.0074)  loss_ce_2_unscaled: 0.1114 (0.1136)  loss_bbox_2_unscaled: 0.0106 (0.0105)  loss_giou_2_unscaled: 0.2097 (0.2140)  loss_xy_2_unscaled: 0.0033 (0.0031)  loss_hw_2_unscaled: 0.0075 (0.0074)  loss_ce_3_unscaled: 0.1159 (0.1134)  loss_bbox_3_unscaled: 0.0106 (0.0105)  loss_giou_3_unscaled: 0.2102 (0.2138)  loss_xy_3_unscaled: 0.0033 (0.0031)  loss_hw_3_unscaled: 0.0074 (0.0074)  loss_ce_4_unscaled: 0.1110 (0.1125)  loss_bbox_4_unscaled: 0.0106 (0.0105)  loss_giou_4_unscaled: 0.2099 (0.2136)  loss_xy_4_unscaled: 0.0033 (0.0031)  loss_hw_4_unscaled: 0.0074 (0.0074)  loss_ce_interm_unscaled: 0.1382 (0.1380)  loss_bbox_interm_unscaled: 0.0114 (0.0110)  loss_giou_interm_unscaled: 0.2224 (0.2206)  loss_xy_interm_unscaled: 0.0035 (0.0033)  loss_hw_interm_unscaled: 0.0078 (0.0077)  time: 2.7621  data: 0.0154  max mem: 8948\n",
            "Epoch: [1]  [70/90]  eta: 0:00:56  lr: 0.000100  loss: 5.1989 (5.1087)  loss_ce: 0.2429 (0.2354)  loss_bbox: 0.0517 (0.0528)  loss_giou: 0.4149 (0.4266)  loss_ce_0: 0.2425 (0.2545)  loss_bbox_0: 0.0528 (0.0534)  loss_giou_0: 0.4230 (0.4301)  loss_ce_1: 0.2389 (0.2462)  loss_bbox_1: 0.0515 (0.0529)  loss_giou_1: 0.4217 (0.4274)  loss_ce_2: 0.2251 (0.2364)  loss_bbox_2: 0.0519 (0.0529)  loss_giou_2: 0.4202 (0.4285)  loss_ce_3: 0.2376 (0.2354)  loss_bbox_3: 0.0516 (0.0526)  loss_giou_3: 0.4203 (0.4278)  loss_ce_4: 0.2351 (0.2337)  loss_bbox_4: 0.0512 (0.0527)  loss_giou_4: 0.4163 (0.4265)  loss_ce_interm: 0.2945 (0.2852)  loss_bbox_interm: 0.0572 (0.0558)  loss_giou_interm: 0.4413 (0.4418)  loss_ce_unscaled: 0.1215 (0.1177)  loss_bbox_unscaled: 0.0103 (0.0106)  loss_giou_unscaled: 0.2074 (0.2133)  loss_xy_unscaled: 0.0031 (0.0031)  loss_hw_unscaled: 0.0073 (0.0074)  loss_ce_0_unscaled: 0.1213 (0.1273)  loss_bbox_0_unscaled: 0.0106 (0.0107)  loss_giou_0_unscaled: 0.2115 (0.2151)  loss_xy_0_unscaled: 0.0032 (0.0032)  loss_hw_0_unscaled: 0.0074 (0.0075)  loss_ce_1_unscaled: 0.1195 (0.1231)  loss_bbox_1_unscaled: 0.0103 (0.0106)  loss_giou_1_unscaled: 0.2108 (0.2137)  loss_xy_1_unscaled: 0.0031 (0.0031)  loss_hw_1_unscaled: 0.0074 (0.0074)  loss_ce_2_unscaled: 0.1126 (0.1182)  loss_bbox_2_unscaled: 0.0104 (0.0106)  loss_giou_2_unscaled: 0.2101 (0.2142)  loss_xy_2_unscaled: 0.0032 (0.0031)  loss_hw_2_unscaled: 0.0073 (0.0074)  loss_ce_3_unscaled: 0.1188 (0.1177)  loss_bbox_3_unscaled: 0.0103 (0.0105)  loss_giou_3_unscaled: 0.2102 (0.2139)  loss_xy_3_unscaled: 0.0031 (0.0031)  loss_hw_3_unscaled: 0.0075 (0.0074)  loss_ce_4_unscaled: 0.1176 (0.1169)  loss_bbox_4_unscaled: 0.0102 (0.0105)  loss_giou_4_unscaled: 0.2082 (0.2133)  loss_xy_4_unscaled: 0.0031 (0.0031)  loss_hw_4_unscaled: 0.0074 (0.0074)  loss_ce_interm_unscaled: 0.1473 (0.1426)  loss_bbox_interm_unscaled: 0.0114 (0.0112)  loss_giou_interm_unscaled: 0.2206 (0.2209)  loss_xy_interm_unscaled: 0.0033 (0.0033)  loss_hw_interm_unscaled: 0.0081 (0.0078)  time: 2.8808  data: 0.0153  max mem: 8948\n",
            "Epoch: [1]  [80/90]  eta: 0:00:28  lr: 0.000100  loss: 5.0814 (5.1003)  loss_ce: 0.2353 (0.2344)  loss_bbox: 0.0515 (0.0526)  loss_giou: 0.4270 (0.4278)  loss_ce_0: 0.2373 (0.2521)  loss_bbox_0: 0.0521 (0.0532)  loss_giou_0: 0.4321 (0.4311)  loss_ce_1: 0.2284 (0.2438)  loss_bbox_1: 0.0515 (0.0528)  loss_giou_1: 0.4405 (0.4284)  loss_ce_2: 0.2246 (0.2350)  loss_bbox_2: 0.0519 (0.0526)  loss_giou_2: 0.4315 (0.4293)  loss_ce_3: 0.2148 (0.2331)  loss_bbox_3: 0.0512 (0.0525)  loss_giou_3: 0.4341 (0.4291)  loss_ce_4: 0.2295 (0.2327)  loss_bbox_4: 0.0513 (0.0525)  loss_giou_4: 0.4244 (0.4274)  loss_ce_interm: 0.2773 (0.2835)  loss_bbox_interm: 0.0561 (0.0555)  loss_giou_interm: 0.4294 (0.4408)  loss_ce_unscaled: 0.1177 (0.1172)  loss_bbox_unscaled: 0.0103 (0.0105)  loss_giou_unscaled: 0.2135 (0.2139)  loss_xy_unscaled: 0.0031 (0.0031)  loss_hw_unscaled: 0.0071 (0.0074)  loss_ce_0_unscaled: 0.1186 (0.1261)  loss_bbox_0_unscaled: 0.0104 (0.0106)  loss_giou_0_unscaled: 0.2160 (0.2156)  loss_xy_0_unscaled: 0.0031 (0.0032)  loss_hw_0_unscaled: 0.0073 (0.0075)  loss_ce_1_unscaled: 0.1142 (0.1219)  loss_bbox_1_unscaled: 0.0103 (0.0106)  loss_giou_1_unscaled: 0.2202 (0.2142)  loss_xy_1_unscaled: 0.0031 (0.0031)  loss_hw_1_unscaled: 0.0072 (0.0074)  loss_ce_2_unscaled: 0.1123 (0.1175)  loss_bbox_2_unscaled: 0.0104 (0.0105)  loss_giou_2_unscaled: 0.2157 (0.2147)  loss_xy_2_unscaled: 0.0031 (0.0031)  loss_hw_2_unscaled: 0.0072 (0.0074)  loss_ce_3_unscaled: 0.1074 (0.1165)  loss_bbox_3_unscaled: 0.0102 (0.0105)  loss_giou_3_unscaled: 0.2170 (0.2146)  loss_xy_3_unscaled: 0.0030 (0.0031)  loss_hw_3_unscaled: 0.0071 (0.0074)  loss_ce_4_unscaled: 0.1148 (0.1164)  loss_bbox_4_unscaled: 0.0103 (0.0105)  loss_giou_4_unscaled: 0.2122 (0.2137)  loss_xy_4_unscaled: 0.0031 (0.0031)  loss_hw_4_unscaled: 0.0071 (0.0074)  loss_ce_interm_unscaled: 0.1387 (0.1418)  loss_bbox_interm_unscaled: 0.0112 (0.0111)  loss_giou_interm_unscaled: 0.2147 (0.2204)  loss_xy_interm_unscaled: 0.0031 (0.0033)  loss_hw_interm_unscaled: 0.0081 (0.0078)  time: 2.9565  data: 0.0151  max mem: 8948\n",
            "Epoch: [1]  [89/90]  eta: 0:00:02  lr: 0.000100  loss: 4.9705 (5.0676)  loss_ce: 0.2070 (0.2327)  loss_bbox: 0.0515 (0.0522)  loss_giou: 0.4331 (0.4246)  loss_ce_0: 0.2270 (0.2491)  loss_bbox_0: 0.0495 (0.0529)  loss_giou_0: 0.4415 (0.4295)  loss_ce_1: 0.2217 (0.2421)  loss_bbox_1: 0.0489 (0.0524)  loss_giou_1: 0.4327 (0.4259)  loss_ce_2: 0.2227 (0.2329)  loss_bbox_2: 0.0491 (0.0523)  loss_giou_2: 0.4317 (0.4266)  loss_ce_3: 0.2093 (0.2316)  loss_bbox_3: 0.0511 (0.0521)  loss_giou_3: 0.4310 (0.4259)  loss_ce_4: 0.2123 (0.2307)  loss_bbox_4: 0.0513 (0.0521)  loss_giou_4: 0.4350 (0.4246)  loss_ce_interm: 0.2659 (0.2829)  loss_bbox_interm: 0.0506 (0.0551)  loss_giou_interm: 0.4294 (0.4394)  loss_ce_unscaled: 0.1035 (0.1164)  loss_bbox_unscaled: 0.0103 (0.0104)  loss_giou_unscaled: 0.2165 (0.2123)  loss_xy_unscaled: 0.0030 (0.0031)  loss_hw_unscaled: 0.0071 (0.0073)  loss_ce_0_unscaled: 0.1135 (0.1245)  loss_bbox_0_unscaled: 0.0099 (0.0106)  loss_giou_0_unscaled: 0.2208 (0.2147)  loss_xy_0_unscaled: 0.0031 (0.0031)  loss_hw_0_unscaled: 0.0070 (0.0075)  loss_ce_1_unscaled: 0.1108 (0.1211)  loss_bbox_1_unscaled: 0.0098 (0.0105)  loss_giou_1_unscaled: 0.2163 (0.2129)  loss_xy_1_unscaled: 0.0029 (0.0031)  loss_hw_1_unscaled: 0.0066 (0.0074)  loss_ce_2_unscaled: 0.1113 (0.1164)  loss_bbox_2_unscaled: 0.0098 (0.0105)  loss_giou_2_unscaled: 0.2158 (0.2133)  loss_xy_2_unscaled: 0.0029 (0.0031)  loss_hw_2_unscaled: 0.0066 (0.0074)  loss_ce_3_unscaled: 0.1046 (0.1158)  loss_bbox_3_unscaled: 0.0102 (0.0104)  loss_giou_3_unscaled: 0.2155 (0.2130)  loss_xy_3_unscaled: 0.0030 (0.0031)  loss_hw_3_unscaled: 0.0070 (0.0073)  loss_ce_4_unscaled: 0.1062 (0.1154)  loss_bbox_4_unscaled: 0.0103 (0.0104)  loss_giou_4_unscaled: 0.2175 (0.2123)  loss_xy_4_unscaled: 0.0030 (0.0031)  loss_hw_4_unscaled: 0.0070 (0.0073)  loss_ce_interm_unscaled: 0.1329 (0.1414)  loss_bbox_interm_unscaled: 0.0101 (0.0110)  loss_giou_interm_unscaled: 0.2147 (0.2197)  loss_xy_interm_unscaled: 0.0032 (0.0033)  loss_hw_interm_unscaled: 0.0070 (0.0078)  time: 2.9130  data: 0.0150  max mem: 8948\n",
            "Epoch: [1] Total time: 0:04:14 (2.8308 s / it)\n",
            "Averaged stats: lr: 0.000100  loss: 4.9705 (5.0676)  loss_ce: 0.2070 (0.2327)  loss_bbox: 0.0515 (0.0522)  loss_giou: 0.4331 (0.4246)  loss_ce_0: 0.2270 (0.2491)  loss_bbox_0: 0.0495 (0.0529)  loss_giou_0: 0.4415 (0.4295)  loss_ce_1: 0.2217 (0.2421)  loss_bbox_1: 0.0489 (0.0524)  loss_giou_1: 0.4327 (0.4259)  loss_ce_2: 0.2227 (0.2329)  loss_bbox_2: 0.0491 (0.0523)  loss_giou_2: 0.4317 (0.4266)  loss_ce_3: 0.2093 (0.2316)  loss_bbox_3: 0.0511 (0.0521)  loss_giou_3: 0.4310 (0.4259)  loss_ce_4: 0.2123 (0.2307)  loss_bbox_4: 0.0513 (0.0521)  loss_giou_4: 0.4350 (0.4246)  loss_ce_interm: 0.2659 (0.2829)  loss_bbox_interm: 0.0506 (0.0551)  loss_giou_interm: 0.4294 (0.4394)  loss_ce_unscaled: 0.1035 (0.1164)  loss_bbox_unscaled: 0.0103 (0.0104)  loss_giou_unscaled: 0.2165 (0.2123)  loss_xy_unscaled: 0.0030 (0.0031)  loss_hw_unscaled: 0.0071 (0.0073)  loss_ce_0_unscaled: 0.1135 (0.1245)  loss_bbox_0_unscaled: 0.0099 (0.0106)  loss_giou_0_unscaled: 0.2208 (0.2147)  loss_xy_0_unscaled: 0.0031 (0.0031)  loss_hw_0_unscaled: 0.0070 (0.0075)  loss_ce_1_unscaled: 0.1108 (0.1211)  loss_bbox_1_unscaled: 0.0098 (0.0105)  loss_giou_1_unscaled: 0.2163 (0.2129)  loss_xy_1_unscaled: 0.0029 (0.0031)  loss_hw_1_unscaled: 0.0066 (0.0074)  loss_ce_2_unscaled: 0.1113 (0.1164)  loss_bbox_2_unscaled: 0.0098 (0.0105)  loss_giou_2_unscaled: 0.2158 (0.2133)  loss_xy_2_unscaled: 0.0029 (0.0031)  loss_hw_2_unscaled: 0.0066 (0.0074)  loss_ce_3_unscaled: 0.1046 (0.1158)  loss_bbox_3_unscaled: 0.0102 (0.0104)  loss_giou_3_unscaled: 0.2155 (0.2130)  loss_xy_3_unscaled: 0.0030 (0.0031)  loss_hw_3_unscaled: 0.0070 (0.0073)  loss_ce_4_unscaled: 0.1062 (0.1154)  loss_bbox_4_unscaled: 0.0103 (0.0104)  loss_giou_4_unscaled: 0.2175 (0.2123)  loss_xy_4_unscaled: 0.0030 (0.0031)  loss_hw_4_unscaled: 0.0070 (0.0073)  loss_ce_interm_unscaled: 0.1329 (0.1414)  loss_bbox_interm_unscaled: 0.0101 (0.0110)  loss_giou_interm_unscaled: 0.2147 (0.2197)  loss_xy_interm_unscaled: 0.0032 (0.0033)  loss_hw_interm_unscaled: 0.0070 (0.0078)\n",
            "Input text prompt: bolt . wrong direction . 1 . 2 . 3 . 4 . 5 .\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Test:  [ 0/10]  eta: 0:00:18    time: 1.8087  data: 0.8797  max mem: 8948\n",
            "Test:  [ 9/10]  eta: 0:00:01    time: 1.0006  data: 0.1052  max mem: 8948\n",
            "Test: Total time: 0:00:10 (1.0085 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.050\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.136\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.021\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.064\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.431\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.417\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.024\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.245\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.412\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.397\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.426\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch: [2]  [ 0/90]  eta: 0:06:43  lr: 0.000100  loss: 4.6861 (4.6861)  loss_ce: 0.1735 (0.1735)  loss_bbox: 0.0548 (0.0548)  loss_giou: 0.4188 (0.4188)  loss_ce_0: 0.2024 (0.2024)  loss_bbox_0: 0.0569 (0.0569)  loss_giou_0: 0.4290 (0.4290)  loss_ce_1: 0.2020 (0.2020)  loss_bbox_1: 0.0566 (0.0566)  loss_giou_1: 0.4304 (0.4304)  loss_ce_2: 0.1802 (0.1802)  loss_bbox_2: 0.0554 (0.0554)  loss_giou_2: 0.4270 (0.4270)  loss_ce_3: 0.1769 (0.1769)  loss_bbox_3: 0.0549 (0.0549)  loss_giou_3: 0.4224 (0.4224)  loss_ce_4: 0.1747 (0.1747)  loss_bbox_4: 0.0548 (0.0548)  loss_giou_4: 0.4195 (0.4195)  loss_ce_interm: 0.2153 (0.2153)  loss_bbox_interm: 0.0563 (0.0563)  loss_giou_interm: 0.4241 (0.4241)  loss_ce_unscaled: 0.0868 (0.0868)  loss_bbox_unscaled: 0.0110 (0.0110)  loss_giou_unscaled: 0.2094 (0.2094)  loss_xy_unscaled: 0.0031 (0.0031)  loss_hw_unscaled: 0.0079 (0.0079)  loss_ce_0_unscaled: 0.1012 (0.1012)  loss_bbox_0_unscaled: 0.0114 (0.0114)  loss_giou_0_unscaled: 0.2145 (0.2145)  loss_xy_0_unscaled: 0.0032 (0.0032)  loss_hw_0_unscaled: 0.0082 (0.0082)  loss_ce_1_unscaled: 0.1010 (0.1010)  loss_bbox_1_unscaled: 0.0113 (0.0113)  loss_giou_1_unscaled: 0.2152 (0.2152)  loss_xy_1_unscaled: 0.0032 (0.0032)  loss_hw_1_unscaled: 0.0081 (0.0081)  loss_ce_2_unscaled: 0.0901 (0.0901)  loss_bbox_2_unscaled: 0.0111 (0.0111)  loss_giou_2_unscaled: 0.2135 (0.2135)  loss_xy_2_unscaled: 0.0031 (0.0031)  loss_hw_2_unscaled: 0.0079 (0.0079)  loss_ce_3_unscaled: 0.0885 (0.0885)  loss_bbox_3_unscaled: 0.0110 (0.0110)  loss_giou_3_unscaled: 0.2112 (0.2112)  loss_xy_3_unscaled: 0.0031 (0.0031)  loss_hw_3_unscaled: 0.0078 (0.0078)  loss_ce_4_unscaled: 0.0873 (0.0873)  loss_bbox_4_unscaled: 0.0110 (0.0110)  loss_giou_4_unscaled: 0.2098 (0.2098)  loss_xy_4_unscaled: 0.0031 (0.0031)  loss_hw_4_unscaled: 0.0078 (0.0078)  loss_ce_interm_unscaled: 0.1077 (0.1077)  loss_bbox_interm_unscaled: 0.0113 (0.0113)  loss_giou_interm_unscaled: 0.2120 (0.2120)  loss_xy_interm_unscaled: 0.0033 (0.0033)  loss_hw_interm_unscaled: 0.0079 (0.0079)  time: 4.4870  data: 0.7736  max mem: 8948\n",
            "Epoch: [2]  [10/90]  eta: 0:04:02  lr: 0.000100  loss: 4.8588 (4.9941)  loss_ce: 0.2099 (0.2011)  loss_bbox: 0.0538 (0.0533)  loss_giou: 0.4494 (0.4452)  loss_ce_0: 0.2236 (0.2211)  loss_bbox_0: 0.0529 (0.0533)  loss_giou_0: 0.4330 (0.4447)  loss_ce_1: 0.2096 (0.2214)  loss_bbox_1: 0.0529 (0.0528)  loss_giou_1: 0.4232 (0.4368)  loss_ce_2: 0.2093 (0.2111)  loss_bbox_2: 0.0538 (0.0529)  loss_giou_2: 0.4330 (0.4400)  loss_ce_3: 0.2066 (0.2119)  loss_bbox_3: 0.0538 (0.0530)  loss_giou_3: 0.4341 (0.4391)  loss_ce_4: 0.1993 (0.1995)  loss_bbox_4: 0.0538 (0.0533)  loss_giou_4: 0.4474 (0.4445)  loss_ce_interm: 0.2583 (0.2624)  loss_bbox_interm: 0.0566 (0.0546)  loss_giou_interm: 0.4308 (0.4421)  loss_ce_unscaled: 0.1050 (0.1005)  loss_bbox_unscaled: 0.0108 (0.0107)  loss_giou_unscaled: 0.2247 (0.2226)  loss_xy_unscaled: 0.0030 (0.0030)  loss_hw_unscaled: 0.0079 (0.0077)  loss_ce_0_unscaled: 0.1118 (0.1106)  loss_bbox_0_unscaled: 0.0106 (0.0107)  loss_giou_0_unscaled: 0.2165 (0.2223)  loss_xy_0_unscaled: 0.0030 (0.0029)  loss_hw_0_unscaled: 0.0082 (0.0077)  loss_ce_1_unscaled: 0.1048 (0.1107)  loss_bbox_1_unscaled: 0.0106 (0.0106)  loss_giou_1_unscaled: 0.2116 (0.2184)  loss_xy_1_unscaled: 0.0030 (0.0029)  loss_hw_1_unscaled: 0.0081 (0.0077)  loss_ce_2_unscaled: 0.1047 (0.1055)  loss_bbox_2_unscaled: 0.0108 (0.0106)  loss_giou_2_unscaled: 0.2165 (0.2200)  loss_xy_2_unscaled: 0.0030 (0.0029)  loss_hw_2_unscaled: 0.0079 (0.0076)  loss_ce_3_unscaled: 0.1033 (0.1059)  loss_bbox_3_unscaled: 0.0108 (0.0106)  loss_giou_3_unscaled: 0.2171 (0.2195)  loss_xy_3_unscaled: 0.0030 (0.0030)  loss_hw_3_unscaled: 0.0078 (0.0076)  loss_ce_4_unscaled: 0.0996 (0.0997)  loss_bbox_4_unscaled: 0.0108 (0.0107)  loss_giou_4_unscaled: 0.2237 (0.2223)  loss_xy_4_unscaled: 0.0030 (0.0030)  loss_hw_4_unscaled: 0.0078 (0.0077)  loss_ce_interm_unscaled: 0.1292 (0.1312)  loss_bbox_interm_unscaled: 0.0113 (0.0109)  loss_giou_interm_unscaled: 0.2154 (0.2211)  loss_xy_interm_unscaled: 0.0030 (0.0030)  loss_hw_interm_unscaled: 0.0081 (0.0079)  time: 3.0257  data: 0.0852  max mem: 8948\n",
            "Epoch: [2]  [20/90]  eta: 0:03:29  lr: 0.000100  loss: 4.8588 (5.0415)  loss_ce: 0.1972 (0.2013)  loss_bbox: 0.0525 (0.0527)  loss_giou: 0.4494 (0.4504)  loss_ce_0: 0.2169 (0.2220)  loss_bbox_0: 0.0523 (0.0532)  loss_giou_0: 0.4423 (0.4513)  loss_ce_1: 0.2095 (0.2193)  loss_bbox_1: 0.0522 (0.0525)  loss_giou_1: 0.4232 (0.4444)  loss_ce_2: 0.2054 (0.2105)  loss_bbox_2: 0.0521 (0.0522)  loss_giou_2: 0.4352 (0.4446)  loss_ce_3: 0.2059 (0.2127)  loss_bbox_3: 0.0519 (0.0523)  loss_giou_3: 0.4341 (0.4452)  loss_ce_4: 0.1993 (0.2005)  loss_bbox_4: 0.0526 (0.0527)  loss_giou_4: 0.4474 (0.4505)  loss_ce_interm: 0.2605 (0.2683)  loss_bbox_interm: 0.0548 (0.0550)  loss_giou_interm: 0.4315 (0.4499)  loss_ce_unscaled: 0.0986 (0.1007)  loss_bbox_unscaled: 0.0105 (0.0105)  loss_giou_unscaled: 0.2247 (0.2252)  loss_xy_unscaled: 0.0030 (0.0030)  loss_hw_unscaled: 0.0076 (0.0076)  loss_ce_0_unscaled: 0.1084 (0.1110)  loss_bbox_0_unscaled: 0.0105 (0.0106)  loss_giou_0_unscaled: 0.2212 (0.2257)  loss_xy_0_unscaled: 0.0030 (0.0030)  loss_hw_0_unscaled: 0.0076 (0.0077)  loss_ce_1_unscaled: 0.1048 (0.1096)  loss_bbox_1_unscaled: 0.0104 (0.0105)  loss_giou_1_unscaled: 0.2116 (0.2222)  loss_xy_1_unscaled: 0.0029 (0.0029)  loss_hw_1_unscaled: 0.0077 (0.0076)  loss_ce_2_unscaled: 0.1027 (0.1053)  loss_bbox_2_unscaled: 0.0104 (0.0104)  loss_giou_2_unscaled: 0.2176 (0.2223)  loss_xy_2_unscaled: 0.0029 (0.0029)  loss_hw_2_unscaled: 0.0077 (0.0075)  loss_ce_3_unscaled: 0.1030 (0.1064)  loss_bbox_3_unscaled: 0.0104 (0.0105)  loss_giou_3_unscaled: 0.2171 (0.2226)  loss_xy_3_unscaled: 0.0030 (0.0029)  loss_hw_3_unscaled: 0.0077 (0.0075)  loss_ce_4_unscaled: 0.0996 (0.1002)  loss_bbox_4_unscaled: 0.0105 (0.0105)  loss_giou_4_unscaled: 0.2237 (0.2252)  loss_xy_4_unscaled: 0.0030 (0.0030)  loss_hw_4_unscaled: 0.0077 (0.0076)  loss_ce_interm_unscaled: 0.1303 (0.1341)  loss_bbox_interm_unscaled: 0.0110 (0.0110)  loss_giou_interm_unscaled: 0.2158 (0.2250)  loss_xy_interm_unscaled: 0.0030 (0.0030)  loss_hw_interm_unscaled: 0.0078 (0.0080)  time: 2.9161  data: 0.0153  max mem: 8948\n",
            "Epoch: [2]  [30/90]  eta: 0:02:56  lr: 0.000100  loss: 4.7188 (4.9166)  loss_ce: 0.1885 (0.1954)  loss_bbox: 0.0520 (0.0522)  loss_giou: 0.4226 (0.4382)  loss_ce_0: 0.1959 (0.2145)  loss_bbox_0: 0.0518 (0.0527)  loss_giou_0: 0.4253 (0.4418)  loss_ce_1: 0.1957 (0.2119)  loss_bbox_1: 0.0510 (0.0520)  loss_giou_1: 0.4144 (0.4348)  loss_ce_2: 0.1960 (0.2034)  loss_bbox_2: 0.0502 (0.0518)  loss_giou_2: 0.4167 (0.4349)  loss_ce_3: 0.1921 (0.2049)  loss_bbox_3: 0.0508 (0.0519)  loss_giou_3: 0.4183 (0.4346)  loss_ce_4: 0.1810 (0.1928)  loss_bbox_4: 0.0518 (0.0522)  loss_giou_4: 0.4200 (0.4390)  loss_ce_interm: 0.2670 (0.2620)  loss_bbox_interm: 0.0529 (0.0541)  loss_giou_interm: 0.4269 (0.4414)  loss_ce_unscaled: 0.0942 (0.0977)  loss_bbox_unscaled: 0.0104 (0.0104)  loss_giou_unscaled: 0.2113 (0.2191)  loss_xy_unscaled: 0.0028 (0.0029)  loss_hw_unscaled: 0.0075 (0.0075)  loss_ce_0_unscaled: 0.0980 (0.1073)  loss_bbox_0_unscaled: 0.0104 (0.0105)  loss_giou_0_unscaled: 0.2126 (0.2209)  loss_xy_0_unscaled: 0.0028 (0.0029)  loss_hw_0_unscaled: 0.0075 (0.0076)  loss_ce_1_unscaled: 0.0978 (0.1060)  loss_bbox_1_unscaled: 0.0102 (0.0104)  loss_giou_1_unscaled: 0.2072 (0.2174)  loss_xy_1_unscaled: 0.0028 (0.0029)  loss_hw_1_unscaled: 0.0075 (0.0075)  loss_ce_2_unscaled: 0.0980 (0.1017)  loss_bbox_2_unscaled: 0.0100 (0.0104)  loss_giou_2_unscaled: 0.2084 (0.2175)  loss_xy_2_unscaled: 0.0028 (0.0029)  loss_hw_2_unscaled: 0.0072 (0.0075)  loss_ce_3_unscaled: 0.0960 (0.1024)  loss_bbox_3_unscaled: 0.0102 (0.0104)  loss_giou_3_unscaled: 0.2092 (0.2173)  loss_xy_3_unscaled: 0.0028 (0.0029)  loss_hw_3_unscaled: 0.0073 (0.0075)  loss_ce_4_unscaled: 0.0905 (0.0964)  loss_bbox_4_unscaled: 0.0104 (0.0104)  loss_giou_4_unscaled: 0.2100 (0.2195)  loss_xy_4_unscaled: 0.0028 (0.0029)  loss_hw_4_unscaled: 0.0074 (0.0075)  loss_ce_interm_unscaled: 0.1335 (0.1310)  loss_bbox_interm_unscaled: 0.0106 (0.0108)  loss_giou_interm_unscaled: 0.2135 (0.2207)  loss_xy_interm_unscaled: 0.0029 (0.0030)  loss_hw_interm_unscaled: 0.0077 (0.0078)  time: 2.8969  data: 0.0152  max mem: 8948\n",
            "Epoch: [2]  [40/90]  eta: 0:02:26  lr: 0.000100  loss: 4.6763 (4.8747)  loss_ce: 0.1771 (0.1938)  loss_bbox: 0.0522 (0.0520)  loss_giou: 0.4125 (0.4336)  loss_ce_0: 0.1943 (0.2153)  loss_bbox_0: 0.0516 (0.0525)  loss_giou_0: 0.4235 (0.4356)  loss_ce_1: 0.1918 (0.2111)  loss_bbox_1: 0.0517 (0.0519)  loss_giou_1: 0.4183 (0.4306)  loss_ce_2: 0.1886 (0.2023)  loss_bbox_2: 0.0519 (0.0517)  loss_giou_2: 0.4095 (0.4309)  loss_ce_3: 0.1806 (0.2030)  loss_bbox_3: 0.0517 (0.0517)  loss_giou_3: 0.4150 (0.4298)  loss_ce_4: 0.1738 (0.1912)  loss_bbox_4: 0.0521 (0.0520)  loss_giou_4: 0.4185 (0.4344)  loss_ce_interm: 0.2427 (0.2612)  loss_bbox_interm: 0.0529 (0.0542)  loss_giou_interm: 0.4198 (0.4360)  loss_ce_unscaled: 0.0885 (0.0969)  loss_bbox_unscaled: 0.0104 (0.0104)  loss_giou_unscaled: 0.2063 (0.2168)  loss_xy_unscaled: 0.0028 (0.0030)  loss_hw_unscaled: 0.0076 (0.0074)  loss_ce_0_unscaled: 0.0971 (0.1077)  loss_bbox_0_unscaled: 0.0103 (0.0105)  loss_giou_0_unscaled: 0.2118 (0.2178)  loss_xy_0_unscaled: 0.0028 (0.0030)  loss_hw_0_unscaled: 0.0077 (0.0075)  loss_ce_1_unscaled: 0.0959 (0.1055)  loss_bbox_1_unscaled: 0.0103 (0.0104)  loss_giou_1_unscaled: 0.2092 (0.2153)  loss_xy_1_unscaled: 0.0028 (0.0029)  loss_hw_1_unscaled: 0.0076 (0.0074)  loss_ce_2_unscaled: 0.0943 (0.1012)  loss_bbox_2_unscaled: 0.0104 (0.0103)  loss_giou_2_unscaled: 0.2048 (0.2154)  loss_xy_2_unscaled: 0.0028 (0.0029)  loss_hw_2_unscaled: 0.0076 (0.0074)  loss_ce_3_unscaled: 0.0903 (0.1015)  loss_bbox_3_unscaled: 0.0103 (0.0103)  loss_giou_3_unscaled: 0.2075 (0.2149)  loss_xy_3_unscaled: 0.0028 (0.0029)  loss_hw_3_unscaled: 0.0075 (0.0074)  loss_ce_4_unscaled: 0.0869 (0.0956)  loss_bbox_4_unscaled: 0.0104 (0.0104)  loss_giou_4_unscaled: 0.2092 (0.2172)  loss_xy_4_unscaled: 0.0028 (0.0030)  loss_hw_4_unscaled: 0.0076 (0.0074)  loss_ce_interm_unscaled: 0.1213 (0.1306)  loss_bbox_interm_unscaled: 0.0106 (0.0108)  loss_giou_interm_unscaled: 0.2099 (0.2180)  loss_xy_interm_unscaled: 0.0031 (0.0031)  loss_hw_interm_unscaled: 0.0075 (0.0078)  time: 2.8618  data: 0.0170  max mem: 8948\n",
            "Epoch: [2]  [50/90]  eta: 0:01:56  lr: 0.000100  loss: 4.5166 (4.8122)  loss_ce: 0.1682 (0.1921)  loss_bbox: 0.0471 (0.0504)  loss_giou: 0.4129 (0.4255)  loss_ce_0: 0.2041 (0.2173)  loss_bbox_0: 0.0483 (0.0510)  loss_giou_0: 0.4081 (0.4276)  loss_ce_1: 0.1934 (0.2110)  loss_bbox_1: 0.0478 (0.0504)  loss_giou_1: 0.4035 (0.4237)  loss_ce_2: 0.1841 (0.2034)  loss_bbox_2: 0.0474 (0.0502)  loss_giou_2: 0.4020 (0.4236)  loss_ce_3: 0.1775 (0.2015)  loss_bbox_3: 0.0473 (0.0501)  loss_giou_3: 0.4031 (0.4227)  loss_ce_4: 0.1684 (0.1909)  loss_bbox_4: 0.0470 (0.0504)  loss_giou_4: 0.4114 (0.4262)  loss_ce_interm: 0.2406 (0.2607)  loss_bbox_interm: 0.0497 (0.0529)  loss_giou_interm: 0.4084 (0.4308)  loss_ce_unscaled: 0.0841 (0.0960)  loss_bbox_unscaled: 0.0094 (0.0101)  loss_giou_unscaled: 0.2064 (0.2127)  loss_xy_unscaled: 0.0029 (0.0029)  loss_hw_unscaled: 0.0064 (0.0072)  loss_ce_0_unscaled: 0.1020 (0.1087)  loss_bbox_0_unscaled: 0.0097 (0.0102)  loss_giou_0_unscaled: 0.2040 (0.2138)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0068 (0.0073)  loss_ce_1_unscaled: 0.0967 (0.1055)  loss_bbox_1_unscaled: 0.0096 (0.0101)  loss_giou_1_unscaled: 0.2017 (0.2119)  loss_xy_1_unscaled: 0.0029 (0.0029)  loss_hw_1_unscaled: 0.0067 (0.0072)  loss_ce_2_unscaled: 0.0921 (0.1017)  loss_bbox_2_unscaled: 0.0095 (0.0100)  loss_giou_2_unscaled: 0.2010 (0.2118)  loss_xy_2_unscaled: 0.0029 (0.0029)  loss_hw_2_unscaled: 0.0065 (0.0071)  loss_ce_3_unscaled: 0.0887 (0.1007)  loss_bbox_3_unscaled: 0.0095 (0.0100)  loss_giou_3_unscaled: 0.2016 (0.2114)  loss_xy_3_unscaled: 0.0029 (0.0029)  loss_hw_3_unscaled: 0.0065 (0.0071)  loss_ce_4_unscaled: 0.0842 (0.0954)  loss_bbox_4_unscaled: 0.0094 (0.0101)  loss_giou_4_unscaled: 0.2057 (0.2131)  loss_xy_4_unscaled: 0.0029 (0.0029)  loss_hw_4_unscaled: 0.0065 (0.0072)  loss_ce_interm_unscaled: 0.1203 (0.1303)  loss_bbox_interm_unscaled: 0.0099 (0.0106)  loss_giou_interm_unscaled: 0.2042 (0.2154)  loss_xy_interm_unscaled: 0.0030 (0.0030)  loss_hw_interm_unscaled: 0.0071 (0.0076)  time: 2.8579  data: 0.0171  max mem: 8948\n",
            "Epoch: [2]  [60/90]  eta: 0:01:27  lr: 0.000100  loss: 4.5166 (4.8224)  loss_ce: 0.1828 (0.1976)  loss_bbox: 0.0468 (0.0498)  loss_giou: 0.3958 (0.4224)  loss_ce_0: 0.2060 (0.2226)  loss_bbox_0: 0.0459 (0.0503)  loss_giou_0: 0.3811 (0.4233)  loss_ce_1: 0.1999 (0.2166)  loss_bbox_1: 0.0465 (0.0498)  loss_giou_1: 0.3889 (0.4209)  loss_ce_2: 0.1892 (0.2100)  loss_bbox_2: 0.0468 (0.0496)  loss_giou_2: 0.3905 (0.4208)  loss_ce_3: 0.1862 (0.2068)  loss_bbox_3: 0.0471 (0.0496)  loss_giou_3: 0.3906 (0.4201)  loss_ce_4: 0.1903 (0.1972)  loss_bbox_4: 0.0468 (0.0498)  loss_giou_4: 0.3933 (0.4230)  loss_ce_interm: 0.2464 (0.2636)  loss_bbox_interm: 0.0468 (0.0522)  loss_giou_interm: 0.4018 (0.4264)  loss_ce_unscaled: 0.0914 (0.0988)  loss_bbox_unscaled: 0.0094 (0.0100)  loss_giou_unscaled: 0.1979 (0.2112)  loss_xy_unscaled: 0.0028 (0.0029)  loss_hw_unscaled: 0.0064 (0.0071)  loss_ce_0_unscaled: 0.1030 (0.1113)  loss_bbox_0_unscaled: 0.0092 (0.0101)  loss_giou_0_unscaled: 0.1906 (0.2116)  loss_xy_0_unscaled: 0.0027 (0.0029)  loss_hw_0_unscaled: 0.0063 (0.0072)  loss_ce_1_unscaled: 0.0999 (0.1083)  loss_bbox_1_unscaled: 0.0093 (0.0100)  loss_giou_1_unscaled: 0.1945 (0.2105)  loss_xy_1_unscaled: 0.0028 (0.0029)  loss_hw_1_unscaled: 0.0063 (0.0071)  loss_ce_2_unscaled: 0.0946 (0.1050)  loss_bbox_2_unscaled: 0.0094 (0.0099)  loss_giou_2_unscaled: 0.1953 (0.2104)  loss_xy_2_unscaled: 0.0028 (0.0029)  loss_hw_2_unscaled: 0.0063 (0.0071)  loss_ce_3_unscaled: 0.0931 (0.1034)  loss_bbox_3_unscaled: 0.0094 (0.0099)  loss_giou_3_unscaled: 0.1953 (0.2100)  loss_xy_3_unscaled: 0.0028 (0.0029)  loss_hw_3_unscaled: 0.0064 (0.0071)  loss_ce_4_unscaled: 0.0952 (0.0986)  loss_bbox_4_unscaled: 0.0094 (0.0100)  loss_giou_4_unscaled: 0.1967 (0.2115)  loss_xy_4_unscaled: 0.0028 (0.0029)  loss_hw_4_unscaled: 0.0064 (0.0071)  loss_ce_interm_unscaled: 0.1232 (0.1318)  loss_bbox_interm_unscaled: 0.0094 (0.0104)  loss_giou_interm_unscaled: 0.2009 (0.2132)  loss_xy_interm_unscaled: 0.0029 (0.0030)  loss_hw_interm_unscaled: 0.0066 (0.0074)  time: 2.8596  data: 0.0164  max mem: 8948\n",
            "Epoch: [2]  [70/90]  eta: 0:00:58  lr: 0.000100  loss: 4.4643 (4.7573)  loss_ce: 0.1828 (0.1924)  loss_bbox: 0.0476 (0.0496)  loss_giou: 0.3893 (0.4170)  loss_ce_0: 0.2051 (0.2201)  loss_bbox_0: 0.0475 (0.0501)  loss_giou_0: 0.3853 (0.4176)  loss_ce_1: 0.2055 (0.2135)  loss_bbox_1: 0.0473 (0.0496)  loss_giou_1: 0.3816 (0.4153)  loss_ce_2: 0.1892 (0.2060)  loss_bbox_2: 0.0473 (0.0494)  loss_giou_2: 0.3900 (0.4157)  loss_ce_3: 0.1858 (0.2016)  loss_bbox_3: 0.0477 (0.0495)  loss_giou_3: 0.3910 (0.4153)  loss_ce_4: 0.1852 (0.1925)  loss_bbox_4: 0.0477 (0.0496)  loss_giou_4: 0.3928 (0.4176)  loss_ce_interm: 0.2395 (0.2600)  loss_bbox_interm: 0.0493 (0.0521)  loss_giou_interm: 0.4037 (0.4227)  loss_ce_unscaled: 0.0914 (0.0962)  loss_bbox_unscaled: 0.0095 (0.0099)  loss_giou_unscaled: 0.1946 (0.2085)  loss_xy_unscaled: 0.0026 (0.0029)  loss_hw_unscaled: 0.0068 (0.0071)  loss_ce_0_unscaled: 0.1025 (0.1100)  loss_bbox_0_unscaled: 0.0095 (0.0100)  loss_giou_0_unscaled: 0.1927 (0.2088)  loss_xy_0_unscaled: 0.0027 (0.0029)  loss_hw_0_unscaled: 0.0069 (0.0071)  loss_ce_1_unscaled: 0.1027 (0.1068)  loss_bbox_1_unscaled: 0.0095 (0.0099)  loss_giou_1_unscaled: 0.1908 (0.2077)  loss_xy_1_unscaled: 0.0027 (0.0029)  loss_hw_1_unscaled: 0.0069 (0.0071)  loss_ce_2_unscaled: 0.0946 (0.1030)  loss_bbox_2_unscaled: 0.0095 (0.0099)  loss_giou_2_unscaled: 0.1950 (0.2079)  loss_xy_2_unscaled: 0.0027 (0.0029)  loss_hw_2_unscaled: 0.0067 (0.0070)  loss_ce_3_unscaled: 0.0929 (0.1008)  loss_bbox_3_unscaled: 0.0095 (0.0099)  loss_giou_3_unscaled: 0.1955 (0.2077)  loss_xy_3_unscaled: 0.0027 (0.0029)  loss_hw_3_unscaled: 0.0068 (0.0070)  loss_ce_4_unscaled: 0.0926 (0.0963)  loss_bbox_4_unscaled: 0.0095 (0.0099)  loss_giou_4_unscaled: 0.1964 (0.2088)  loss_xy_4_unscaled: 0.0026 (0.0029)  loss_hw_4_unscaled: 0.0068 (0.0071)  loss_ce_interm_unscaled: 0.1197 (0.1300)  loss_bbox_interm_unscaled: 0.0099 (0.0104)  loss_giou_interm_unscaled: 0.2019 (0.2114)  loss_xy_interm_unscaled: 0.0029 (0.0030)  loss_hw_interm_unscaled: 0.0069 (0.0074)  time: 2.9229  data: 0.0168  max mem: 8948\n",
            "Epoch: [2]  [80/90]  eta: 0:00:28  lr: 0.000100  loss: 4.2798 (4.7056)  loss_ce: 0.1577 (0.1888)  loss_bbox: 0.0471 (0.0493)  loss_giou: 0.3889 (0.4140)  loss_ce_0: 0.1806 (0.2146)  loss_bbox_0: 0.0483 (0.0499)  loss_giou_0: 0.3940 (0.4154)  loss_ce_1: 0.1809 (0.2094)  loss_bbox_1: 0.0480 (0.0494)  loss_giou_1: 0.3816 (0.4120)  loss_ce_2: 0.1731 (0.2021)  loss_bbox_2: 0.0472 (0.0492)  loss_giou_2: 0.3862 (0.4123)  loss_ce_3: 0.1642 (0.1976)  loss_bbox_3: 0.0472 (0.0492)  loss_giou_3: 0.3910 (0.4123)  loss_ce_4: 0.1559 (0.1888)  loss_bbox_4: 0.0471 (0.0494)  loss_giou_4: 0.3890 (0.4147)  loss_ce_interm: 0.2116 (0.2551)  loss_bbox_interm: 0.0505 (0.0519)  loss_giou_interm: 0.4076 (0.4202)  loss_ce_unscaled: 0.0789 (0.0944)  loss_bbox_unscaled: 0.0094 (0.0099)  loss_giou_unscaled: 0.1944 (0.2070)  loss_xy_unscaled: 0.0026 (0.0029)  loss_hw_unscaled: 0.0068 (0.0070)  loss_ce_0_unscaled: 0.0903 (0.1073)  loss_bbox_0_unscaled: 0.0097 (0.0100)  loss_giou_0_unscaled: 0.1970 (0.2077)  loss_xy_0_unscaled: 0.0028 (0.0029)  loss_hw_0_unscaled: 0.0071 (0.0071)  loss_ce_1_unscaled: 0.0904 (0.1047)  loss_bbox_1_unscaled: 0.0096 (0.0099)  loss_giou_1_unscaled: 0.1908 (0.2060)  loss_xy_1_unscaled: 0.0027 (0.0029)  loss_hw_1_unscaled: 0.0069 (0.0070)  loss_ce_2_unscaled: 0.0865 (0.1011)  loss_bbox_2_unscaled: 0.0094 (0.0098)  loss_giou_2_unscaled: 0.1931 (0.2062)  loss_xy_2_unscaled: 0.0027 (0.0029)  loss_hw_2_unscaled: 0.0067 (0.0070)  loss_ce_3_unscaled: 0.0821 (0.0988)  loss_bbox_3_unscaled: 0.0094 (0.0098)  loss_giou_3_unscaled: 0.1955 (0.2062)  loss_xy_3_unscaled: 0.0027 (0.0029)  loss_hw_3_unscaled: 0.0068 (0.0070)  loss_ce_4_unscaled: 0.0780 (0.0944)  loss_bbox_4_unscaled: 0.0094 (0.0099)  loss_giou_4_unscaled: 0.1945 (0.2073)  loss_xy_4_unscaled: 0.0026 (0.0029)  loss_hw_4_unscaled: 0.0068 (0.0070)  loss_ce_interm_unscaled: 0.1058 (0.1275)  loss_bbox_interm_unscaled: 0.0101 (0.0104)  loss_giou_interm_unscaled: 0.2038 (0.2101)  loss_xy_interm_unscaled: 0.0029 (0.0030)  loss_hw_interm_unscaled: 0.0073 (0.0074)  time: 2.8763  data: 0.0159  max mem: 8948\n",
            "Epoch: [2]  [89/90]  eta: 0:00:02  lr: 0.000100  loss: 4.4555 (4.7245)  loss_ce: 0.1622 (0.1897)  loss_bbox: 0.0455 (0.0492)  loss_giou: 0.4080 (0.4144)  loss_ce_0: 0.1828 (0.2173)  loss_bbox_0: 0.0506 (0.0499)  loss_giou_0: 0.4067 (0.4167)  loss_ce_1: 0.1804 (0.2124)  loss_bbox_1: 0.0488 (0.0493)  loss_giou_1: 0.3998 (0.4120)  loss_ce_2: 0.1753 (0.2025)  loss_bbox_2: 0.0456 (0.0490)  loss_giou_2: 0.4027 (0.4131)  loss_ce_3: 0.1667 (0.1999)  loss_bbox_3: 0.0455 (0.0491)  loss_giou_3: 0.4050 (0.4129)  loss_ce_4: 0.1585 (0.1906)  loss_bbox_4: 0.0452 (0.0492)  loss_giou_4: 0.4064 (0.4153)  loss_ce_interm: 0.2292 (0.2589)  loss_bbox_interm: 0.0518 (0.0519)  loss_giou_interm: 0.4189 (0.4215)  loss_ce_unscaled: 0.0811 (0.0948)  loss_bbox_unscaled: 0.0091 (0.0098)  loss_giou_unscaled: 0.2040 (0.2072)  loss_xy_unscaled: 0.0029 (0.0029)  loss_hw_unscaled: 0.0064 (0.0070)  loss_ce_0_unscaled: 0.0914 (0.1087)  loss_bbox_0_unscaled: 0.0101 (0.0100)  loss_giou_0_unscaled: 0.2034 (0.2084)  loss_xy_0_unscaled: 0.0030 (0.0029)  loss_hw_0_unscaled: 0.0071 (0.0071)  loss_ce_1_unscaled: 0.0902 (0.1062)  loss_bbox_1_unscaled: 0.0098 (0.0099)  loss_giou_1_unscaled: 0.1999 (0.2060)  loss_xy_1_unscaled: 0.0029 (0.0029)  loss_hw_1_unscaled: 0.0069 (0.0070)  loss_ce_2_unscaled: 0.0876 (0.1012)  loss_bbox_2_unscaled: 0.0091 (0.0098)  loss_giou_2_unscaled: 0.2013 (0.2065)  loss_xy_2_unscaled: 0.0029 (0.0029)  loss_hw_2_unscaled: 0.0065 (0.0069)  loss_ce_3_unscaled: 0.0834 (0.0999)  loss_bbox_3_unscaled: 0.0091 (0.0098)  loss_giou_3_unscaled: 0.2025 (0.2064)  loss_xy_3_unscaled: 0.0029 (0.0029)  loss_hw_3_unscaled: 0.0065 (0.0070)  loss_ce_4_unscaled: 0.0792 (0.0953)  loss_bbox_4_unscaled: 0.0090 (0.0098)  loss_giou_4_unscaled: 0.2032 (0.2077)  loss_xy_4_unscaled: 0.0029 (0.0029)  loss_hw_4_unscaled: 0.0064 (0.0070)  loss_ce_interm_unscaled: 0.1146 (0.1294)  loss_bbox_interm_unscaled: 0.0104 (0.0104)  loss_giou_interm_unscaled: 0.2095 (0.2107)  loss_xy_interm_unscaled: 0.0032 (0.0030)  loss_hw_interm_unscaled: 0.0069 (0.0074)  time: 2.8469  data: 0.0147  max mem: 8948\n",
            "Epoch: [2] Total time: 0:04:20 (2.8998 s / it)\n",
            "Averaged stats: lr: 0.000100  loss: 4.4555 (4.7245)  loss_ce: 0.1622 (0.1897)  loss_bbox: 0.0455 (0.0492)  loss_giou: 0.4080 (0.4144)  loss_ce_0: 0.1828 (0.2173)  loss_bbox_0: 0.0506 (0.0499)  loss_giou_0: 0.4067 (0.4167)  loss_ce_1: 0.1804 (0.2124)  loss_bbox_1: 0.0488 (0.0493)  loss_giou_1: 0.3998 (0.4120)  loss_ce_2: 0.1753 (0.2025)  loss_bbox_2: 0.0456 (0.0490)  loss_giou_2: 0.4027 (0.4131)  loss_ce_3: 0.1667 (0.1999)  loss_bbox_3: 0.0455 (0.0491)  loss_giou_3: 0.4050 (0.4129)  loss_ce_4: 0.1585 (0.1906)  loss_bbox_4: 0.0452 (0.0492)  loss_giou_4: 0.4064 (0.4153)  loss_ce_interm: 0.2292 (0.2589)  loss_bbox_interm: 0.0518 (0.0519)  loss_giou_interm: 0.4189 (0.4215)  loss_ce_unscaled: 0.0811 (0.0948)  loss_bbox_unscaled: 0.0091 (0.0098)  loss_giou_unscaled: 0.2040 (0.2072)  loss_xy_unscaled: 0.0029 (0.0029)  loss_hw_unscaled: 0.0064 (0.0070)  loss_ce_0_unscaled: 0.0914 (0.1087)  loss_bbox_0_unscaled: 0.0101 (0.0100)  loss_giou_0_unscaled: 0.2034 (0.2084)  loss_xy_0_unscaled: 0.0030 (0.0029)  loss_hw_0_unscaled: 0.0071 (0.0071)  loss_ce_1_unscaled: 0.0902 (0.1062)  loss_bbox_1_unscaled: 0.0098 (0.0099)  loss_giou_1_unscaled: 0.1999 (0.2060)  loss_xy_1_unscaled: 0.0029 (0.0029)  loss_hw_1_unscaled: 0.0069 (0.0070)  loss_ce_2_unscaled: 0.0876 (0.1012)  loss_bbox_2_unscaled: 0.0091 (0.0098)  loss_giou_2_unscaled: 0.2013 (0.2065)  loss_xy_2_unscaled: 0.0029 (0.0029)  loss_hw_2_unscaled: 0.0065 (0.0069)  loss_ce_3_unscaled: 0.0834 (0.0999)  loss_bbox_3_unscaled: 0.0091 (0.0098)  loss_giou_3_unscaled: 0.2025 (0.2064)  loss_xy_3_unscaled: 0.0029 (0.0029)  loss_hw_3_unscaled: 0.0065 (0.0070)  loss_ce_4_unscaled: 0.0792 (0.0953)  loss_bbox_4_unscaled: 0.0090 (0.0098)  loss_giou_4_unscaled: 0.2032 (0.2077)  loss_xy_4_unscaled: 0.0029 (0.0029)  loss_hw_4_unscaled: 0.0064 (0.0070)  loss_ce_interm_unscaled: 0.1146 (0.1294)  loss_bbox_interm_unscaled: 0.0104 (0.0104)  loss_giou_interm_unscaled: 0.2095 (0.2107)  loss_xy_interm_unscaled: 0.0032 (0.0030)  loss_hw_interm_unscaled: 0.0069 (0.0074)\n",
            "Input text prompt: bolt . wrong direction . 1 . 2 . 3 . 4 . 5 .\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Test:  [ 0/10]  eta: 0:00:18    time: 1.8252  data: 0.8961  max mem: 8948\n",
            "Test:  [ 9/10]  eta: 0:00:01    time: 1.0304  data: 0.1035  max mem: 8948\n",
            "Test: Total time: 0:00:10 (1.0390 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.076\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.165\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.047\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.075\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.481\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.456\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.022\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.352\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.495\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.416\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.698\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.683\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch: [3]  [ 0/90]  eta: 0:05:47  lr: 0.000100  loss: 4.5620 (4.5620)  loss_ce: 0.1117 (0.1117)  loss_bbox: 0.0635 (0.0635)  loss_giou: 0.4572 (0.4572)  loss_ce_0: 0.1466 (0.1466)  loss_bbox_0: 0.0639 (0.0639)  loss_giou_0: 0.4508 (0.4508)  loss_ce_1: 0.1363 (0.1363)  loss_bbox_1: 0.0629 (0.0629)  loss_giou_1: 0.4486 (0.4486)  loss_ce_2: 0.1200 (0.1200)  loss_bbox_2: 0.0642 (0.0642)  loss_giou_2: 0.4607 (0.4607)  loss_ce_3: 0.1227 (0.1227)  loss_bbox_3: 0.0621 (0.0621)  loss_giou_3: 0.4557 (0.4557)  loss_ce_4: 0.1199 (0.1199)  loss_bbox_4: 0.0622 (0.0622)  loss_giou_4: 0.4540 (0.4540)  loss_ce_interm: 0.1712 (0.1712)  loss_bbox_interm: 0.0673 (0.0673)  loss_giou_interm: 0.4605 (0.4605)  loss_ce_unscaled: 0.0558 (0.0558)  loss_bbox_unscaled: 0.0127 (0.0127)  loss_giou_unscaled: 0.2286 (0.2286)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0099 (0.0099)  loss_ce_0_unscaled: 0.0733 (0.0733)  loss_bbox_0_unscaled: 0.0128 (0.0128)  loss_giou_0_unscaled: 0.2254 (0.2254)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0100 (0.0100)  loss_ce_1_unscaled: 0.0682 (0.0682)  loss_bbox_1_unscaled: 0.0126 (0.0126)  loss_giou_1_unscaled: 0.2243 (0.2243)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0098 (0.0098)  loss_ce_2_unscaled: 0.0600 (0.0600)  loss_bbox_2_unscaled: 0.0128 (0.0128)  loss_giou_2_unscaled: 0.2304 (0.2304)  loss_xy_2_unscaled: 0.0029 (0.0029)  loss_hw_2_unscaled: 0.0100 (0.0100)  loss_ce_3_unscaled: 0.0613 (0.0613)  loss_bbox_3_unscaled: 0.0124 (0.0124)  loss_giou_3_unscaled: 0.2279 (0.2279)  loss_xy_3_unscaled: 0.0027 (0.0027)  loss_hw_3_unscaled: 0.0097 (0.0097)  loss_ce_4_unscaled: 0.0600 (0.0600)  loss_bbox_4_unscaled: 0.0124 (0.0124)  loss_giou_4_unscaled: 0.2270 (0.2270)  loss_xy_4_unscaled: 0.0027 (0.0027)  loss_hw_4_unscaled: 0.0097 (0.0097)  loss_ce_interm_unscaled: 0.0856 (0.0856)  loss_bbox_interm_unscaled: 0.0135 (0.0135)  loss_giou_interm_unscaled: 0.2302 (0.2302)  loss_xy_interm_unscaled: 0.0028 (0.0028)  loss_hw_interm_unscaled: 0.0106 (0.0106)  time: 3.8625  data: 0.8619  max mem: 8948\n",
            "Epoch: [3]  [10/90]  eta: 0:04:18  lr: 0.000100  loss: 4.5620 (4.6602)  loss_ce: 0.1755 (0.1738)  loss_bbox: 0.0534 (0.0527)  loss_giou: 0.4086 (0.4171)  loss_ce_0: 0.1846 (0.1941)  loss_bbox_0: 0.0525 (0.0531)  loss_giou_0: 0.4234 (0.4309)  loss_ce_1: 0.1739 (0.1854)  loss_bbox_1: 0.0548 (0.0528)  loss_giou_1: 0.4204 (0.4252)  loss_ce_2: 0.1815 (0.1791)  loss_bbox_2: 0.0522 (0.0522)  loss_giou_2: 0.4099 (0.4218)  loss_ce_3: 0.1672 (0.1740)  loss_bbox_3: 0.0525 (0.0521)  loss_giou_3: 0.4097 (0.4192)  loss_ce_4: 0.1770 (0.1760)  loss_bbox_4: 0.0518 (0.0521)  loss_giou_4: 0.4088 (0.4179)  loss_ce_interm: 0.2178 (0.2356)  loss_bbox_interm: 0.0539 (0.0545)  loss_giou_interm: 0.4452 (0.4407)  loss_ce_unscaled: 0.0877 (0.0869)  loss_bbox_unscaled: 0.0107 (0.0105)  loss_giou_unscaled: 0.2043 (0.2085)  loss_xy_unscaled: 0.0028 (0.0029)  loss_hw_unscaled: 0.0073 (0.0076)  loss_ce_0_unscaled: 0.0923 (0.0970)  loss_bbox_0_unscaled: 0.0105 (0.0106)  loss_giou_0_unscaled: 0.2117 (0.2155)  loss_xy_0_unscaled: 0.0030 (0.0030)  loss_hw_0_unscaled: 0.0072 (0.0076)  loss_ce_1_unscaled: 0.0870 (0.0927)  loss_bbox_1_unscaled: 0.0110 (0.0106)  loss_giou_1_unscaled: 0.2102 (0.2126)  loss_xy_1_unscaled: 0.0029 (0.0029)  loss_hw_1_unscaled: 0.0074 (0.0076)  loss_ce_2_unscaled: 0.0907 (0.0896)  loss_bbox_2_unscaled: 0.0104 (0.0104)  loss_giou_2_unscaled: 0.2050 (0.2109)  loss_xy_2_unscaled: 0.0029 (0.0029)  loss_hw_2_unscaled: 0.0072 (0.0075)  loss_ce_3_unscaled: 0.0836 (0.0870)  loss_bbox_3_unscaled: 0.0105 (0.0104)  loss_giou_3_unscaled: 0.2048 (0.2096)  loss_xy_3_unscaled: 0.0028 (0.0029)  loss_hw_3_unscaled: 0.0072 (0.0075)  loss_ce_4_unscaled: 0.0885 (0.0880)  loss_bbox_4_unscaled: 0.0104 (0.0104)  loss_giou_4_unscaled: 0.2044 (0.2089)  loss_xy_4_unscaled: 0.0028 (0.0029)  loss_hw_4_unscaled: 0.0073 (0.0075)  loss_ce_interm_unscaled: 0.1089 (0.1178)  loss_bbox_interm_unscaled: 0.0108 (0.0109)  loss_giou_interm_unscaled: 0.2226 (0.2203)  loss_xy_interm_unscaled: 0.0029 (0.0031)  loss_hw_interm_unscaled: 0.0075 (0.0078)  time: 3.2345  data: 0.0937  max mem: 8948\n",
            "Epoch: [3]  [20/90]  eta: 0:03:33  lr: 0.000100  loss: 4.5679 (4.7156)  loss_ce: 0.1755 (0.1762)  loss_bbox: 0.0504 (0.0528)  loss_giou: 0.4086 (0.4178)  loss_ce_0: 0.1867 (0.2001)  loss_bbox_0: 0.0516 (0.0541)  loss_giou_0: 0.4234 (0.4313)  loss_ce_1: 0.1890 (0.1948)  loss_bbox_1: 0.0521 (0.0536)  loss_giou_1: 0.4160 (0.4252)  loss_ce_2: 0.1821 (0.1885)  loss_bbox_2: 0.0503 (0.0525)  loss_giou_2: 0.4061 (0.4202)  loss_ce_3: 0.1764 (0.1835)  loss_bbox_3: 0.0502 (0.0525)  loss_giou_3: 0.4061 (0.4168)  loss_ce_4: 0.1805 (0.1798)  loss_bbox_4: 0.0504 (0.0524)  loss_giou_4: 0.4088 (0.4179)  loss_ce_interm: 0.2242 (0.2410)  loss_bbox_interm: 0.0552 (0.0564)  loss_giou_interm: 0.4452 (0.4483)  loss_ce_unscaled: 0.0877 (0.0881)  loss_bbox_unscaled: 0.0101 (0.0106)  loss_giou_unscaled: 0.2043 (0.2089)  loss_xy_unscaled: 0.0030 (0.0031)  loss_hw_unscaled: 0.0072 (0.0075)  loss_ce_0_unscaled: 0.0933 (0.1001)  loss_bbox_0_unscaled: 0.0103 (0.0108)  loss_giou_0_unscaled: 0.2117 (0.2156)  loss_xy_0_unscaled: 0.0030 (0.0032)  loss_hw_0_unscaled: 0.0072 (0.0077)  loss_ce_1_unscaled: 0.0945 (0.0974)  loss_bbox_1_unscaled: 0.0104 (0.0107)  loss_giou_1_unscaled: 0.2080 (0.2126)  loss_xy_1_unscaled: 0.0030 (0.0031)  loss_hw_1_unscaled: 0.0073 (0.0076)  loss_ce_2_unscaled: 0.0911 (0.0942)  loss_bbox_2_unscaled: 0.0101 (0.0105)  loss_giou_2_unscaled: 0.2030 (0.2101)  loss_xy_2_unscaled: 0.0030 (0.0031)  loss_hw_2_unscaled: 0.0072 (0.0074)  loss_ce_3_unscaled: 0.0882 (0.0917)  loss_bbox_3_unscaled: 0.0100 (0.0105)  loss_giou_3_unscaled: 0.2030 (0.2084)  loss_xy_3_unscaled: 0.0029 (0.0031)  loss_hw_3_unscaled: 0.0072 (0.0074)  loss_ce_4_unscaled: 0.0903 (0.0899)  loss_bbox_4_unscaled: 0.0101 (0.0105)  loss_giou_4_unscaled: 0.2044 (0.2090)  loss_xy_4_unscaled: 0.0030 (0.0030)  loss_hw_4_unscaled: 0.0072 (0.0074)  loss_ce_interm_unscaled: 0.1121 (0.1205)  loss_bbox_interm_unscaled: 0.0110 (0.0113)  loss_giou_interm_unscaled: 0.2226 (0.2242)  loss_xy_interm_unscaled: 0.0032 (0.0033)  loss_hw_interm_unscaled: 0.0079 (0.0080)  time: 3.0106  data: 0.0146  max mem: 8948\n",
            "Epoch: [3]  [30/90]  eta: 0:03:02  lr: 0.000100  loss: 4.5679 (4.6939)  loss_ce: 0.1696 (0.1755)  loss_bbox: 0.0482 (0.0515)  loss_giou: 0.4194 (0.4203)  loss_ce_0: 0.1867 (0.1975)  loss_bbox_0: 0.0503 (0.0528)  loss_giou_0: 0.4362 (0.4312)  loss_ce_1: 0.1880 (0.1899)  loss_bbox_1: 0.0493 (0.0522)  loss_giou_1: 0.4222 (0.4267)  loss_ce_2: 0.1789 (0.1852)  loss_bbox_2: 0.0486 (0.0514)  loss_giou_2: 0.4183 (0.4233)  loss_ce_3: 0.1701 (0.1785)  loss_bbox_3: 0.0484 (0.0514)  loss_giou_3: 0.4176 (0.4213)  loss_ce_4: 0.1673 (0.1766)  loss_bbox_4: 0.0481 (0.0513)  loss_giou_4: 0.4185 (0.4210)  loss_ce_interm: 0.2138 (0.2323)  loss_bbox_interm: 0.0567 (0.0558)  loss_giou_interm: 0.4502 (0.4481)  loss_ce_unscaled: 0.0848 (0.0878)  loss_bbox_unscaled: 0.0096 (0.0103)  loss_giou_unscaled: 0.2097 (0.2101)  loss_xy_unscaled: 0.0029 (0.0030)  loss_hw_unscaled: 0.0067 (0.0073)  loss_ce_0_unscaled: 0.0933 (0.0987)  loss_bbox_0_unscaled: 0.0101 (0.0106)  loss_giou_0_unscaled: 0.2181 (0.2156)  loss_xy_0_unscaled: 0.0030 (0.0031)  loss_hw_0_unscaled: 0.0074 (0.0075)  loss_ce_1_unscaled: 0.0940 (0.0949)  loss_bbox_1_unscaled: 0.0099 (0.0104)  loss_giou_1_unscaled: 0.2111 (0.2133)  loss_xy_1_unscaled: 0.0029 (0.0030)  loss_hw_1_unscaled: 0.0073 (0.0074)  loss_ce_2_unscaled: 0.0894 (0.0926)  loss_bbox_2_unscaled: 0.0097 (0.0103)  loss_giou_2_unscaled: 0.2091 (0.2116)  loss_xy_2_unscaled: 0.0030 (0.0030)  loss_hw_2_unscaled: 0.0069 (0.0073)  loss_ce_3_unscaled: 0.0851 (0.0893)  loss_bbox_3_unscaled: 0.0097 (0.0103)  loss_giou_3_unscaled: 0.2088 (0.2107)  loss_xy_3_unscaled: 0.0029 (0.0030)  loss_hw_3_unscaled: 0.0069 (0.0073)  loss_ce_4_unscaled: 0.0837 (0.0883)  loss_bbox_4_unscaled: 0.0096 (0.0103)  loss_giou_4_unscaled: 0.2093 (0.2105)  loss_xy_4_unscaled: 0.0029 (0.0030)  loss_hw_4_unscaled: 0.0068 (0.0073)  loss_ce_interm_unscaled: 0.1069 (0.1162)  loss_bbox_interm_unscaled: 0.0113 (0.0112)  loss_giou_interm_unscaled: 0.2251 (0.2241)  loss_xy_interm_unscaled: 0.0032 (0.0032)  loss_hw_interm_unscaled: 0.0079 (0.0080)  time: 2.9387  data: 0.0141  max mem: 8948\n",
            "Epoch: [3]  [40/90]  eta: 0:02:31  lr: 0.000100  loss: 4.5453 (4.6722)  loss_ce: 0.1689 (0.1783)  loss_bbox: 0.0478 (0.0508)  loss_giou: 0.4194 (0.4175)  loss_ce_0: 0.1916 (0.1962)  loss_bbox_0: 0.0470 (0.0518)  loss_giou_0: 0.4273 (0.4292)  loss_ce_1: 0.1803 (0.1900)  loss_bbox_1: 0.0485 (0.0515)  loss_giou_1: 0.4222 (0.4246)  loss_ce_2: 0.1862 (0.1871)  loss_bbox_2: 0.0468 (0.0506)  loss_giou_2: 0.4173 (0.4193)  loss_ce_3: 0.1623 (0.1760)  loss_bbox_3: 0.0477 (0.0508)  loss_giou_3: 0.4185 (0.4206)  loss_ce_4: 0.1639 (0.1747)  loss_bbox_4: 0.0479 (0.0507)  loss_giou_4: 0.4248 (0.4199)  loss_ce_interm: 0.2054 (0.2304)  loss_bbox_interm: 0.0532 (0.0552)  loss_giou_interm: 0.4502 (0.4470)  loss_ce_unscaled: 0.0844 (0.0892)  loss_bbox_unscaled: 0.0096 (0.0102)  loss_giou_unscaled: 0.2097 (0.2088)  loss_xy_unscaled: 0.0027 (0.0029)  loss_hw_unscaled: 0.0066 (0.0072)  loss_ce_0_unscaled: 0.0958 (0.0981)  loss_bbox_0_unscaled: 0.0094 (0.0104)  loss_giou_0_unscaled: 0.2136 (0.2146)  loss_xy_0_unscaled: 0.0028 (0.0030)  loss_hw_0_unscaled: 0.0067 (0.0074)  loss_ce_1_unscaled: 0.0901 (0.0950)  loss_bbox_1_unscaled: 0.0097 (0.0103)  loss_giou_1_unscaled: 0.2111 (0.2123)  loss_xy_1_unscaled: 0.0027 (0.0030)  loss_hw_1_unscaled: 0.0067 (0.0073)  loss_ce_2_unscaled: 0.0931 (0.0935)  loss_bbox_2_unscaled: 0.0094 (0.0101)  loss_giou_2_unscaled: 0.2086 (0.2096)  loss_xy_2_unscaled: 0.0027 (0.0029)  loss_hw_2_unscaled: 0.0065 (0.0072)  loss_ce_3_unscaled: 0.0811 (0.0880)  loss_bbox_3_unscaled: 0.0095 (0.0102)  loss_giou_3_unscaled: 0.2092 (0.2103)  loss_xy_3_unscaled: 0.0027 (0.0029)  loss_hw_3_unscaled: 0.0065 (0.0072)  loss_ce_4_unscaled: 0.0820 (0.0873)  loss_bbox_4_unscaled: 0.0096 (0.0101)  loss_giou_4_unscaled: 0.2124 (0.2099)  loss_xy_4_unscaled: 0.0027 (0.0029)  loss_hw_4_unscaled: 0.0068 (0.0072)  loss_ce_interm_unscaled: 0.1027 (0.1152)  loss_bbox_interm_unscaled: 0.0106 (0.0110)  loss_giou_interm_unscaled: 0.2251 (0.2235)  loss_xy_interm_unscaled: 0.0031 (0.0032)  loss_hw_interm_unscaled: 0.0077 (0.0079)  time: 2.9978  data: 0.0169  max mem: 8948\n",
            "Epoch: [3]  [50/90]  eta: 0:01:57  lr: 0.000100  loss: 4.5204 (4.6583)  loss_ce: 0.1650 (0.1781)  loss_bbox: 0.0494 (0.0514)  loss_giou: 0.4103 (0.4164)  loss_ce_0: 0.1600 (0.1939)  loss_bbox_0: 0.0497 (0.0525)  loss_giou_0: 0.4116 (0.4283)  loss_ce_1: 0.1606 (0.1879)  loss_bbox_1: 0.0498 (0.0520)  loss_giou_1: 0.4095 (0.4232)  loss_ce_2: 0.1648 (0.1864)  loss_bbox_2: 0.0495 (0.0513)  loss_giou_2: 0.4055 (0.4185)  loss_ce_3: 0.1466 (0.1761)  loss_bbox_3: 0.0494 (0.0515)  loss_giou_3: 0.4136 (0.4195)  loss_ce_4: 0.1422 (0.1738)  loss_bbox_4: 0.0495 (0.0514)  loss_giou_4: 0.4102 (0.4188)  loss_ce_interm: 0.1993 (0.2284)  loss_bbox_interm: 0.0537 (0.0554)  loss_giou_interm: 0.4331 (0.4437)  loss_ce_unscaled: 0.0825 (0.0890)  loss_bbox_unscaled: 0.0099 (0.0103)  loss_giou_unscaled: 0.2051 (0.2082)  loss_xy_unscaled: 0.0029 (0.0030)  loss_hw_unscaled: 0.0068 (0.0073)  loss_ce_0_unscaled: 0.0800 (0.0970)  loss_bbox_0_unscaled: 0.0099 (0.0105)  loss_giou_0_unscaled: 0.2058 (0.2142)  loss_xy_0_unscaled: 0.0030 (0.0030)  loss_hw_0_unscaled: 0.0072 (0.0075)  loss_ce_1_unscaled: 0.0803 (0.0940)  loss_bbox_1_unscaled: 0.0100 (0.0104)  loss_giou_1_unscaled: 0.2048 (0.2116)  loss_xy_1_unscaled: 0.0029 (0.0030)  loss_hw_1_unscaled: 0.0071 (0.0074)  loss_ce_2_unscaled: 0.0824 (0.0932)  loss_bbox_2_unscaled: 0.0099 (0.0103)  loss_giou_2_unscaled: 0.2028 (0.2092)  loss_xy_2_unscaled: 0.0028 (0.0030)  loss_hw_2_unscaled: 0.0067 (0.0073)  loss_ce_3_unscaled: 0.0733 (0.0881)  loss_bbox_3_unscaled: 0.0099 (0.0103)  loss_giou_3_unscaled: 0.2068 (0.2097)  loss_xy_3_unscaled: 0.0029 (0.0030)  loss_hw_3_unscaled: 0.0071 (0.0073)  loss_ce_4_unscaled: 0.0711 (0.0869)  loss_bbox_4_unscaled: 0.0099 (0.0103)  loss_giou_4_unscaled: 0.2051 (0.2094)  loss_xy_4_unscaled: 0.0029 (0.0030)  loss_hw_4_unscaled: 0.0068 (0.0073)  loss_ce_interm_unscaled: 0.0996 (0.1142)  loss_bbox_interm_unscaled: 0.0107 (0.0111)  loss_giou_interm_unscaled: 0.2166 (0.2218)  loss_xy_interm_unscaled: 0.0031 (0.0032)  loss_hw_interm_unscaled: 0.0077 (0.0079)  time: 2.8033  data: 0.0164  max mem: 8948\n",
            "Epoch: [3]  [60/90]  eta: 0:01:27  lr: 0.000100  loss: 4.3354 (4.6083)  loss_ce: 0.1307 (0.1693)  loss_bbox: 0.0497 (0.0514)  loss_giou: 0.4048 (0.4175)  loss_ce_0: 0.1581 (0.1917)  loss_bbox_0: 0.0529 (0.0523)  loss_giou_0: 0.4046 (0.4253)  loss_ce_1: 0.1338 (0.1811)  loss_bbox_1: 0.0532 (0.0521)  loss_giou_1: 0.4095 (0.4232)  loss_ce_2: 0.1425 (0.1793)  loss_bbox_2: 0.0500 (0.0513)  loss_giou_2: 0.4122 (0.4184)  loss_ce_3: 0.1269 (0.1684)  loss_bbox_3: 0.0514 (0.0515)  loss_giou_3: 0.4130 (0.4200)  loss_ce_4: 0.1222 (0.1660)  loss_bbox_4: 0.0497 (0.0513)  loss_giou_4: 0.4060 (0.4193)  loss_ce_interm: 0.1942 (0.2236)  loss_bbox_interm: 0.0536 (0.0552)  loss_giou_interm: 0.4138 (0.4399)  loss_ce_unscaled: 0.0653 (0.0847)  loss_bbox_unscaled: 0.0099 (0.0103)  loss_giou_unscaled: 0.2024 (0.2088)  loss_xy_unscaled: 0.0029 (0.0030)  loss_hw_unscaled: 0.0071 (0.0073)  loss_ce_0_unscaled: 0.0791 (0.0959)  loss_bbox_0_unscaled: 0.0106 (0.0105)  loss_giou_0_unscaled: 0.2023 (0.2126)  loss_xy_0_unscaled: 0.0030 (0.0030)  loss_hw_0_unscaled: 0.0072 (0.0074)  loss_ce_1_unscaled: 0.0669 (0.0906)  loss_bbox_1_unscaled: 0.0106 (0.0104)  loss_giou_1_unscaled: 0.2048 (0.2116)  loss_xy_1_unscaled: 0.0029 (0.0030)  loss_hw_1_unscaled: 0.0071 (0.0074)  loss_ce_2_unscaled: 0.0713 (0.0897)  loss_bbox_2_unscaled: 0.0100 (0.0103)  loss_giou_2_unscaled: 0.2061 (0.2092)  loss_xy_2_unscaled: 0.0029 (0.0030)  loss_hw_2_unscaled: 0.0071 (0.0073)  loss_ce_3_unscaled: 0.0634 (0.0842)  loss_bbox_3_unscaled: 0.0103 (0.0103)  loss_giou_3_unscaled: 0.2065 (0.2100)  loss_xy_3_unscaled: 0.0029 (0.0030)  loss_hw_3_unscaled: 0.0072 (0.0073)  loss_ce_4_unscaled: 0.0611 (0.0830)  loss_bbox_4_unscaled: 0.0099 (0.0103)  loss_giou_4_unscaled: 0.2030 (0.2097)  loss_xy_4_unscaled: 0.0030 (0.0030)  loss_hw_4_unscaled: 0.0071 (0.0073)  loss_ce_interm_unscaled: 0.0971 (0.1118)  loss_bbox_interm_unscaled: 0.0107 (0.0110)  loss_giou_interm_unscaled: 0.2069 (0.2200)  loss_xy_interm_unscaled: 0.0030 (0.0031)  loss_hw_interm_unscaled: 0.0075 (0.0079)  time: 2.6679  data: 0.0148  max mem: 8948\n",
            "Epoch: [3]  [70/90]  eta: 0:00:58  lr: 0.000100  loss: 4.1694 (4.5752)  loss_ce: 0.1189 (0.1626)  loss_bbox: 0.0457 (0.0515)  loss_giou: 0.4114 (0.4192)  loss_ce_0: 0.1576 (0.1855)  loss_bbox_0: 0.0467 (0.0526)  loss_giou_0: 0.4063 (0.4269)  loss_ce_1: 0.1299 (0.1747)  loss_bbox_1: 0.0463 (0.0522)  loss_giou_1: 0.4127 (0.4236)  loss_ce_2: 0.1226 (0.1717)  loss_bbox_2: 0.0460 (0.0515)  loss_giou_2: 0.4122 (0.4198)  loss_ce_3: 0.1184 (0.1618)  loss_bbox_3: 0.0461 (0.0516)  loss_giou_3: 0.4130 (0.4212)  loss_ce_4: 0.1159 (0.1587)  loss_bbox_4: 0.0456 (0.0515)  loss_giou_4: 0.4109 (0.4212)  loss_ce_interm: 0.2023 (0.2191)  loss_bbox_interm: 0.0502 (0.0556)  loss_giou_interm: 0.4154 (0.4427)  loss_ce_unscaled: 0.0594 (0.0813)  loss_bbox_unscaled: 0.0091 (0.0103)  loss_giou_unscaled: 0.2057 (0.2096)  loss_xy_unscaled: 0.0028 (0.0030)  loss_hw_unscaled: 0.0063 (0.0073)  loss_ce_0_unscaled: 0.0788 (0.0927)  loss_bbox_0_unscaled: 0.0093 (0.0105)  loss_giou_0_unscaled: 0.2032 (0.2134)  loss_xy_0_unscaled: 0.0028 (0.0030)  loss_hw_0_unscaled: 0.0066 (0.0075)  loss_ce_1_unscaled: 0.0650 (0.0874)  loss_bbox_1_unscaled: 0.0093 (0.0104)  loss_giou_1_unscaled: 0.2063 (0.2118)  loss_xy_1_unscaled: 0.0028 (0.0030)  loss_hw_1_unscaled: 0.0064 (0.0074)  loss_ce_2_unscaled: 0.0613 (0.0858)  loss_bbox_2_unscaled: 0.0092 (0.0103)  loss_giou_2_unscaled: 0.2061 (0.2099)  loss_xy_2_unscaled: 0.0028 (0.0030)  loss_hw_2_unscaled: 0.0064 (0.0073)  loss_ce_3_unscaled: 0.0592 (0.0809)  loss_bbox_3_unscaled: 0.0092 (0.0103)  loss_giou_3_unscaled: 0.2065 (0.2106)  loss_xy_3_unscaled: 0.0028 (0.0030)  loss_hw_3_unscaled: 0.0064 (0.0073)  loss_ce_4_unscaled: 0.0579 (0.0794)  loss_bbox_4_unscaled: 0.0091 (0.0103)  loss_giou_4_unscaled: 0.2055 (0.2106)  loss_xy_4_unscaled: 0.0028 (0.0030)  loss_hw_4_unscaled: 0.0064 (0.0073)  loss_ce_interm_unscaled: 0.1012 (0.1095)  loss_bbox_interm_unscaled: 0.0100 (0.0111)  loss_giou_interm_unscaled: 0.2077 (0.2214)  loss_xy_interm_unscaled: 0.0029 (0.0032)  loss_hw_interm_unscaled: 0.0071 (0.0080)  time: 2.7942  data: 0.0150  max mem: 8948\n",
            "Epoch: [3]  [80/90]  eta: 0:00:29  lr: 0.000100  loss: 4.2142 (4.5433)  loss_ce: 0.1188 (0.1590)  loss_bbox: 0.0505 (0.0519)  loss_giou: 0.4168 (0.4176)  loss_ce_0: 0.1455 (0.1818)  loss_bbox_0: 0.0515 (0.0530)  loss_giou_0: 0.4147 (0.4253)  loss_ce_1: 0.1350 (0.1718)  loss_bbox_1: 0.0505 (0.0524)  loss_giou_1: 0.4148 (0.4217)  loss_ce_2: 0.1303 (0.1679)  loss_bbox_2: 0.0505 (0.0518)  loss_giou_2: 0.4108 (0.4182)  loss_ce_3: 0.1198 (0.1590)  loss_bbox_3: 0.0503 (0.0519)  loss_giou_3: 0.4199 (0.4192)  loss_ce_4: 0.1145 (0.1549)  loss_bbox_4: 0.0505 (0.0519)  loss_giou_4: 0.4252 (0.4198)  loss_ce_interm: 0.1978 (0.2163)  loss_bbox_interm: 0.0546 (0.0561)  loss_giou_interm: 0.4302 (0.4416)  loss_ce_unscaled: 0.0594 (0.0795)  loss_bbox_unscaled: 0.0101 (0.0104)  loss_giou_unscaled: 0.2084 (0.2088)  loss_xy_unscaled: 0.0028 (0.0030)  loss_hw_unscaled: 0.0068 (0.0074)  loss_ce_0_unscaled: 0.0727 (0.0909)  loss_bbox_0_unscaled: 0.0103 (0.0106)  loss_giou_0_unscaled: 0.2074 (0.2126)  loss_xy_0_unscaled: 0.0028 (0.0030)  loss_hw_0_unscaled: 0.0072 (0.0076)  loss_ce_1_unscaled: 0.0675 (0.0859)  loss_bbox_1_unscaled: 0.0101 (0.0105)  loss_giou_1_unscaled: 0.2074 (0.2109)  loss_xy_1_unscaled: 0.0028 (0.0030)  loss_hw_1_unscaled: 0.0072 (0.0075)  loss_ce_2_unscaled: 0.0652 (0.0840)  loss_bbox_2_unscaled: 0.0101 (0.0104)  loss_giou_2_unscaled: 0.2054 (0.2091)  loss_xy_2_unscaled: 0.0028 (0.0030)  loss_hw_2_unscaled: 0.0069 (0.0074)  loss_ce_3_unscaled: 0.0599 (0.0795)  loss_bbox_3_unscaled: 0.0101 (0.0104)  loss_giou_3_unscaled: 0.2099 (0.2096)  loss_xy_3_unscaled: 0.0028 (0.0030)  loss_hw_3_unscaled: 0.0069 (0.0074)  loss_ce_4_unscaled: 0.0573 (0.0774)  loss_bbox_4_unscaled: 0.0101 (0.0104)  loss_giou_4_unscaled: 0.2126 (0.2099)  loss_xy_4_unscaled: 0.0028 (0.0030)  loss_hw_4_unscaled: 0.0068 (0.0074)  loss_ce_interm_unscaled: 0.0989 (0.1082)  loss_bbox_interm_unscaled: 0.0109 (0.0112)  loss_giou_interm_unscaled: 0.2151 (0.2208)  loss_xy_interm_unscaled: 0.0031 (0.0032)  loss_hw_interm_unscaled: 0.0079 (0.0080)  time: 2.9508  data: 0.0157  max mem: 8948\n",
            "Epoch: [3]  [89/90]  eta: 0:00:02  lr: 0.000100  loss: 4.3106 (4.5390)  loss_ce: 0.1293 (0.1574)  loss_bbox: 0.0499 (0.0514)  loss_giou: 0.4222 (0.4181)  loss_ce_0: 0.1571 (0.1821)  loss_bbox_0: 0.0515 (0.0525)  loss_giou_0: 0.4218 (0.4259)  loss_ce_1: 0.1523 (0.1712)  loss_bbox_1: 0.0505 (0.0520)  loss_giou_1: 0.4218 (0.4226)  loss_ce_2: 0.1303 (0.1663)  loss_bbox_2: 0.0505 (0.0513)  loss_giou_2: 0.4224 (0.4191)  loss_ce_3: 0.1404 (0.1580)  loss_bbox_3: 0.0503 (0.0514)  loss_giou_3: 0.4191 (0.4197)  loss_ce_4: 0.1287 (0.1545)  loss_bbox_4: 0.0499 (0.0514)  loss_giou_4: 0.4184 (0.4200)  loss_ce_interm: 0.2133 (0.2170)  loss_bbox_interm: 0.0543 (0.0554)  loss_giou_interm: 0.4355 (0.4414)  loss_ce_unscaled: 0.0646 (0.0787)  loss_bbox_unscaled: 0.0100 (0.0103)  loss_giou_unscaled: 0.2111 (0.2091)  loss_xy_unscaled: 0.0027 (0.0030)  loss_hw_unscaled: 0.0073 (0.0073)  loss_ce_0_unscaled: 0.0785 (0.0910)  loss_bbox_0_unscaled: 0.0103 (0.0105)  loss_giou_0_unscaled: 0.2109 (0.2129)  loss_xy_0_unscaled: 0.0027 (0.0030)  loss_hw_0_unscaled: 0.0074 (0.0075)  loss_ce_1_unscaled: 0.0762 (0.0856)  loss_bbox_1_unscaled: 0.0101 (0.0104)  loss_giou_1_unscaled: 0.2109 (0.2113)  loss_xy_1_unscaled: 0.0027 (0.0030)  loss_hw_1_unscaled: 0.0073 (0.0074)  loss_ce_2_unscaled: 0.0652 (0.0831)  loss_bbox_2_unscaled: 0.0101 (0.0103)  loss_giou_2_unscaled: 0.2112 (0.2095)  loss_xy_2_unscaled: 0.0027 (0.0030)  loss_hw_2_unscaled: 0.0072 (0.0073)  loss_ce_3_unscaled: 0.0702 (0.0790)  loss_bbox_3_unscaled: 0.0101 (0.0103)  loss_giou_3_unscaled: 0.2096 (0.2098)  loss_xy_3_unscaled: 0.0027 (0.0030)  loss_hw_3_unscaled: 0.0073 (0.0073)  loss_ce_4_unscaled: 0.0643 (0.0773)  loss_bbox_4_unscaled: 0.0100 (0.0103)  loss_giou_4_unscaled: 0.2092 (0.2100)  loss_xy_4_unscaled: 0.0028 (0.0030)  loss_hw_4_unscaled: 0.0073 (0.0073)  loss_ce_interm_unscaled: 0.1066 (0.1085)  loss_bbox_interm_unscaled: 0.0109 (0.0111)  loss_giou_interm_unscaled: 0.2178 (0.2207)  loss_xy_interm_unscaled: 0.0031 (0.0032)  loss_hw_interm_unscaled: 0.0075 (0.0079)  time: 2.9007  data: 0.0151  max mem: 8948\n",
            "Epoch: [3] Total time: 0:04:21 (2.9051 s / it)\n",
            "Averaged stats: lr: 0.000100  loss: 4.3106 (4.5390)  loss_ce: 0.1293 (0.1574)  loss_bbox: 0.0499 (0.0514)  loss_giou: 0.4222 (0.4181)  loss_ce_0: 0.1571 (0.1821)  loss_bbox_0: 0.0515 (0.0525)  loss_giou_0: 0.4218 (0.4259)  loss_ce_1: 0.1523 (0.1712)  loss_bbox_1: 0.0505 (0.0520)  loss_giou_1: 0.4218 (0.4226)  loss_ce_2: 0.1303 (0.1663)  loss_bbox_2: 0.0505 (0.0513)  loss_giou_2: 0.4224 (0.4191)  loss_ce_3: 0.1404 (0.1580)  loss_bbox_3: 0.0503 (0.0514)  loss_giou_3: 0.4191 (0.4197)  loss_ce_4: 0.1287 (0.1545)  loss_bbox_4: 0.0499 (0.0514)  loss_giou_4: 0.4184 (0.4200)  loss_ce_interm: 0.2133 (0.2170)  loss_bbox_interm: 0.0543 (0.0554)  loss_giou_interm: 0.4355 (0.4414)  loss_ce_unscaled: 0.0646 (0.0787)  loss_bbox_unscaled: 0.0100 (0.0103)  loss_giou_unscaled: 0.2111 (0.2091)  loss_xy_unscaled: 0.0027 (0.0030)  loss_hw_unscaled: 0.0073 (0.0073)  loss_ce_0_unscaled: 0.0785 (0.0910)  loss_bbox_0_unscaled: 0.0103 (0.0105)  loss_giou_0_unscaled: 0.2109 (0.2129)  loss_xy_0_unscaled: 0.0027 (0.0030)  loss_hw_0_unscaled: 0.0074 (0.0075)  loss_ce_1_unscaled: 0.0762 (0.0856)  loss_bbox_1_unscaled: 0.0101 (0.0104)  loss_giou_1_unscaled: 0.2109 (0.2113)  loss_xy_1_unscaled: 0.0027 (0.0030)  loss_hw_1_unscaled: 0.0073 (0.0074)  loss_ce_2_unscaled: 0.0652 (0.0831)  loss_bbox_2_unscaled: 0.0101 (0.0103)  loss_giou_2_unscaled: 0.2112 (0.2095)  loss_xy_2_unscaled: 0.0027 (0.0030)  loss_hw_2_unscaled: 0.0072 (0.0073)  loss_ce_3_unscaled: 0.0702 (0.0790)  loss_bbox_3_unscaled: 0.0101 (0.0103)  loss_giou_3_unscaled: 0.2096 (0.2098)  loss_xy_3_unscaled: 0.0027 (0.0030)  loss_hw_3_unscaled: 0.0073 (0.0073)  loss_ce_4_unscaled: 0.0643 (0.0773)  loss_bbox_4_unscaled: 0.0100 (0.0103)  loss_giou_4_unscaled: 0.2092 (0.2100)  loss_xy_4_unscaled: 0.0028 (0.0030)  loss_hw_4_unscaled: 0.0073 (0.0073)  loss_ce_interm_unscaled: 0.1066 (0.1085)  loss_bbox_interm_unscaled: 0.0109 (0.0111)  loss_giou_interm_unscaled: 0.2178 (0.2207)  loss_xy_interm_unscaled: 0.0031 (0.0032)  loss_hw_interm_unscaled: 0.0075 (0.0079)\n",
            "Input text prompt: bolt . wrong direction . 1 . 2 . 3 . 4 . 5 .\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Test:  [ 0/10]  eta: 0:00:17    time: 1.7186  data: 0.7775  max mem: 8948\n",
            "Test:  [ 9/10]  eta: 0:00:01    time: 1.0200  data: 0.0936  max mem: 8948\n",
            "Test: Total time: 0:00:10 (1.0283 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.040\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.114\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.023\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.391\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.277\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.015\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.211\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.431\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.306\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.757\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.706\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch: [4]  [ 0/90]  eta: 0:05:53  lr: 0.000010  loss: 4.4247 (4.4247)  loss_ce: 0.1150 (0.1150)  loss_bbox: 0.0564 (0.0564)  loss_giou: 0.4431 (0.4431)  loss_ce_0: 0.1607 (0.1607)  loss_bbox_0: 0.0560 (0.0560)  loss_giou_0: 0.4231 (0.4231)  loss_ce_1: 0.1543 (0.1543)  loss_bbox_1: 0.0550 (0.0550)  loss_giou_1: 0.4301 (0.4301)  loss_ce_2: 0.1321 (0.1321)  loss_bbox_2: 0.0568 (0.0568)  loss_giou_2: 0.4322 (0.4322)  loss_ce_3: 0.1330 (0.1330)  loss_bbox_3: 0.0563 (0.0563)  loss_giou_3: 0.4381 (0.4381)  loss_ce_4: 0.1226 (0.1226)  loss_bbox_4: 0.0565 (0.0565)  loss_giou_4: 0.4448 (0.4448)  loss_ce_interm: 0.2196 (0.2196)  loss_bbox_interm: 0.0506 (0.0506)  loss_giou_interm: 0.3883 (0.3883)  loss_ce_unscaled: 0.0575 (0.0575)  loss_bbox_unscaled: 0.0113 (0.0113)  loss_giou_unscaled: 0.2216 (0.2216)  loss_xy_unscaled: 0.0035 (0.0035)  loss_hw_unscaled: 0.0078 (0.0078)  loss_ce_0_unscaled: 0.0803 (0.0803)  loss_bbox_0_unscaled: 0.0112 (0.0112)  loss_giou_0_unscaled: 0.2116 (0.2116)  loss_xy_0_unscaled: 0.0030 (0.0030)  loss_hw_0_unscaled: 0.0082 (0.0082)  loss_ce_1_unscaled: 0.0772 (0.0772)  loss_bbox_1_unscaled: 0.0110 (0.0110)  loss_giou_1_unscaled: 0.2150 (0.2150)  loss_xy_1_unscaled: 0.0032 (0.0032)  loss_hw_1_unscaled: 0.0078 (0.0078)  loss_ce_2_unscaled: 0.0661 (0.0661)  loss_bbox_2_unscaled: 0.0114 (0.0114)  loss_giou_2_unscaled: 0.2161 (0.2161)  loss_xy_2_unscaled: 0.0032 (0.0032)  loss_hw_2_unscaled: 0.0082 (0.0082)  loss_ce_3_unscaled: 0.0665 (0.0665)  loss_bbox_3_unscaled: 0.0113 (0.0113)  loss_giou_3_unscaled: 0.2190 (0.2190)  loss_xy_3_unscaled: 0.0035 (0.0035)  loss_hw_3_unscaled: 0.0078 (0.0078)  loss_ce_4_unscaled: 0.0613 (0.0613)  loss_bbox_4_unscaled: 0.0113 (0.0113)  loss_giou_4_unscaled: 0.2224 (0.2224)  loss_xy_4_unscaled: 0.0035 (0.0035)  loss_hw_4_unscaled: 0.0078 (0.0078)  loss_ce_interm_unscaled: 0.1098 (0.1098)  loss_bbox_interm_unscaled: 0.0101 (0.0101)  loss_giou_interm_unscaled: 0.1942 (0.1942)  loss_xy_interm_unscaled: 0.0029 (0.0029)  loss_hw_interm_unscaled: 0.0072 (0.0072)  time: 3.9248  data: 0.6723  max mem: 8948\n",
            "Epoch: [4]  [10/90]  eta: 0:03:28  lr: 0.000010  loss: 4.1691 (4.2218)  loss_ce: 0.1206 (0.1128)  loss_bbox: 0.0511 (0.0518)  loss_giou: 0.4041 (0.4130)  loss_ce_0: 0.1340 (0.1491)  loss_bbox_0: 0.0516 (0.0522)  loss_giou_0: 0.4001 (0.4086)  loss_ce_1: 0.1189 (0.1367)  loss_bbox_1: 0.0504 (0.0518)  loss_giou_1: 0.3941 (0.4091)  loss_ce_2: 0.1321 (0.1286)  loss_bbox_2: 0.0503 (0.0516)  loss_giou_2: 0.3950 (0.4045)  loss_ce_3: 0.1309 (0.1263)  loss_bbox_3: 0.0511 (0.0517)  loss_giou_3: 0.4038 (0.4108)  loss_ce_4: 0.1254 (0.1160)  loss_bbox_4: 0.0513 (0.0519)  loss_giou_4: 0.4045 (0.4158)  loss_ce_interm: 0.2031 (0.1928)  loss_bbox_interm: 0.0579 (0.0587)  loss_giou_interm: 0.4266 (0.4280)  loss_ce_unscaled: 0.0603 (0.0564)  loss_bbox_unscaled: 0.0102 (0.0104)  loss_giou_unscaled: 0.2021 (0.2065)  loss_xy_unscaled: 0.0034 (0.0033)  loss_hw_unscaled: 0.0068 (0.0071)  loss_ce_0_unscaled: 0.0670 (0.0746)  loss_bbox_0_unscaled: 0.0103 (0.0104)  loss_giou_0_unscaled: 0.2000 (0.2043)  loss_xy_0_unscaled: 0.0034 (0.0032)  loss_hw_0_unscaled: 0.0070 (0.0072)  loss_ce_1_unscaled: 0.0594 (0.0683)  loss_bbox_1_unscaled: 0.0101 (0.0104)  loss_giou_1_unscaled: 0.1971 (0.2046)  loss_xy_1_unscaled: 0.0033 (0.0032)  loss_hw_1_unscaled: 0.0068 (0.0071)  loss_ce_2_unscaled: 0.0661 (0.0643)  loss_bbox_2_unscaled: 0.0101 (0.0103)  loss_giou_2_unscaled: 0.1975 (0.2022)  loss_xy_2_unscaled: 0.0032 (0.0032)  loss_hw_2_unscaled: 0.0069 (0.0071)  loss_ce_3_unscaled: 0.0654 (0.0632)  loss_bbox_3_unscaled: 0.0102 (0.0103)  loss_giou_3_unscaled: 0.2019 (0.2054)  loss_xy_3_unscaled: 0.0034 (0.0033)  loss_hw_3_unscaled: 0.0068 (0.0070)  loss_ce_4_unscaled: 0.0627 (0.0580)  loss_bbox_4_unscaled: 0.0103 (0.0104)  loss_giou_4_unscaled: 0.2022 (0.2079)  loss_xy_4_unscaled: 0.0034 (0.0033)  loss_hw_4_unscaled: 0.0068 (0.0071)  loss_ce_interm_unscaled: 0.1016 (0.0964)  loss_bbox_interm_unscaled: 0.0116 (0.0117)  loss_giou_interm_unscaled: 0.2133 (0.2140)  loss_xy_interm_unscaled: 0.0035 (0.0035)  loss_hw_interm_unscaled: 0.0080 (0.0083)  time: 2.6078  data: 0.0758  max mem: 8948\n",
            "Epoch: [4]  [20/90]  eta: 0:03:15  lr: 0.000010  loss: 4.1335 (4.2047)  loss_ce: 0.1006 (0.1137)  loss_bbox: 0.0478 (0.0498)  loss_giou: 0.4041 (0.4143)  loss_ce_0: 0.1340 (0.1444)  loss_bbox_0: 0.0500 (0.0500)  loss_giou_0: 0.4099 (0.4124)  loss_ce_1: 0.1172 (0.1302)  loss_bbox_1: 0.0488 (0.0500)  loss_giou_1: 0.4083 (0.4158)  loss_ce_2: 0.1058 (0.1229)  loss_bbox_2: 0.0472 (0.0496)  loss_giou_2: 0.4043 (0.4082)  loss_ce_3: 0.1084 (0.1228)  loss_bbox_3: 0.0474 (0.0498)  loss_giou_3: 0.4092 (0.4140)  loss_ce_4: 0.1091 (0.1180)  loss_bbox_4: 0.0480 (0.0498)  loss_giou_4: 0.4045 (0.4163)  loss_ce_interm: 0.1890 (0.1927)  loss_bbox_interm: 0.0567 (0.0548)  loss_giou_interm: 0.4266 (0.4253)  loss_ce_unscaled: 0.0503 (0.0569)  loss_bbox_unscaled: 0.0096 (0.0100)  loss_giou_unscaled: 0.2021 (0.2072)  loss_xy_unscaled: 0.0031 (0.0031)  loss_hw_unscaled: 0.0065 (0.0069)  loss_ce_0_unscaled: 0.0670 (0.0722)  loss_bbox_0_unscaled: 0.0100 (0.0100)  loss_giou_0_unscaled: 0.2049 (0.2062)  loss_xy_0_unscaled: 0.0029 (0.0030)  loss_hw_0_unscaled: 0.0067 (0.0070)  loss_ce_1_unscaled: 0.0586 (0.0651)  loss_bbox_1_unscaled: 0.0098 (0.0100)  loss_giou_1_unscaled: 0.2042 (0.2079)  loss_xy_1_unscaled: 0.0029 (0.0030)  loss_hw_1_unscaled: 0.0066 (0.0070)  loss_ce_2_unscaled: 0.0529 (0.0615)  loss_bbox_2_unscaled: 0.0094 (0.0099)  loss_giou_2_unscaled: 0.2021 (0.2041)  loss_xy_2_unscaled: 0.0029 (0.0030)  loss_hw_2_unscaled: 0.0066 (0.0069)  loss_ce_3_unscaled: 0.0542 (0.0614)  loss_bbox_3_unscaled: 0.0095 (0.0100)  loss_giou_3_unscaled: 0.2046 (0.2070)  loss_xy_3_unscaled: 0.0030 (0.0031)  loss_hw_3_unscaled: 0.0067 (0.0069)  loss_ce_4_unscaled: 0.0545 (0.0590)  loss_bbox_4_unscaled: 0.0096 (0.0100)  loss_giou_4_unscaled: 0.2022 (0.2081)  loss_xy_4_unscaled: 0.0031 (0.0031)  loss_hw_4_unscaled: 0.0066 (0.0069)  loss_ce_interm_unscaled: 0.0945 (0.0963)  loss_bbox_interm_unscaled: 0.0113 (0.0110)  loss_giou_interm_unscaled: 0.2133 (0.2126)  loss_xy_interm_unscaled: 0.0032 (0.0032)  loss_hw_interm_unscaled: 0.0076 (0.0078)  time: 2.7375  data: 0.0143  max mem: 8948\n",
            "Epoch: [4]  [30/90]  eta: 0:02:46  lr: 0.000010  loss: 4.1335 (4.1739)  loss_ce: 0.1087 (0.1154)  loss_bbox: 0.0478 (0.0498)  loss_giou: 0.4043 (0.4099)  loss_ce_0: 0.1314 (0.1434)  loss_bbox_0: 0.0477 (0.0502)  loss_giou_0: 0.4149 (0.4099)  loss_ce_1: 0.1262 (0.1290)  loss_bbox_1: 0.0472 (0.0501)  loss_giou_1: 0.4108 (0.4116)  loss_ce_2: 0.1082 (0.1214)  loss_bbox_2: 0.0466 (0.0497)  loss_giou_2: 0.4111 (0.4055)  loss_ce_3: 0.1100 (0.1230)  loss_bbox_3: 0.0474 (0.0498)  loss_giou_3: 0.4110 (0.4091)  loss_ce_4: 0.1150 (0.1193)  loss_bbox_4: 0.0480 (0.0498)  loss_giou_4: 0.4056 (0.4112)  loss_ce_interm: 0.1885 (0.1895)  loss_bbox_interm: 0.0519 (0.0541)  loss_giou_interm: 0.4202 (0.4220)  loss_ce_unscaled: 0.0543 (0.0577)  loss_bbox_unscaled: 0.0096 (0.0100)  loss_giou_unscaled: 0.2022 (0.2050)  loss_xy_unscaled: 0.0028 (0.0030)  loss_hw_unscaled: 0.0067 (0.0069)  loss_ce_0_unscaled: 0.0657 (0.0717)  loss_bbox_0_unscaled: 0.0095 (0.0100)  loss_giou_0_unscaled: 0.2074 (0.2049)  loss_xy_0_unscaled: 0.0028 (0.0030)  loss_hw_0_unscaled: 0.0067 (0.0070)  loss_ce_1_unscaled: 0.0631 (0.0645)  loss_bbox_1_unscaled: 0.0094 (0.0100)  loss_giou_1_unscaled: 0.2054 (0.2058)  loss_xy_1_unscaled: 0.0027 (0.0030)  loss_hw_1_unscaled: 0.0066 (0.0070)  loss_ce_2_unscaled: 0.0541 (0.0607)  loss_bbox_2_unscaled: 0.0093 (0.0099)  loss_giou_2_unscaled: 0.2055 (0.2027)  loss_xy_2_unscaled: 0.0027 (0.0030)  loss_hw_2_unscaled: 0.0066 (0.0070)  loss_ce_3_unscaled: 0.0550 (0.0615)  loss_bbox_3_unscaled: 0.0095 (0.0100)  loss_giou_3_unscaled: 0.2055 (0.2045)  loss_xy_3_unscaled: 0.0027 (0.0030)  loss_hw_3_unscaled: 0.0067 (0.0070)  loss_ce_4_unscaled: 0.0575 (0.0597)  loss_bbox_4_unscaled: 0.0096 (0.0100)  loss_giou_4_unscaled: 0.2028 (0.2056)  loss_xy_4_unscaled: 0.0028 (0.0030)  loss_hw_4_unscaled: 0.0066 (0.0069)  loss_ce_interm_unscaled: 0.0943 (0.0948)  loss_bbox_interm_unscaled: 0.0104 (0.0108)  loss_giou_interm_unscaled: 0.2101 (0.2110)  loss_xy_interm_unscaled: 0.0029 (0.0032)  loss_hw_interm_unscaled: 0.0073 (0.0076)  time: 2.8586  data: 0.0143  max mem: 8948\n",
            "Epoch: [4]  [40/90]  eta: 0:02:19  lr: 0.000010  loss: 3.9952 (4.2361)  loss_ce: 0.1148 (0.1217)  loss_bbox: 0.0500 (0.0505)  loss_giou: 0.3992 (0.4104)  loss_ce_0: 0.1337 (0.1521)  loss_bbox_0: 0.0497 (0.0509)  loss_giou_0: 0.4040 (0.4109)  loss_ce_1: 0.1301 (0.1381)  loss_bbox_1: 0.0502 (0.0504)  loss_giou_1: 0.4056 (0.4109)  loss_ce_2: 0.1230 (0.1329)  loss_bbox_2: 0.0499 (0.0501)  loss_giou_2: 0.4081 (0.4058)  loss_ce_3: 0.1272 (0.1295)  loss_bbox_3: 0.0501 (0.0503)  loss_giou_3: 0.4060 (0.4087)  loss_ce_4: 0.1203 (0.1263)  loss_bbox_4: 0.0499 (0.0504)  loss_giou_4: 0.4034 (0.4105)  loss_ce_interm: 0.1882 (0.2018)  loss_bbox_interm: 0.0520 (0.0540)  loss_giou_interm: 0.4158 (0.4200)  loss_ce_unscaled: 0.0574 (0.0608)  loss_bbox_unscaled: 0.0100 (0.0101)  loss_giou_unscaled: 0.1996 (0.2052)  loss_xy_unscaled: 0.0028 (0.0030)  loss_hw_unscaled: 0.0069 (0.0071)  loss_ce_0_unscaled: 0.0669 (0.0760)  loss_bbox_0_unscaled: 0.0099 (0.0102)  loss_giou_0_unscaled: 0.2020 (0.2054)  loss_xy_0_unscaled: 0.0029 (0.0030)  loss_hw_0_unscaled: 0.0070 (0.0072)  loss_ce_1_unscaled: 0.0650 (0.0691)  loss_bbox_1_unscaled: 0.0100 (0.0101)  loss_giou_1_unscaled: 0.2028 (0.2055)  loss_xy_1_unscaled: 0.0028 (0.0030)  loss_hw_1_unscaled: 0.0069 (0.0071)  loss_ce_2_unscaled: 0.0615 (0.0664)  loss_bbox_2_unscaled: 0.0100 (0.0100)  loss_giou_2_unscaled: 0.2040 (0.2029)  loss_xy_2_unscaled: 0.0028 (0.0030)  loss_hw_2_unscaled: 0.0071 (0.0071)  loss_ce_3_unscaled: 0.0636 (0.0647)  loss_bbox_3_unscaled: 0.0100 (0.0101)  loss_giou_3_unscaled: 0.2030 (0.2044)  loss_xy_3_unscaled: 0.0028 (0.0030)  loss_hw_3_unscaled: 0.0071 (0.0071)  loss_ce_4_unscaled: 0.0601 (0.0631)  loss_bbox_4_unscaled: 0.0100 (0.0101)  loss_giou_4_unscaled: 0.2017 (0.2053)  loss_xy_4_unscaled: 0.0028 (0.0030)  loss_hw_4_unscaled: 0.0069 (0.0071)  loss_ce_interm_unscaled: 0.0941 (0.1009)  loss_bbox_interm_unscaled: 0.0104 (0.0108)  loss_giou_interm_unscaled: 0.2079 (0.2100)  loss_xy_interm_unscaled: 0.0030 (0.0032)  loss_hw_interm_unscaled: 0.0073 (0.0076)  time: 2.7917  data: 0.0154  max mem: 8948\n",
            "Epoch: [4]  [50/90]  eta: 0:01:52  lr: 0.000010  loss: 3.9597 (4.2229)  loss_ce: 0.0996 (0.1215)  loss_bbox: 0.0479 (0.0501)  loss_giou: 0.3936 (0.4083)  loss_ce_0: 0.1346 (0.1538)  loss_bbox_0: 0.0476 (0.0504)  loss_giou_0: 0.3914 (0.4092)  loss_ce_1: 0.1207 (0.1388)  loss_bbox_1: 0.0473 (0.0502)  loss_giou_1: 0.3864 (0.4085)  loss_ce_2: 0.1166 (0.1326)  loss_bbox_2: 0.0474 (0.0499)  loss_giou_2: 0.3877 (0.4047)  loss_ce_3: 0.1052 (0.1281)  loss_bbox_3: 0.0480 (0.0501)  loss_giou_3: 0.3871 (0.4072)  loss_ce_4: 0.1044 (0.1264)  loss_bbox_4: 0.0476 (0.0501)  loss_giou_4: 0.3855 (0.4083)  loss_ce_interm: 0.1966 (0.2054)  loss_bbox_interm: 0.0513 (0.0532)  loss_giou_interm: 0.3896 (0.4161)  loss_ce_unscaled: 0.0498 (0.0608)  loss_bbox_unscaled: 0.0096 (0.0100)  loss_giou_unscaled: 0.1968 (0.2041)  loss_xy_unscaled: 0.0028 (0.0030)  loss_hw_unscaled: 0.0069 (0.0070)  loss_ce_0_unscaled: 0.0673 (0.0769)  loss_bbox_0_unscaled: 0.0095 (0.0101)  loss_giou_0_unscaled: 0.1957 (0.2046)  loss_xy_0_unscaled: 0.0029 (0.0030)  loss_hw_0_unscaled: 0.0068 (0.0071)  loss_ce_1_unscaled: 0.0604 (0.0694)  loss_bbox_1_unscaled: 0.0095 (0.0100)  loss_giou_1_unscaled: 0.1932 (0.2043)  loss_xy_1_unscaled: 0.0028 (0.0029)  loss_hw_1_unscaled: 0.0068 (0.0071)  loss_ce_2_unscaled: 0.0583 (0.0663)  loss_bbox_2_unscaled: 0.0095 (0.0100)  loss_giou_2_unscaled: 0.1939 (0.2023)  loss_xy_2_unscaled: 0.0028 (0.0029)  loss_hw_2_unscaled: 0.0069 (0.0071)  loss_ce_3_unscaled: 0.0526 (0.0641)  loss_bbox_3_unscaled: 0.0096 (0.0100)  loss_giou_3_unscaled: 0.1936 (0.2036)  loss_xy_3_unscaled: 0.0028 (0.0030)  loss_hw_3_unscaled: 0.0069 (0.0070)  loss_ce_4_unscaled: 0.0522 (0.0632)  loss_bbox_4_unscaled: 0.0095 (0.0100)  loss_giou_4_unscaled: 0.1928 (0.2041)  loss_xy_4_unscaled: 0.0027 (0.0030)  loss_hw_4_unscaled: 0.0069 (0.0070)  loss_ce_interm_unscaled: 0.0983 (0.1027)  loss_bbox_interm_unscaled: 0.0103 (0.0106)  loss_giou_interm_unscaled: 0.1948 (0.2080)  loss_xy_interm_unscaled: 0.0029 (0.0031)  loss_hw_interm_unscaled: 0.0073 (0.0075)  time: 2.8800  data: 0.0158  max mem: 8948\n",
            "Epoch: [4]  [60/90]  eta: 0:01:25  lr: 0.000010  loss: 3.8472 (4.1768)  loss_ce: 0.0932 (0.1177)  loss_bbox: 0.0445 (0.0495)  loss_giou: 0.3852 (0.4056)  loss_ce_0: 0.1312 (0.1500)  loss_bbox_0: 0.0447 (0.0498)  loss_giou_0: 0.3923 (0.4077)  loss_ce_1: 0.1112 (0.1344)  loss_bbox_1: 0.0452 (0.0496)  loss_giou_1: 0.3864 (0.4073)  loss_ce_2: 0.1038 (0.1291)  loss_bbox_2: 0.0447 (0.0493)  loss_giou_2: 0.3828 (0.4032)  loss_ce_3: 0.0990 (0.1244)  loss_bbox_3: 0.0452 (0.0494)  loss_giou_3: 0.3855 (0.4052)  loss_ce_4: 0.0977 (0.1223)  loss_bbox_4: 0.0445 (0.0495)  loss_giou_4: 0.3823 (0.4055)  loss_ce_interm: 0.1647 (0.2001)  loss_bbox_interm: 0.0484 (0.0523)  loss_giou_interm: 0.3896 (0.4150)  loss_ce_unscaled: 0.0466 (0.0589)  loss_bbox_unscaled: 0.0089 (0.0099)  loss_giou_unscaled: 0.1926 (0.2028)  loss_xy_unscaled: 0.0026 (0.0030)  loss_hw_unscaled: 0.0065 (0.0069)  loss_ce_0_unscaled: 0.0656 (0.0750)  loss_bbox_0_unscaled: 0.0089 (0.0100)  loss_giou_0_unscaled: 0.1961 (0.2038)  loss_xy_0_unscaled: 0.0026 (0.0030)  loss_hw_0_unscaled: 0.0065 (0.0070)  loss_ce_1_unscaled: 0.0556 (0.0672)  loss_bbox_1_unscaled: 0.0090 (0.0099)  loss_giou_1_unscaled: 0.1932 (0.2036)  loss_xy_1_unscaled: 0.0027 (0.0029)  loss_hw_1_unscaled: 0.0065 (0.0070)  loss_ce_2_unscaled: 0.0519 (0.0646)  loss_bbox_2_unscaled: 0.0089 (0.0099)  loss_giou_2_unscaled: 0.1914 (0.2016)  loss_xy_2_unscaled: 0.0026 (0.0029)  loss_hw_2_unscaled: 0.0064 (0.0069)  loss_ce_3_unscaled: 0.0495 (0.0622)  loss_bbox_3_unscaled: 0.0090 (0.0099)  loss_giou_3_unscaled: 0.1927 (0.2026)  loss_xy_3_unscaled: 0.0026 (0.0029)  loss_hw_3_unscaled: 0.0065 (0.0069)  loss_ce_4_unscaled: 0.0488 (0.0611)  loss_bbox_4_unscaled: 0.0089 (0.0099)  loss_giou_4_unscaled: 0.1912 (0.2028)  loss_xy_4_unscaled: 0.0026 (0.0030)  loss_hw_4_unscaled: 0.0064 (0.0069)  loss_ce_interm_unscaled: 0.0824 (0.1001)  loss_bbox_interm_unscaled: 0.0097 (0.0105)  loss_giou_interm_unscaled: 0.1948 (0.2075)  loss_xy_interm_unscaled: 0.0028 (0.0031)  loss_hw_interm_unscaled: 0.0071 (0.0074)  time: 2.9377  data: 0.0158  max mem: 8948\n",
            "Epoch: [4]  [70/90]  eta: 0:00:56  lr: 0.000010  loss: 3.8432 (4.1581)  loss_ce: 0.0958 (0.1170)  loss_bbox: 0.0445 (0.0489)  loss_giou: 0.3765 (0.4039)  loss_ce_0: 0.1261 (0.1498)  loss_bbox_0: 0.0447 (0.0492)  loss_giou_0: 0.3894 (0.4059)  loss_ce_1: 0.1143 (0.1348)  loss_bbox_1: 0.0452 (0.0490)  loss_giou_1: 0.3857 (0.4055)  loss_ce_2: 0.1081 (0.1297)  loss_bbox_2: 0.0447 (0.0487)  loss_giou_2: 0.3828 (0.4013)  loss_ce_3: 0.1047 (0.1248)  loss_bbox_3: 0.0451 (0.0488)  loss_giou_3: 0.3855 (0.4031)  loss_ce_4: 0.0977 (0.1223)  loss_bbox_4: 0.0445 (0.0489)  loss_giou_4: 0.3799 (0.4036)  loss_ce_interm: 0.1691 (0.1979)  loss_bbox_interm: 0.0451 (0.0517)  loss_giou_interm: 0.3968 (0.4132)  loss_ce_unscaled: 0.0479 (0.0585)  loss_bbox_unscaled: 0.0089 (0.0098)  loss_giou_unscaled: 0.1883 (0.2019)  loss_xy_unscaled: 0.0027 (0.0029)  loss_hw_unscaled: 0.0062 (0.0069)  loss_ce_0_unscaled: 0.0630 (0.0749)  loss_bbox_0_unscaled: 0.0089 (0.0098)  loss_giou_0_unscaled: 0.1947 (0.2029)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0063 (0.0069)  loss_ce_1_unscaled: 0.0571 (0.0674)  loss_bbox_1_unscaled: 0.0090 (0.0098)  loss_giou_1_unscaled: 0.1929 (0.2027)  loss_xy_1_unscaled: 0.0027 (0.0029)  loss_hw_1_unscaled: 0.0062 (0.0069)  loss_ce_2_unscaled: 0.0541 (0.0649)  loss_bbox_2_unscaled: 0.0089 (0.0097)  loss_giou_2_unscaled: 0.1914 (0.2007)  loss_xy_2_unscaled: 0.0027 (0.0029)  loss_hw_2_unscaled: 0.0063 (0.0068)  loss_ce_3_unscaled: 0.0524 (0.0624)  loss_bbox_3_unscaled: 0.0090 (0.0098)  loss_giou_3_unscaled: 0.1927 (0.2016)  loss_xy_3_unscaled: 0.0027 (0.0029)  loss_hw_3_unscaled: 0.0062 (0.0068)  loss_ce_4_unscaled: 0.0488 (0.0612)  loss_bbox_4_unscaled: 0.0089 (0.0098)  loss_giou_4_unscaled: 0.1900 (0.2018)  loss_xy_4_unscaled: 0.0027 (0.0029)  loss_hw_4_unscaled: 0.0062 (0.0068)  loss_ce_interm_unscaled: 0.0845 (0.0990)  loss_bbox_interm_unscaled: 0.0090 (0.0103)  loss_giou_interm_unscaled: 0.1984 (0.2066)  loss_xy_interm_unscaled: 0.0030 (0.0031)  loss_hw_interm_unscaled: 0.0061 (0.0073)  time: 2.8522  data: 0.0147  max mem: 8948\n",
            "Epoch: [4]  [80/90]  eta: 0:00:28  lr: 0.000010  loss: 3.9477 (4.1446)  loss_ce: 0.1025 (0.1171)  loss_bbox: 0.0458 (0.0491)  loss_giou: 0.3750 (0.4028)  loss_ce_0: 0.1263 (0.1477)  loss_bbox_0: 0.0474 (0.0494)  loss_giou_0: 0.3842 (0.4052)  loss_ce_1: 0.1164 (0.1336)  loss_bbox_1: 0.0464 (0.0491)  loss_giou_1: 0.3770 (0.4044)  loss_ce_2: 0.1101 (0.1287)  loss_bbox_2: 0.0467 (0.0489)  loss_giou_2: 0.3798 (0.4009)  loss_ce_3: 0.1088 (0.1236)  loss_bbox_3: 0.0458 (0.0490)  loss_giou_3: 0.3774 (0.4023)  loss_ce_4: 0.1057 (0.1214)  loss_bbox_4: 0.0459 (0.0491)  loss_giou_4: 0.3735 (0.4026)  loss_ce_interm: 0.1797 (0.1957)  loss_bbox_interm: 0.0492 (0.0518)  loss_giou_interm: 0.3968 (0.4124)  loss_ce_unscaled: 0.0512 (0.0585)  loss_bbox_unscaled: 0.0092 (0.0098)  loss_giou_unscaled: 0.1875 (0.2014)  loss_xy_unscaled: 0.0028 (0.0029)  loss_hw_unscaled: 0.0064 (0.0069)  loss_ce_0_unscaled: 0.0632 (0.0739)  loss_bbox_0_unscaled: 0.0095 (0.0099)  loss_giou_0_unscaled: 0.1921 (0.2026)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0066 (0.0069)  loss_ce_1_unscaled: 0.0582 (0.0668)  loss_bbox_1_unscaled: 0.0093 (0.0098)  loss_giou_1_unscaled: 0.1885 (0.2022)  loss_xy_1_unscaled: 0.0028 (0.0029)  loss_hw_1_unscaled: 0.0065 (0.0069)  loss_ce_2_unscaled: 0.0550 (0.0644)  loss_bbox_2_unscaled: 0.0093 (0.0098)  loss_giou_2_unscaled: 0.1899 (0.2005)  loss_xy_2_unscaled: 0.0028 (0.0029)  loss_hw_2_unscaled: 0.0066 (0.0069)  loss_ce_3_unscaled: 0.0544 (0.0618)  loss_bbox_3_unscaled: 0.0092 (0.0098)  loss_giou_3_unscaled: 0.1887 (0.2012)  loss_xy_3_unscaled: 0.0028 (0.0029)  loss_hw_3_unscaled: 0.0065 (0.0069)  loss_ce_4_unscaled: 0.0528 (0.0607)  loss_bbox_4_unscaled: 0.0092 (0.0098)  loss_giou_4_unscaled: 0.1868 (0.2013)  loss_xy_4_unscaled: 0.0028 (0.0029)  loss_hw_4_unscaled: 0.0064 (0.0069)  loss_ce_interm_unscaled: 0.0898 (0.0979)  loss_bbox_interm_unscaled: 0.0098 (0.0104)  loss_giou_interm_unscaled: 0.1984 (0.2062)  loss_xy_interm_unscaled: 0.0030 (0.0031)  loss_hw_interm_unscaled: 0.0069 (0.0073)  time: 2.8339  data: 0.0149  max mem: 8948\n",
            "Epoch: [4]  [89/90]  eta: 0:00:02  lr: 0.000010  loss: 3.8372 (4.1252)  loss_ce: 0.0970 (0.1158)  loss_bbox: 0.0461 (0.0491)  loss_giou: 0.3743 (0.4009)  loss_ce_0: 0.1220 (0.1471)  loss_bbox_0: 0.0471 (0.0494)  loss_giou_0: 0.3800 (0.4032)  loss_ce_1: 0.1143 (0.1330)  loss_bbox_1: 0.0465 (0.0492)  loss_giou_1: 0.3744 (0.4025)  loss_ce_2: 0.1030 (0.1273)  loss_bbox_2: 0.0467 (0.0490)  loss_giou_2: 0.3733 (0.3993)  loss_ce_3: 0.1045 (0.1222)  loss_bbox_3: 0.0461 (0.0491)  loss_giou_3: 0.3747 (0.4004)  loss_ce_4: 0.1031 (0.1200)  loss_bbox_4: 0.0461 (0.0491)  loss_giou_4: 0.3716 (0.4007)  loss_ce_interm: 0.1751 (0.1947)  loss_bbox_interm: 0.0516 (0.0519)  loss_giou_interm: 0.3947 (0.4111)  loss_ce_unscaled: 0.0485 (0.0579)  loss_bbox_unscaled: 0.0092 (0.0098)  loss_giou_unscaled: 0.1871 (0.2004)  loss_xy_unscaled: 0.0029 (0.0029)  loss_hw_unscaled: 0.0066 (0.0069)  loss_ce_0_unscaled: 0.0610 (0.0736)  loss_bbox_0_unscaled: 0.0094 (0.0099)  loss_giou_0_unscaled: 0.1900 (0.2016)  loss_xy_0_unscaled: 0.0029 (0.0030)  loss_hw_0_unscaled: 0.0066 (0.0069)  loss_ce_1_unscaled: 0.0572 (0.0665)  loss_bbox_1_unscaled: 0.0093 (0.0098)  loss_giou_1_unscaled: 0.1872 (0.2013)  loss_xy_1_unscaled: 0.0029 (0.0029)  loss_hw_1_unscaled: 0.0066 (0.0069)  loss_ce_2_unscaled: 0.0515 (0.0636)  loss_bbox_2_unscaled: 0.0093 (0.0098)  loss_giou_2_unscaled: 0.1867 (0.1997)  loss_xy_2_unscaled: 0.0029 (0.0029)  loss_hw_2_unscaled: 0.0066 (0.0069)  loss_ce_3_unscaled: 0.0523 (0.0611)  loss_bbox_3_unscaled: 0.0092 (0.0098)  loss_giou_3_unscaled: 0.1873 (0.2002)  loss_xy_3_unscaled: 0.0029 (0.0029)  loss_hw_3_unscaled: 0.0065 (0.0069)  loss_ce_4_unscaled: 0.0515 (0.0600)  loss_bbox_4_unscaled: 0.0092 (0.0098)  loss_giou_4_unscaled: 0.1858 (0.2004)  loss_xy_4_unscaled: 0.0029 (0.0029)  loss_hw_4_unscaled: 0.0066 (0.0069)  loss_ce_interm_unscaled: 0.0875 (0.0974)  loss_bbox_interm_unscaled: 0.0103 (0.0104)  loss_giou_interm_unscaled: 0.1974 (0.2056)  loss_xy_interm_unscaled: 0.0030 (0.0031)  loss_hw_interm_unscaled: 0.0071 (0.0073)  time: 2.9261  data: 0.0151  max mem: 8948\n",
            "Epoch: [4] Total time: 0:04:15 (2.8419 s / it)\n",
            "Averaged stats: lr: 0.000010  loss: 3.8372 (4.1252)  loss_ce: 0.0970 (0.1158)  loss_bbox: 0.0461 (0.0491)  loss_giou: 0.3743 (0.4009)  loss_ce_0: 0.1220 (0.1471)  loss_bbox_0: 0.0471 (0.0494)  loss_giou_0: 0.3800 (0.4032)  loss_ce_1: 0.1143 (0.1330)  loss_bbox_1: 0.0465 (0.0492)  loss_giou_1: 0.3744 (0.4025)  loss_ce_2: 0.1030 (0.1273)  loss_bbox_2: 0.0467 (0.0490)  loss_giou_2: 0.3733 (0.3993)  loss_ce_3: 0.1045 (0.1222)  loss_bbox_3: 0.0461 (0.0491)  loss_giou_3: 0.3747 (0.4004)  loss_ce_4: 0.1031 (0.1200)  loss_bbox_4: 0.0461 (0.0491)  loss_giou_4: 0.3716 (0.4007)  loss_ce_interm: 0.1751 (0.1947)  loss_bbox_interm: 0.0516 (0.0519)  loss_giou_interm: 0.3947 (0.4111)  loss_ce_unscaled: 0.0485 (0.0579)  loss_bbox_unscaled: 0.0092 (0.0098)  loss_giou_unscaled: 0.1871 (0.2004)  loss_xy_unscaled: 0.0029 (0.0029)  loss_hw_unscaled: 0.0066 (0.0069)  loss_ce_0_unscaled: 0.0610 (0.0736)  loss_bbox_0_unscaled: 0.0094 (0.0099)  loss_giou_0_unscaled: 0.1900 (0.2016)  loss_xy_0_unscaled: 0.0029 (0.0030)  loss_hw_0_unscaled: 0.0066 (0.0069)  loss_ce_1_unscaled: 0.0572 (0.0665)  loss_bbox_1_unscaled: 0.0093 (0.0098)  loss_giou_1_unscaled: 0.1872 (0.2013)  loss_xy_1_unscaled: 0.0029 (0.0029)  loss_hw_1_unscaled: 0.0066 (0.0069)  loss_ce_2_unscaled: 0.0515 (0.0636)  loss_bbox_2_unscaled: 0.0093 (0.0098)  loss_giou_2_unscaled: 0.1867 (0.1997)  loss_xy_2_unscaled: 0.0029 (0.0029)  loss_hw_2_unscaled: 0.0066 (0.0069)  loss_ce_3_unscaled: 0.0523 (0.0611)  loss_bbox_3_unscaled: 0.0092 (0.0098)  loss_giou_3_unscaled: 0.1873 (0.2002)  loss_xy_3_unscaled: 0.0029 (0.0029)  loss_hw_3_unscaled: 0.0065 (0.0069)  loss_ce_4_unscaled: 0.0515 (0.0600)  loss_bbox_4_unscaled: 0.0092 (0.0098)  loss_giou_4_unscaled: 0.1858 (0.2004)  loss_xy_4_unscaled: 0.0029 (0.0029)  loss_hw_4_unscaled: 0.0066 (0.0069)  loss_ce_interm_unscaled: 0.0875 (0.0974)  loss_bbox_interm_unscaled: 0.0103 (0.0104)  loss_giou_interm_unscaled: 0.1974 (0.2056)  loss_xy_interm_unscaled: 0.0030 (0.0031)  loss_hw_interm_unscaled: 0.0071 (0.0073)\n",
            "Input text prompt: bolt . wrong direction . 1 . 2 . 3 . 4 . 5 .\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Test:  [ 0/10]  eta: 0:00:17    time: 1.7150  data: 0.7746  max mem: 8948\n",
            "Test:  [ 9/10]  eta: 0:00:01    time: 1.0211  data: 0.0938  max mem: 8948\n",
            "Test: Total time: 0:00:10 (1.0305 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.055\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.128\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.033\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.291\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.220\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.024\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.267\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.499\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.446\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.662\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.557\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch: [5]  [ 0/90]  eta: 0:04:23  lr: 0.000010  loss: 4.1355 (4.1355)  loss_ce: 0.1491 (0.1491)  loss_bbox: 0.0429 (0.0429)  loss_giou: 0.3599 (0.3599)  loss_ce_0: 0.1875 (0.1875)  loss_bbox_0: 0.0439 (0.0439)  loss_giou_0: 0.3770 (0.3770)  loss_ce_1: 0.1973 (0.1973)  loss_bbox_1: 0.0435 (0.0435)  loss_giou_1: 0.3732 (0.3732)  loss_ce_2: 0.1728 (0.1728)  loss_bbox_2: 0.0435 (0.0435)  loss_giou_2: 0.3717 (0.3717)  loss_ce_3: 0.1652 (0.1652)  loss_bbox_3: 0.0430 (0.0430)  loss_giou_3: 0.3684 (0.3684)  loss_ce_4: 0.1608 (0.1608)  loss_bbox_4: 0.0430 (0.0430)  loss_giou_4: 0.3621 (0.3621)  loss_ce_interm: 0.2125 (0.2125)  loss_bbox_interm: 0.0458 (0.0458)  loss_giou_interm: 0.3723 (0.3723)  loss_ce_unscaled: 0.0745 (0.0745)  loss_bbox_unscaled: 0.0086 (0.0086)  loss_giou_unscaled: 0.1799 (0.1799)  loss_xy_unscaled: 0.0029 (0.0029)  loss_hw_unscaled: 0.0057 (0.0057)  loss_ce_0_unscaled: 0.0938 (0.0938)  loss_bbox_0_unscaled: 0.0088 (0.0088)  loss_giou_0_unscaled: 0.1885 (0.1885)  loss_xy_0_unscaled: 0.0030 (0.0030)  loss_hw_0_unscaled: 0.0058 (0.0058)  loss_ce_1_unscaled: 0.0987 (0.0987)  loss_bbox_1_unscaled: 0.0087 (0.0087)  loss_giou_1_unscaled: 0.1866 (0.1866)  loss_xy_1_unscaled: 0.0029 (0.0029)  loss_hw_1_unscaled: 0.0058 (0.0058)  loss_ce_2_unscaled: 0.0864 (0.0864)  loss_bbox_2_unscaled: 0.0087 (0.0087)  loss_giou_2_unscaled: 0.1858 (0.1858)  loss_xy_2_unscaled: 0.0029 (0.0029)  loss_hw_2_unscaled: 0.0058 (0.0058)  loss_ce_3_unscaled: 0.0826 (0.0826)  loss_bbox_3_unscaled: 0.0086 (0.0086)  loss_giou_3_unscaled: 0.1842 (0.1842)  loss_xy_3_unscaled: 0.0029 (0.0029)  loss_hw_3_unscaled: 0.0057 (0.0057)  loss_ce_4_unscaled: 0.0804 (0.0804)  loss_bbox_4_unscaled: 0.0086 (0.0086)  loss_giou_4_unscaled: 0.1810 (0.1810)  loss_xy_4_unscaled: 0.0029 (0.0029)  loss_hw_4_unscaled: 0.0057 (0.0057)  loss_ce_interm_unscaled: 0.1063 (0.1063)  loss_bbox_interm_unscaled: 0.0092 (0.0092)  loss_giou_interm_unscaled: 0.1862 (0.1862)  loss_xy_interm_unscaled: 0.0031 (0.0031)  loss_hw_interm_unscaled: 0.0061 (0.0061)  time: 2.9278  data: 0.6765  max mem: 8948\n",
            "Epoch: [5]  [10/90]  eta: 0:03:45  lr: 0.000010  loss: 4.1308 (4.0160)  loss_ce: 0.1006 (0.1114)  loss_bbox: 0.0445 (0.0469)  loss_giou: 0.3908 (0.3949)  loss_ce_0: 0.1432 (0.1382)  loss_bbox_0: 0.0446 (0.0473)  loss_giou_0: 0.3981 (0.3948)  loss_ce_1: 0.1157 (0.1290)  loss_bbox_1: 0.0446 (0.0472)  loss_giou_1: 0.3927 (0.3943)  loss_ce_2: 0.1166 (0.1218)  loss_bbox_2: 0.0448 (0.0472)  loss_giou_2: 0.3973 (0.3942)  loss_ce_3: 0.1160 (0.1167)  loss_bbox_3: 0.0455 (0.0476)  loss_giou_3: 0.3915 (0.3952)  loss_ce_4: 0.1129 (0.1131)  loss_bbox_4: 0.0446 (0.0471)  loss_giou_4: 0.3884 (0.3951)  loss_ce_interm: 0.1797 (0.1832)  loss_bbox_interm: 0.0465 (0.0488)  loss_giou_interm: 0.3941 (0.4021)  loss_ce_unscaled: 0.0503 (0.0557)  loss_bbox_unscaled: 0.0089 (0.0094)  loss_giou_unscaled: 0.1954 (0.1974)  loss_xy_unscaled: 0.0027 (0.0027)  loss_hw_unscaled: 0.0065 (0.0067)  loss_ce_0_unscaled: 0.0716 (0.0691)  loss_bbox_0_unscaled: 0.0089 (0.0095)  loss_giou_0_unscaled: 0.1991 (0.1974)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0063 (0.0067)  loss_ce_1_unscaled: 0.0578 (0.0645)  loss_bbox_1_unscaled: 0.0089 (0.0094)  loss_giou_1_unscaled: 0.1964 (0.1972)  loss_xy_1_unscaled: 0.0027 (0.0027)  loss_hw_1_unscaled: 0.0064 (0.0067)  loss_ce_2_unscaled: 0.0583 (0.0609)  loss_bbox_2_unscaled: 0.0090 (0.0094)  loss_giou_2_unscaled: 0.1987 (0.1971)  loss_xy_2_unscaled: 0.0027 (0.0027)  loss_hw_2_unscaled: 0.0065 (0.0067)  loss_ce_3_unscaled: 0.0580 (0.0584)  loss_bbox_3_unscaled: 0.0091 (0.0095)  loss_giou_3_unscaled: 0.1957 (0.1976)  loss_xy_3_unscaled: 0.0027 (0.0027)  loss_hw_3_unscaled: 0.0064 (0.0068)  loss_ce_4_unscaled: 0.0565 (0.0565)  loss_bbox_4_unscaled: 0.0089 (0.0094)  loss_giou_4_unscaled: 0.1942 (0.1976)  loss_xy_4_unscaled: 0.0027 (0.0027)  loss_hw_4_unscaled: 0.0065 (0.0067)  loss_ce_interm_unscaled: 0.0899 (0.0916)  loss_bbox_interm_unscaled: 0.0093 (0.0098)  loss_giou_interm_unscaled: 0.1970 (0.2010)  loss_xy_interm_unscaled: 0.0029 (0.0029)  loss_hw_interm_unscaled: 0.0066 (0.0068)  time: 2.8234  data: 0.0753  max mem: 8948\n",
            "Epoch: [5]  [20/90]  eta: 0:03:22  lr: 0.000010  loss: 3.9577 (3.9510)  loss_ce: 0.0955 (0.1026)  loss_bbox: 0.0463 (0.0476)  loss_giou: 0.3841 (0.3927)  loss_ce_0: 0.1232 (0.1296)  loss_bbox_0: 0.0465 (0.0481)  loss_giou_0: 0.3867 (0.3935)  loss_ce_1: 0.1122 (0.1204)  loss_bbox_1: 0.0466 (0.0479)  loss_giou_1: 0.3926 (0.3931)  loss_ce_2: 0.1166 (0.1173)  loss_bbox_2: 0.0467 (0.0477)  loss_giou_2: 0.3862 (0.3915)  loss_ce_3: 0.1098 (0.1127)  loss_bbox_3: 0.0464 (0.0479)  loss_giou_3: 0.3884 (0.3912)  loss_ce_4: 0.0942 (0.1030)  loss_bbox_4: 0.0463 (0.0477)  loss_giou_4: 0.3831 (0.3929)  loss_ce_interm: 0.1724 (0.1740)  loss_bbox_interm: 0.0477 (0.0493)  loss_giou_interm: 0.3978 (0.4005)  loss_ce_unscaled: 0.0477 (0.0513)  loss_bbox_unscaled: 0.0093 (0.0095)  loss_giou_unscaled: 0.1921 (0.1964)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0065 (0.0068)  loss_ce_0_unscaled: 0.0616 (0.0648)  loss_bbox_0_unscaled: 0.0093 (0.0096)  loss_giou_0_unscaled: 0.1933 (0.1967)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0065 (0.0068)  loss_ce_1_unscaled: 0.0561 (0.0602)  loss_bbox_1_unscaled: 0.0093 (0.0096)  loss_giou_1_unscaled: 0.1963 (0.1965)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0065 (0.0068)  loss_ce_2_unscaled: 0.0583 (0.0587)  loss_bbox_2_unscaled: 0.0093 (0.0095)  loss_giou_2_unscaled: 0.1931 (0.1957)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0066 (0.0068)  loss_ce_3_unscaled: 0.0549 (0.0563)  loss_bbox_3_unscaled: 0.0093 (0.0096)  loss_giou_3_unscaled: 0.1942 (0.1956)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0066 (0.0068)  loss_ce_4_unscaled: 0.0471 (0.0515)  loss_bbox_4_unscaled: 0.0093 (0.0095)  loss_giou_4_unscaled: 0.1916 (0.1964)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0065 (0.0068)  loss_ce_interm_unscaled: 0.0862 (0.0870)  loss_bbox_interm_unscaled: 0.0095 (0.0099)  loss_giou_interm_unscaled: 0.1989 (0.2002)  loss_xy_interm_unscaled: 0.0028 (0.0030)  loss_hw_interm_unscaled: 0.0066 (0.0069)  time: 2.8935  data: 0.0150  max mem: 8948\n",
            "Epoch: [5]  [30/90]  eta: 0:02:57  lr: 0.000010  loss: 3.9653 (4.0018)  loss_ce: 0.0972 (0.1090)  loss_bbox: 0.0471 (0.0468)  loss_giou: 0.3810 (0.3937)  loss_ce_0: 0.1319 (0.1390)  loss_bbox_0: 0.0476 (0.0472)  loss_giou_0: 0.3848 (0.3943)  loss_ce_1: 0.1194 (0.1262)  loss_bbox_1: 0.0475 (0.0471)  loss_giou_1: 0.3788 (0.3941)  loss_ce_2: 0.1196 (0.1235)  loss_bbox_2: 0.0473 (0.0467)  loss_giou_2: 0.3790 (0.3923)  loss_ce_3: 0.1174 (0.1173)  loss_bbox_3: 0.0472 (0.0469)  loss_giou_3: 0.3770 (0.3924)  loss_ce_4: 0.1065 (0.1106)  loss_bbox_4: 0.0471 (0.0468)  loss_giou_4: 0.3811 (0.3936)  loss_ce_interm: 0.1700 (0.1841)  loss_bbox_interm: 0.0500 (0.0485)  loss_giou_interm: 0.3978 (0.4015)  loss_ce_unscaled: 0.0486 (0.0545)  loss_bbox_unscaled: 0.0094 (0.0094)  loss_giou_unscaled: 0.1905 (0.1969)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0065 (0.0066)  loss_ce_0_unscaled: 0.0659 (0.0695)  loss_bbox_0_unscaled: 0.0095 (0.0094)  loss_giou_0_unscaled: 0.1924 (0.1972)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0066 (0.0066)  loss_ce_1_unscaled: 0.0597 (0.0631)  loss_bbox_1_unscaled: 0.0095 (0.0094)  loss_giou_1_unscaled: 0.1894 (0.1971)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0066 (0.0066)  loss_ce_2_unscaled: 0.0598 (0.0617)  loss_bbox_2_unscaled: 0.0095 (0.0093)  loss_giou_2_unscaled: 0.1895 (0.1962)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0066 (0.0066)  loss_ce_3_unscaled: 0.0587 (0.0586)  loss_bbox_3_unscaled: 0.0094 (0.0094)  loss_giou_3_unscaled: 0.1885 (0.1962)  loss_xy_3_unscaled: 0.0029 (0.0028)  loss_hw_3_unscaled: 0.0065 (0.0066)  loss_ce_4_unscaled: 0.0532 (0.0553)  loss_bbox_4_unscaled: 0.0094 (0.0094)  loss_giou_4_unscaled: 0.1905 (0.1968)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0065 (0.0066)  loss_ce_interm_unscaled: 0.0850 (0.0920)  loss_bbox_interm_unscaled: 0.0100 (0.0097)  loss_giou_interm_unscaled: 0.1989 (0.2008)  loss_xy_interm_unscaled: 0.0030 (0.0030)  loss_hw_interm_unscaled: 0.0068 (0.0067)  time: 3.0277  data: 0.0169  max mem: 8948\n",
            "Epoch: [5]  [40/90]  eta: 0:02:27  lr: 0.000010  loss: 3.9782 (4.0103)  loss_ce: 0.0919 (0.1097)  loss_bbox: 0.0477 (0.0472)  loss_giou: 0.3870 (0.3950)  loss_ce_0: 0.1289 (0.1376)  loss_bbox_0: 0.0482 (0.0476)  loss_giou_0: 0.3889 (0.3958)  loss_ce_1: 0.1189 (0.1253)  loss_bbox_1: 0.0479 (0.0475)  loss_giou_1: 0.3897 (0.3965)  loss_ce_2: 0.1196 (0.1223)  loss_bbox_2: 0.0480 (0.0472)  loss_giou_2: 0.3900 (0.3945)  loss_ce_3: 0.1205 (0.1165)  loss_bbox_3: 0.0473 (0.0474)  loss_giou_3: 0.3892 (0.3942)  loss_ce_4: 0.1143 (0.1110)  loss_bbox_4: 0.0474 (0.0472)  loss_giou_4: 0.3878 (0.3951)  loss_ce_interm: 0.1750 (0.1815)  loss_bbox_interm: 0.0498 (0.0490)  loss_giou_interm: 0.3886 (0.4023)  loss_ce_unscaled: 0.0460 (0.0548)  loss_bbox_unscaled: 0.0095 (0.0094)  loss_giou_unscaled: 0.1935 (0.1975)  loss_xy_unscaled: 0.0030 (0.0028)  loss_hw_unscaled: 0.0063 (0.0066)  loss_ce_0_unscaled: 0.0644 (0.0688)  loss_bbox_0_unscaled: 0.0096 (0.0095)  loss_giou_0_unscaled: 0.1944 (0.1979)  loss_xy_0_unscaled: 0.0031 (0.0029)  loss_hw_0_unscaled: 0.0065 (0.0067)  loss_ce_1_unscaled: 0.0594 (0.0626)  loss_bbox_1_unscaled: 0.0096 (0.0095)  loss_giou_1_unscaled: 0.1948 (0.1983)  loss_xy_1_unscaled: 0.0031 (0.0028)  loss_hw_1_unscaled: 0.0064 (0.0067)  loss_ce_2_unscaled: 0.0598 (0.0611)  loss_bbox_2_unscaled: 0.0096 (0.0094)  loss_giou_2_unscaled: 0.1950 (0.1972)  loss_xy_2_unscaled: 0.0030 (0.0028)  loss_hw_2_unscaled: 0.0065 (0.0066)  loss_ce_3_unscaled: 0.0602 (0.0583)  loss_bbox_3_unscaled: 0.0095 (0.0095)  loss_giou_3_unscaled: 0.1946 (0.1971)  loss_xy_3_unscaled: 0.0030 (0.0028)  loss_hw_3_unscaled: 0.0064 (0.0067)  loss_ce_4_unscaled: 0.0572 (0.0555)  loss_bbox_4_unscaled: 0.0095 (0.0094)  loss_giou_4_unscaled: 0.1939 (0.1976)  loss_xy_4_unscaled: 0.0030 (0.0028)  loss_hw_4_unscaled: 0.0063 (0.0066)  loss_ce_interm_unscaled: 0.0875 (0.0908)  loss_bbox_interm_unscaled: 0.0100 (0.0098)  loss_giou_interm_unscaled: 0.1943 (0.2011)  loss_xy_interm_unscaled: 0.0031 (0.0030)  loss_hw_interm_unscaled: 0.0068 (0.0068)  time: 2.9959  data: 0.0171  max mem: 8948\n",
            "Epoch: [5]  [50/90]  eta: 0:01:57  lr: 0.000010  loss: 3.8624 (3.9834)  loss_ce: 0.1003 (0.1095)  loss_bbox: 0.0483 (0.0471)  loss_giou: 0.3841 (0.3922)  loss_ce_0: 0.1214 (0.1365)  loss_bbox_0: 0.0482 (0.0475)  loss_giou_0: 0.3832 (0.3930)  loss_ce_1: 0.1127 (0.1245)  loss_bbox_1: 0.0479 (0.0473)  loss_giou_1: 0.3879 (0.3934)  loss_ce_2: 0.1074 (0.1205)  loss_bbox_2: 0.0480 (0.0471)  loss_giou_2: 0.3834 (0.3917)  loss_ce_3: 0.1082 (0.1158)  loss_bbox_3: 0.0477 (0.0472)  loss_giou_3: 0.3772 (0.3912)  loss_ce_4: 0.1011 (0.1104)  loss_bbox_4: 0.0482 (0.0470)  loss_giou_4: 0.3847 (0.3923)  loss_ce_interm: 0.1732 (0.1798)  loss_bbox_interm: 0.0498 (0.0489)  loss_giou_interm: 0.3822 (0.4004)  loss_ce_unscaled: 0.0502 (0.0548)  loss_bbox_unscaled: 0.0097 (0.0094)  loss_giou_unscaled: 0.1921 (0.1961)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0064 (0.0066)  loss_ce_0_unscaled: 0.0607 (0.0683)  loss_bbox_0_unscaled: 0.0096 (0.0095)  loss_giou_0_unscaled: 0.1916 (0.1965)  loss_xy_0_unscaled: 0.0029 (0.0028)  loss_hw_0_unscaled: 0.0065 (0.0067)  loss_ce_1_unscaled: 0.0564 (0.0622)  loss_bbox_1_unscaled: 0.0096 (0.0095)  loss_giou_1_unscaled: 0.1939 (0.1967)  loss_xy_1_unscaled: 0.0029 (0.0028)  loss_hw_1_unscaled: 0.0064 (0.0067)  loss_ce_2_unscaled: 0.0537 (0.0602)  loss_bbox_2_unscaled: 0.0096 (0.0094)  loss_giou_2_unscaled: 0.1917 (0.1959)  loss_xy_2_unscaled: 0.0029 (0.0028)  loss_hw_2_unscaled: 0.0065 (0.0066)  loss_ce_3_unscaled: 0.0541 (0.0579)  loss_bbox_3_unscaled: 0.0095 (0.0094)  loss_giou_3_unscaled: 0.1886 (0.1956)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0065 (0.0066)  loss_ce_4_unscaled: 0.0506 (0.0552)  loss_bbox_4_unscaled: 0.0096 (0.0094)  loss_giou_4_unscaled: 0.1924 (0.1961)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0064 (0.0066)  loss_ce_interm_unscaled: 0.0866 (0.0899)  loss_bbox_interm_unscaled: 0.0100 (0.0098)  loss_giou_interm_unscaled: 0.1911 (0.2002)  loss_xy_interm_unscaled: 0.0029 (0.0030)  loss_hw_interm_unscaled: 0.0068 (0.0068)  time: 2.9390  data: 0.0159  max mem: 8948\n",
            "Epoch: [5]  [60/90]  eta: 0:01:27  lr: 0.000010  loss: 3.8365 (3.9700)  loss_ce: 0.0938 (0.1060)  loss_bbox: 0.0459 (0.0471)  loss_giou: 0.3881 (0.3928)  loss_ce_0: 0.1243 (0.1354)  loss_bbox_0: 0.0462 (0.0476)  loss_giou_0: 0.3833 (0.3934)  loss_ce_1: 0.1136 (0.1224)  loss_bbox_1: 0.0448 (0.0474)  loss_giou_1: 0.3785 (0.3935)  loss_ce_2: 0.1022 (0.1180)  loss_bbox_2: 0.0463 (0.0472)  loss_giou_2: 0.3767 (0.3922)  loss_ce_3: 0.0964 (0.1129)  loss_bbox_3: 0.0449 (0.0472)  loss_giou_3: 0.3840 (0.3915)  loss_ce_4: 0.0987 (0.1088)  loss_bbox_4: 0.0453 (0.0471)  loss_giou_4: 0.3864 (0.3922)  loss_ce_interm: 0.1732 (0.1784)  loss_bbox_interm: 0.0467 (0.0489)  loss_giou_interm: 0.3847 (0.4000)  loss_ce_unscaled: 0.0469 (0.0530)  loss_bbox_unscaled: 0.0092 (0.0094)  loss_giou_unscaled: 0.1940 (0.1964)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0063 (0.0066)  loss_ce_0_unscaled: 0.0621 (0.0677)  loss_bbox_0_unscaled: 0.0092 (0.0095)  loss_giou_0_unscaled: 0.1916 (0.1967)  loss_xy_0_unscaled: 0.0029 (0.0028)  loss_hw_0_unscaled: 0.0064 (0.0067)  loss_ce_1_unscaled: 0.0568 (0.0612)  loss_bbox_1_unscaled: 0.0090 (0.0095)  loss_giou_1_unscaled: 0.1893 (0.1968)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0061 (0.0067)  loss_ce_2_unscaled: 0.0511 (0.0590)  loss_bbox_2_unscaled: 0.0093 (0.0094)  loss_giou_2_unscaled: 0.1883 (0.1961)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0062 (0.0066)  loss_ce_3_unscaled: 0.0482 (0.0565)  loss_bbox_3_unscaled: 0.0090 (0.0094)  loss_giou_3_unscaled: 0.1920 (0.1958)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0062 (0.0066)  loss_ce_4_unscaled: 0.0493 (0.0544)  loss_bbox_4_unscaled: 0.0091 (0.0094)  loss_giou_4_unscaled: 0.1932 (0.1961)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0062 (0.0066)  loss_ce_interm_unscaled: 0.0866 (0.0892)  loss_bbox_interm_unscaled: 0.0093 (0.0098)  loss_giou_interm_unscaled: 0.1924 (0.2000)  loss_xy_interm_unscaled: 0.0029 (0.0030)  loss_hw_interm_unscaled: 0.0063 (0.0068)  time: 2.8756  data: 0.0154  max mem: 8948\n",
            "Epoch: [5]  [70/90]  eta: 0:00:58  lr: 0.000010  loss: 3.7616 (3.9617)  loss_ce: 0.0788 (0.1044)  loss_bbox: 0.0457 (0.0472)  loss_giou: 0.3946 (0.3928)  loss_ce_0: 0.1243 (0.1359)  loss_bbox_0: 0.0453 (0.0476)  loss_giou_0: 0.3950 (0.3932)  loss_ce_1: 0.1037 (0.1215)  loss_bbox_1: 0.0453 (0.0474)  loss_giou_1: 0.3914 (0.3938)  loss_ce_2: 0.0901 (0.1157)  loss_bbox_2: 0.0447 (0.0472)  loss_giou_2: 0.3902 (0.3924)  loss_ce_3: 0.0866 (0.1107)  loss_bbox_3: 0.0452 (0.0473)  loss_giou_3: 0.3917 (0.3922)  loss_ce_4: 0.0837 (0.1066)  loss_bbox_4: 0.0453 (0.0472)  loss_giou_4: 0.3935 (0.3925)  loss_ce_interm: 0.1721 (0.1764)  loss_bbox_interm: 0.0482 (0.0492)  loss_giou_interm: 0.3847 (0.4006)  loss_ce_unscaled: 0.0394 (0.0522)  loss_bbox_unscaled: 0.0091 (0.0094)  loss_giou_unscaled: 0.1973 (0.1964)  loss_xy_unscaled: 0.0029 (0.0028)  loss_hw_unscaled: 0.0061 (0.0066)  loss_ce_0_unscaled: 0.0621 (0.0680)  loss_bbox_0_unscaled: 0.0091 (0.0095)  loss_giou_0_unscaled: 0.1975 (0.1966)  loss_xy_0_unscaled: 0.0029 (0.0028)  loss_hw_0_unscaled: 0.0061 (0.0067)  loss_ce_1_unscaled: 0.0518 (0.0608)  loss_bbox_1_unscaled: 0.0091 (0.0095)  loss_giou_1_unscaled: 0.1957 (0.1969)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0061 (0.0067)  loss_ce_2_unscaled: 0.0451 (0.0579)  loss_bbox_2_unscaled: 0.0089 (0.0094)  loss_giou_2_unscaled: 0.1951 (0.1962)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0061 (0.0066)  loss_ce_3_unscaled: 0.0433 (0.0554)  loss_bbox_3_unscaled: 0.0090 (0.0095)  loss_giou_3_unscaled: 0.1958 (0.1961)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0062 (0.0067)  loss_ce_4_unscaled: 0.0419 (0.0533)  loss_bbox_4_unscaled: 0.0091 (0.0094)  loss_giou_4_unscaled: 0.1968 (0.1962)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0061 (0.0066)  loss_ce_interm_unscaled: 0.0861 (0.0882)  loss_bbox_interm_unscaled: 0.0096 (0.0098)  loss_giou_interm_unscaled: 0.1924 (0.2003)  loss_xy_interm_unscaled: 0.0029 (0.0030)  loss_hw_interm_unscaled: 0.0067 (0.0069)  time: 2.8864  data: 0.0153  max mem: 8948\n",
            "Epoch: [5]  [80/90]  eta: 0:00:29  lr: 0.000010  loss: 3.8701 (4.0259)  loss_ce: 0.0968 (0.1108)  loss_bbox: 0.0488 (0.0485)  loss_giou: 0.3942 (0.3951)  loss_ce_0: 0.1359 (0.1407)  loss_bbox_0: 0.0493 (0.0489)  loss_giou_0: 0.3976 (0.3959)  loss_ce_1: 0.1092 (0.1265)  loss_bbox_1: 0.0488 (0.0485)  loss_giou_1: 0.4004 (0.3963)  loss_ce_2: 0.0937 (0.1212)  loss_bbox_2: 0.0496 (0.0486)  loss_giou_2: 0.4010 (0.3956)  loss_ce_3: 0.1001 (0.1163)  loss_bbox_3: 0.0494 (0.0487)  loss_giou_3: 0.4017 (0.3945)  loss_ce_4: 0.0987 (0.1121)  loss_bbox_4: 0.0484 (0.0485)  loss_giou_4: 0.3923 (0.3951)  loss_ce_interm: 0.1677 (0.1801)  loss_bbox_interm: 0.0498 (0.0504)  loss_giou_interm: 0.4085 (0.4034)  loss_ce_unscaled: 0.0484 (0.0554)  loss_bbox_unscaled: 0.0098 (0.0097)  loss_giou_unscaled: 0.1971 (0.1976)  loss_xy_unscaled: 0.0029 (0.0028)  loss_hw_unscaled: 0.0069 (0.0069)  loss_ce_0_unscaled: 0.0680 (0.0703)  loss_bbox_0_unscaled: 0.0099 (0.0098)  loss_giou_0_unscaled: 0.1988 (0.1979)  loss_xy_0_unscaled: 0.0030 (0.0029)  loss_hw_0_unscaled: 0.0069 (0.0069)  loss_ce_1_unscaled: 0.0546 (0.0633)  loss_bbox_1_unscaled: 0.0098 (0.0097)  loss_giou_1_unscaled: 0.2002 (0.1982)  loss_xy_1_unscaled: 0.0028 (0.0029)  loss_hw_1_unscaled: 0.0069 (0.0069)  loss_ce_2_unscaled: 0.0468 (0.0606)  loss_bbox_2_unscaled: 0.0099 (0.0097)  loss_giou_2_unscaled: 0.2005 (0.1978)  loss_xy_2_unscaled: 0.0029 (0.0029)  loss_hw_2_unscaled: 0.0068 (0.0069)  loss_ce_3_unscaled: 0.0501 (0.0582)  loss_bbox_3_unscaled: 0.0099 (0.0097)  loss_giou_3_unscaled: 0.2008 (0.1973)  loss_xy_3_unscaled: 0.0029 (0.0029)  loss_hw_3_unscaled: 0.0070 (0.0069)  loss_ce_4_unscaled: 0.0494 (0.0560)  loss_bbox_4_unscaled: 0.0097 (0.0097)  loss_giou_4_unscaled: 0.1962 (0.1976)  loss_xy_4_unscaled: 0.0029 (0.0029)  loss_hw_4_unscaled: 0.0069 (0.0068)  loss_ce_interm_unscaled: 0.0838 (0.0901)  loss_bbox_interm_unscaled: 0.0100 (0.0101)  loss_giou_interm_unscaled: 0.2043 (0.2017)  loss_xy_interm_unscaled: 0.0032 (0.0030)  loss_hw_interm_unscaled: 0.0073 (0.0071)  time: 2.9524  data: 0.0162  max mem: 8948\n",
            "Epoch: [5]  [89/90]  eta: 0:00:02  lr: 0.000010  loss: 3.9038 (4.0130)  loss_ce: 0.0968 (0.1090)  loss_bbox: 0.0488 (0.0485)  loss_giou: 0.3912 (0.3951)  loss_ce_0: 0.1233 (0.1375)  loss_bbox_0: 0.0502 (0.0489)  loss_giou_0: 0.3951 (0.3966)  loss_ce_1: 0.1058 (0.1243)  loss_bbox_1: 0.0488 (0.0485)  loss_giou_1: 0.3851 (0.3966)  loss_ce_2: 0.0992 (0.1193)  loss_bbox_2: 0.0502 (0.0486)  loss_giou_2: 0.3831 (0.3955)  loss_ce_3: 0.0971 (0.1143)  loss_bbox_3: 0.0503 (0.0486)  loss_giou_3: 0.3823 (0.3945)  loss_ce_4: 0.0987 (0.1107)  loss_bbox_4: 0.0484 (0.0485)  loss_giou_4: 0.3841 (0.3948)  loss_ce_interm: 0.1677 (0.1796)  loss_bbox_interm: 0.0491 (0.0503)  loss_giou_interm: 0.4085 (0.4033)  loss_ce_unscaled: 0.0484 (0.0545)  loss_bbox_unscaled: 0.0098 (0.0097)  loss_giou_unscaled: 0.1956 (0.1975)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0069 (0.0068)  loss_ce_0_unscaled: 0.0617 (0.0688)  loss_bbox_0_unscaled: 0.0100 (0.0098)  loss_giou_0_unscaled: 0.1975 (0.1983)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0069 (0.0069)  loss_ce_1_unscaled: 0.0529 (0.0622)  loss_bbox_1_unscaled: 0.0098 (0.0097)  loss_giou_1_unscaled: 0.1925 (0.1983)  loss_xy_1_unscaled: 0.0028 (0.0029)  loss_hw_1_unscaled: 0.0069 (0.0069)  loss_ce_2_unscaled: 0.0496 (0.0597)  loss_bbox_2_unscaled: 0.0100 (0.0097)  loss_giou_2_unscaled: 0.1916 (0.1978)  loss_xy_2_unscaled: 0.0028 (0.0029)  loss_hw_2_unscaled: 0.0068 (0.0069)  loss_ce_3_unscaled: 0.0486 (0.0572)  loss_bbox_3_unscaled: 0.0101 (0.0097)  loss_giou_3_unscaled: 0.1912 (0.1972)  loss_xy_3_unscaled: 0.0028 (0.0029)  loss_hw_3_unscaled: 0.0070 (0.0069)  loss_ce_4_unscaled: 0.0494 (0.0554)  loss_bbox_4_unscaled: 0.0097 (0.0097)  loss_giou_4_unscaled: 0.1920 (0.1974)  loss_xy_4_unscaled: 0.0028 (0.0029)  loss_hw_4_unscaled: 0.0070 (0.0068)  loss_ce_interm_unscaled: 0.0838 (0.0898)  loss_bbox_interm_unscaled: 0.0098 (0.0101)  loss_giou_interm_unscaled: 0.2043 (0.2017)  loss_xy_interm_unscaled: 0.0030 (0.0030)  loss_hw_interm_unscaled: 0.0068 (0.0071)  time: 2.9519  data: 0.0156  max mem: 8948\n",
            "Epoch: [5] Total time: 0:04:23 (2.9325 s / it)\n",
            "Averaged stats: lr: 0.000010  loss: 3.9038 (4.0130)  loss_ce: 0.0968 (0.1090)  loss_bbox: 0.0488 (0.0485)  loss_giou: 0.3912 (0.3951)  loss_ce_0: 0.1233 (0.1375)  loss_bbox_0: 0.0502 (0.0489)  loss_giou_0: 0.3951 (0.3966)  loss_ce_1: 0.1058 (0.1243)  loss_bbox_1: 0.0488 (0.0485)  loss_giou_1: 0.3851 (0.3966)  loss_ce_2: 0.0992 (0.1193)  loss_bbox_2: 0.0502 (0.0486)  loss_giou_2: 0.3831 (0.3955)  loss_ce_3: 0.0971 (0.1143)  loss_bbox_3: 0.0503 (0.0486)  loss_giou_3: 0.3823 (0.3945)  loss_ce_4: 0.0987 (0.1107)  loss_bbox_4: 0.0484 (0.0485)  loss_giou_4: 0.3841 (0.3948)  loss_ce_interm: 0.1677 (0.1796)  loss_bbox_interm: 0.0491 (0.0503)  loss_giou_interm: 0.4085 (0.4033)  loss_ce_unscaled: 0.0484 (0.0545)  loss_bbox_unscaled: 0.0098 (0.0097)  loss_giou_unscaled: 0.1956 (0.1975)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0069 (0.0068)  loss_ce_0_unscaled: 0.0617 (0.0688)  loss_bbox_0_unscaled: 0.0100 (0.0098)  loss_giou_0_unscaled: 0.1975 (0.1983)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0069 (0.0069)  loss_ce_1_unscaled: 0.0529 (0.0622)  loss_bbox_1_unscaled: 0.0098 (0.0097)  loss_giou_1_unscaled: 0.1925 (0.1983)  loss_xy_1_unscaled: 0.0028 (0.0029)  loss_hw_1_unscaled: 0.0069 (0.0069)  loss_ce_2_unscaled: 0.0496 (0.0597)  loss_bbox_2_unscaled: 0.0100 (0.0097)  loss_giou_2_unscaled: 0.1916 (0.1978)  loss_xy_2_unscaled: 0.0028 (0.0029)  loss_hw_2_unscaled: 0.0068 (0.0069)  loss_ce_3_unscaled: 0.0486 (0.0572)  loss_bbox_3_unscaled: 0.0101 (0.0097)  loss_giou_3_unscaled: 0.1912 (0.1972)  loss_xy_3_unscaled: 0.0028 (0.0029)  loss_hw_3_unscaled: 0.0070 (0.0069)  loss_ce_4_unscaled: 0.0494 (0.0554)  loss_bbox_4_unscaled: 0.0097 (0.0097)  loss_giou_4_unscaled: 0.1920 (0.1974)  loss_xy_4_unscaled: 0.0028 (0.0029)  loss_hw_4_unscaled: 0.0070 (0.0068)  loss_ce_interm_unscaled: 0.0838 (0.0898)  loss_bbox_interm_unscaled: 0.0098 (0.0101)  loss_giou_interm_unscaled: 0.2043 (0.2017)  loss_xy_interm_unscaled: 0.0030 (0.0030)  loss_hw_interm_unscaled: 0.0068 (0.0071)\n",
            "Input text prompt: bolt . wrong direction . 1 . 2 . 3 . 4 . 5 .\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Test:  [ 0/10]  eta: 0:00:18    time: 1.8735  data: 0.9390  max mem: 8948\n",
            "Test:  [ 9/10]  eta: 0:00:01    time: 1.0292  data: 0.1056  max mem: 8948\n",
            "Test: Total time: 0:00:10 (1.0381 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.059\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.134\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.040\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.284\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.200\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.018\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.263\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.498\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.452\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.655\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.495\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch: [6]  [ 0/90]  eta: 0:05:20  lr: 0.000010  loss: 3.8346 (3.8346)  loss_ce: 0.0895 (0.0895)  loss_bbox: 0.0484 (0.0484)  loss_giou: 0.3952 (0.3952)  loss_ce_0: 0.1402 (0.1402)  loss_bbox_0: 0.0472 (0.0472)  loss_giou_0: 0.3706 (0.3706)  loss_ce_1: 0.0966 (0.0966)  loss_bbox_1: 0.0484 (0.0484)  loss_giou_1: 0.3989 (0.3989)  loss_ce_2: 0.0915 (0.0915)  loss_bbox_2: 0.0488 (0.0488)  loss_giou_2: 0.4020 (0.4020)  loss_ce_3: 0.0927 (0.0927)  loss_bbox_3: 0.0486 (0.0486)  loss_giou_3: 0.3979 (0.3979)  loss_ce_4: 0.0872 (0.0872)  loss_bbox_4: 0.0484 (0.0484)  loss_giou_4: 0.3943 (0.3943)  loss_ce_interm: 0.1609 (0.1609)  loss_bbox_interm: 0.0465 (0.0465)  loss_giou_interm: 0.3807 (0.3807)  loss_ce_unscaled: 0.0447 (0.0447)  loss_bbox_unscaled: 0.0097 (0.0097)  loss_giou_unscaled: 0.1976 (0.1976)  loss_xy_unscaled: 0.0030 (0.0030)  loss_hw_unscaled: 0.0066 (0.0066)  loss_ce_0_unscaled: 0.0701 (0.0701)  loss_bbox_0_unscaled: 0.0094 (0.0094)  loss_giou_0_unscaled: 0.1853 (0.1853)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0065 (0.0065)  loss_ce_1_unscaled: 0.0483 (0.0483)  loss_bbox_1_unscaled: 0.0097 (0.0097)  loss_giou_1_unscaled: 0.1995 (0.1995)  loss_xy_1_unscaled: 0.0031 (0.0031)  loss_hw_1_unscaled: 0.0066 (0.0066)  loss_ce_2_unscaled: 0.0458 (0.0458)  loss_bbox_2_unscaled: 0.0098 (0.0098)  loss_giou_2_unscaled: 0.2010 (0.2010)  loss_xy_2_unscaled: 0.0030 (0.0030)  loss_hw_2_unscaled: 0.0067 (0.0067)  loss_ce_3_unscaled: 0.0464 (0.0464)  loss_bbox_3_unscaled: 0.0097 (0.0097)  loss_giou_3_unscaled: 0.1990 (0.1990)  loss_xy_3_unscaled: 0.0030 (0.0030)  loss_hw_3_unscaled: 0.0067 (0.0067)  loss_ce_4_unscaled: 0.0436 (0.0436)  loss_bbox_4_unscaled: 0.0097 (0.0097)  loss_giou_4_unscaled: 0.1971 (0.1971)  loss_xy_4_unscaled: 0.0030 (0.0030)  loss_hw_4_unscaled: 0.0066 (0.0066)  loss_ce_interm_unscaled: 0.0804 (0.0804)  loss_bbox_interm_unscaled: 0.0093 (0.0093)  loss_giou_interm_unscaled: 0.1904 (0.1904)  loss_xy_interm_unscaled: 0.0030 (0.0030)  loss_hw_interm_unscaled: 0.0063 (0.0063)  time: 3.5645  data: 0.7747  max mem: 8948\n",
            "Epoch: [6]  [10/90]  eta: 0:03:57  lr: 0.000010  loss: 4.0722 (4.2159)  loss_ce: 0.1144 (0.1247)  loss_bbox: 0.0484 (0.0490)  loss_giou: 0.3952 (0.4028)  loss_ce_0: 0.1402 (0.1646)  loss_bbox_0: 0.0486 (0.0495)  loss_giou_0: 0.3875 (0.4048)  loss_ce_1: 0.1285 (0.1399)  loss_bbox_1: 0.0484 (0.0493)  loss_giou_1: 0.3928 (0.4127)  loss_ce_2: 0.1239 (0.1331)  loss_bbox_2: 0.0485 (0.0490)  loss_giou_2: 0.4011 (0.4098)  loss_ce_3: 0.1113 (0.1301)  loss_bbox_3: 0.0486 (0.0492)  loss_giou_3: 0.3959 (0.4060)  loss_ce_4: 0.1154 (0.1290)  loss_bbox_4: 0.0484 (0.0489)  loss_giou_4: 0.3949 (0.4040)  loss_ce_interm: 0.1976 (0.1994)  loss_bbox_interm: 0.0499 (0.0497)  loss_giou_interm: 0.4023 (0.4105)  loss_ce_unscaled: 0.0572 (0.0623)  loss_bbox_unscaled: 0.0097 (0.0098)  loss_giou_unscaled: 0.1976 (0.2014)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0068 (0.0070)  loss_ce_0_unscaled: 0.0701 (0.0823)  loss_bbox_0_unscaled: 0.0097 (0.0099)  loss_giou_0_unscaled: 0.1937 (0.2024)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0069 (0.0070)  loss_ce_1_unscaled: 0.0643 (0.0700)  loss_bbox_1_unscaled: 0.0097 (0.0099)  loss_giou_1_unscaled: 0.1964 (0.2064)  loss_xy_1_unscaled: 0.0030 (0.0029)  loss_hw_1_unscaled: 0.0068 (0.0070)  loss_ce_2_unscaled: 0.0619 (0.0665)  loss_bbox_2_unscaled: 0.0097 (0.0098)  loss_giou_2_unscaled: 0.2005 (0.2049)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0067 (0.0070)  loss_ce_3_unscaled: 0.0556 (0.0651)  loss_bbox_3_unscaled: 0.0097 (0.0098)  loss_giou_3_unscaled: 0.1979 (0.2030)  loss_xy_3_unscaled: 0.0029 (0.0028)  loss_hw_3_unscaled: 0.0068 (0.0070)  loss_ce_4_unscaled: 0.0577 (0.0645)  loss_bbox_4_unscaled: 0.0097 (0.0098)  loss_giou_4_unscaled: 0.1974 (0.2020)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0068 (0.0070)  loss_ce_interm_unscaled: 0.0988 (0.0997)  loss_bbox_interm_unscaled: 0.0100 (0.0099)  loss_giou_interm_unscaled: 0.2012 (0.2053)  loss_xy_interm_unscaled: 0.0030 (0.0029)  loss_hw_interm_unscaled: 0.0073 (0.0071)  time: 2.9699  data: 0.0851  max mem: 8948\n",
            "Epoch: [6]  [20/90]  eta: 0:03:26  lr: 0.000010  loss: 3.8602 (3.9857)  loss_ce: 0.0905 (0.1078)  loss_bbox: 0.0475 (0.0480)  loss_giou: 0.3807 (0.3890)  loss_ce_0: 0.1230 (0.1455)  loss_bbox_0: 0.0482 (0.0485)  loss_giou_0: 0.3822 (0.3922)  loss_ce_1: 0.1090 (0.1225)  loss_bbox_1: 0.0473 (0.0485)  loss_giou_1: 0.3867 (0.3957)  loss_ce_2: 0.0971 (0.1145)  loss_bbox_2: 0.0471 (0.0481)  loss_giou_2: 0.3871 (0.3936)  loss_ce_3: 0.0999 (0.1140)  loss_bbox_3: 0.0481 (0.0483)  loss_giou_3: 0.3818 (0.3908)  loss_ce_4: 0.0920 (0.1091)  loss_bbox_4: 0.0473 (0.0479)  loss_giou_4: 0.3810 (0.3905)  loss_ce_interm: 0.1666 (0.1854)  loss_bbox_interm: 0.0492 (0.0495)  loss_giou_interm: 0.3883 (0.3962)  loss_ce_unscaled: 0.0452 (0.0539)  loss_bbox_unscaled: 0.0095 (0.0096)  loss_giou_unscaled: 0.1903 (0.1945)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0068 (0.0068)  loss_ce_0_unscaled: 0.0615 (0.0728)  loss_bbox_0_unscaled: 0.0096 (0.0097)  loss_giou_0_unscaled: 0.1911 (0.1961)  loss_xy_0_unscaled: 0.0029 (0.0028)  loss_hw_0_unscaled: 0.0068 (0.0069)  loss_ce_1_unscaled: 0.0545 (0.0613)  loss_bbox_1_unscaled: 0.0095 (0.0097)  loss_giou_1_unscaled: 0.1933 (0.1979)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0068 (0.0069)  loss_ce_2_unscaled: 0.0486 (0.0572)  loss_bbox_2_unscaled: 0.0094 (0.0096)  loss_giou_2_unscaled: 0.1935 (0.1968)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0067 (0.0068)  loss_ce_3_unscaled: 0.0499 (0.0570)  loss_bbox_3_unscaled: 0.0096 (0.0097)  loss_giou_3_unscaled: 0.1909 (0.1954)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0068 (0.0069)  loss_ce_4_unscaled: 0.0460 (0.0546)  loss_bbox_4_unscaled: 0.0095 (0.0096)  loss_giou_4_unscaled: 0.1905 (0.1953)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0067 (0.0068)  loss_ce_interm_unscaled: 0.0833 (0.0927)  loss_bbox_interm_unscaled: 0.0098 (0.0099)  loss_giou_interm_unscaled: 0.1942 (0.1981)  loss_xy_interm_unscaled: 0.0027 (0.0029)  loss_hw_interm_unscaled: 0.0071 (0.0070)  time: 2.9207  data: 0.0149  max mem: 8948\n",
            "Epoch: [6]  [30/90]  eta: 0:02:53  lr: 0.000010  loss: 3.8140 (3.9316)  loss_ce: 0.0761 (0.0986)  loss_bbox: 0.0460 (0.0475)  loss_giou: 0.3832 (0.3900)  loss_ce_0: 0.1106 (0.1353)  loss_bbox_0: 0.0466 (0.0480)  loss_giou_0: 0.3827 (0.3947)  loss_ce_1: 0.1000 (0.1153)  loss_bbox_1: 0.0464 (0.0480)  loss_giou_1: 0.3855 (0.3968)  loss_ce_2: 0.0899 (0.1089)  loss_bbox_2: 0.0464 (0.0477)  loss_giou_2: 0.3815 (0.3944)  loss_ce_3: 0.0865 (0.1064)  loss_bbox_3: 0.0461 (0.0477)  loss_giou_3: 0.3801 (0.3913)  loss_ce_4: 0.0772 (0.1003)  loss_bbox_4: 0.0460 (0.0475)  loss_giou_4: 0.3836 (0.3911)  loss_ce_interm: 0.1514 (0.1760)  loss_bbox_interm: 0.0473 (0.0488)  loss_giou_interm: 0.3821 (0.3971)  loss_ce_unscaled: 0.0381 (0.0493)  loss_bbox_unscaled: 0.0092 (0.0095)  loss_giou_unscaled: 0.1916 (0.1950)  loss_xy_unscaled: 0.0027 (0.0027)  loss_hw_unscaled: 0.0066 (0.0068)  loss_ce_0_unscaled: 0.0553 (0.0677)  loss_bbox_0_unscaled: 0.0093 (0.0096)  loss_giou_0_unscaled: 0.1914 (0.1973)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0066 (0.0068)  loss_ce_1_unscaled: 0.0500 (0.0576)  loss_bbox_1_unscaled: 0.0093 (0.0096)  loss_giou_1_unscaled: 0.1927 (0.1984)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0066 (0.0068)  loss_ce_2_unscaled: 0.0450 (0.0545)  loss_bbox_2_unscaled: 0.0093 (0.0095)  loss_giou_2_unscaled: 0.1907 (0.1972)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0066 (0.0068)  loss_ce_3_unscaled: 0.0433 (0.0532)  loss_bbox_3_unscaled: 0.0092 (0.0095)  loss_giou_3_unscaled: 0.1900 (0.1957)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0066 (0.0068)  loss_ce_4_unscaled: 0.0386 (0.0502)  loss_bbox_4_unscaled: 0.0092 (0.0095)  loss_giou_4_unscaled: 0.1918 (0.1955)  loss_xy_4_unscaled: 0.0027 (0.0027)  loss_hw_4_unscaled: 0.0067 (0.0068)  loss_ce_interm_unscaled: 0.0757 (0.0880)  loss_bbox_interm_unscaled: 0.0095 (0.0098)  loss_giou_interm_unscaled: 0.1910 (0.1986)  loss_xy_interm_unscaled: 0.0028 (0.0028)  loss_hw_interm_unscaled: 0.0067 (0.0069)  time: 2.8381  data: 0.0144  max mem: 8948\n",
            "Epoch: [6]  [40/90]  eta: 0:02:24  lr: 0.000010  loss: 3.8770 (3.9404)  loss_ce: 0.0779 (0.0976)  loss_bbox: 0.0473 (0.0485)  loss_giou: 0.3897 (0.3920)  loss_ce_0: 0.1093 (0.1327)  loss_bbox_0: 0.0480 (0.0488)  loss_giou_0: 0.3951 (0.3968)  loss_ce_1: 0.0974 (0.1130)  loss_bbox_1: 0.0483 (0.0488)  loss_giou_1: 0.3999 (0.3989)  loss_ce_2: 0.0983 (0.1080)  loss_bbox_2: 0.0474 (0.0485)  loss_giou_2: 0.3957 (0.3960)  loss_ce_3: 0.0865 (0.1041)  loss_bbox_3: 0.0472 (0.0486)  loss_giou_3: 0.3936 (0.3937)  loss_ce_4: 0.0867 (0.1002)  loss_bbox_4: 0.0474 (0.0484)  loss_giou_4: 0.3889 (0.3925)  loss_ce_interm: 0.1571 (0.1746)  loss_bbox_interm: 0.0494 (0.0496)  loss_giou_interm: 0.3905 (0.3989)  loss_ce_unscaled: 0.0390 (0.0488)  loss_bbox_unscaled: 0.0095 (0.0097)  loss_giou_unscaled: 0.1949 (0.1960)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0067 (0.0069)  loss_ce_0_unscaled: 0.0547 (0.0664)  loss_bbox_0_unscaled: 0.0096 (0.0098)  loss_giou_0_unscaled: 0.1975 (0.1984)  loss_xy_0_unscaled: 0.0028 (0.0029)  loss_hw_0_unscaled: 0.0068 (0.0069)  loss_ce_1_unscaled: 0.0487 (0.0565)  loss_bbox_1_unscaled: 0.0097 (0.0098)  loss_giou_1_unscaled: 0.1999 (0.1994)  loss_xy_1_unscaled: 0.0028 (0.0029)  loss_hw_1_unscaled: 0.0069 (0.0069)  loss_ce_2_unscaled: 0.0491 (0.0540)  loss_bbox_2_unscaled: 0.0095 (0.0097)  loss_giou_2_unscaled: 0.1979 (0.1980)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0068 (0.0069)  loss_ce_3_unscaled: 0.0433 (0.0520)  loss_bbox_3_unscaled: 0.0094 (0.0097)  loss_giou_3_unscaled: 0.1968 (0.1969)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0067 (0.0069)  loss_ce_4_unscaled: 0.0433 (0.0501)  loss_bbox_4_unscaled: 0.0095 (0.0097)  loss_giou_4_unscaled: 0.1945 (0.1962)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0068 (0.0069)  loss_ce_interm_unscaled: 0.0786 (0.0873)  loss_bbox_interm_unscaled: 0.0099 (0.0099)  loss_giou_interm_unscaled: 0.1952 (0.1994)  loss_xy_interm_unscaled: 0.0029 (0.0029)  loss_hw_interm_unscaled: 0.0069 (0.0070)  time: 2.8182  data: 0.0168  max mem: 8948\n",
            "Epoch: [6]  [50/90]  eta: 0:01:54  lr: 0.000010  loss: 3.8829 (3.9315)  loss_ce: 0.0791 (0.0979)  loss_bbox: 0.0499 (0.0485)  loss_giou: 0.3881 (0.3911)  loss_ce_0: 0.1103 (0.1317)  loss_bbox_0: 0.0510 (0.0489)  loss_giou_0: 0.3893 (0.3960)  loss_ce_1: 0.0974 (0.1119)  loss_bbox_1: 0.0496 (0.0488)  loss_giou_1: 0.3945 (0.3973)  loss_ce_2: 0.0983 (0.1084)  loss_bbox_2: 0.0489 (0.0485)  loss_giou_2: 0.3957 (0.3945)  loss_ce_3: 0.0805 (0.1040)  loss_bbox_3: 0.0501 (0.0486)  loss_giou_3: 0.3913 (0.3926)  loss_ce_4: 0.0941 (0.1000)  loss_bbox_4: 0.0499 (0.0484)  loss_giou_4: 0.3869 (0.3918)  loss_ce_interm: 0.1702 (0.1729)  loss_bbox_interm: 0.0520 (0.0498)  loss_giou_interm: 0.3905 (0.4000)  loss_ce_unscaled: 0.0396 (0.0490)  loss_bbox_unscaled: 0.0100 (0.0097)  loss_giou_unscaled: 0.1941 (0.1955)  loss_xy_unscaled: 0.0029 (0.0028)  loss_hw_unscaled: 0.0074 (0.0069)  loss_ce_0_unscaled: 0.0551 (0.0658)  loss_bbox_0_unscaled: 0.0102 (0.0098)  loss_giou_0_unscaled: 0.1946 (0.1980)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0072 (0.0069)  loss_ce_1_unscaled: 0.0487 (0.0560)  loss_bbox_1_unscaled: 0.0099 (0.0098)  loss_giou_1_unscaled: 0.1973 (0.1986)  loss_xy_1_unscaled: 0.0028 (0.0029)  loss_hw_1_unscaled: 0.0073 (0.0069)  loss_ce_2_unscaled: 0.0491 (0.0542)  loss_bbox_2_unscaled: 0.0098 (0.0097)  loss_giou_2_unscaled: 0.1979 (0.1973)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0072 (0.0069)  loss_ce_3_unscaled: 0.0403 (0.0520)  loss_bbox_3_unscaled: 0.0100 (0.0097)  loss_giou_3_unscaled: 0.1957 (0.1963)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0074 (0.0069)  loss_ce_4_unscaled: 0.0471 (0.0500)  loss_bbox_4_unscaled: 0.0100 (0.0097)  loss_giou_4_unscaled: 0.1934 (0.1959)  loss_xy_4_unscaled: 0.0029 (0.0028)  loss_hw_4_unscaled: 0.0074 (0.0069)  loss_ce_interm_unscaled: 0.0851 (0.0864)  loss_bbox_interm_unscaled: 0.0104 (0.0100)  loss_giou_interm_unscaled: 0.1952 (0.2000)  loss_xy_interm_unscaled: 0.0031 (0.0030)  loss_hw_interm_unscaled: 0.0071 (0.0070)  time: 2.8124  data: 0.0165  max mem: 8948\n",
            "Epoch: [6]  [60/90]  eta: 0:01:27  lr: 0.000010  loss: 3.8204 (3.9118)  loss_ce: 0.0762 (0.0966)  loss_bbox: 0.0483 (0.0479)  loss_giou: 0.3850 (0.3913)  loss_ce_0: 0.1155 (0.1287)  loss_bbox_0: 0.0486 (0.0484)  loss_giou_0: 0.3892 (0.3958)  loss_ce_1: 0.1041 (0.1101)  loss_bbox_1: 0.0480 (0.0482)  loss_giou_1: 0.3906 (0.3968)  loss_ce_2: 0.0889 (0.1062)  loss_bbox_2: 0.0485 (0.0480)  loss_giou_2: 0.3894 (0.3944)  loss_ce_3: 0.0740 (0.1024)  loss_bbox_3: 0.0484 (0.0481)  loss_giou_3: 0.3889 (0.3928)  loss_ce_4: 0.0730 (0.0984)  loss_bbox_4: 0.0483 (0.0479)  loss_giou_4: 0.3869 (0.3919)  loss_ce_interm: 0.1615 (0.1699)  loss_bbox_interm: 0.0476 (0.0491)  loss_giou_interm: 0.3970 (0.3988)  loss_ce_unscaled: 0.0381 (0.0483)  loss_bbox_unscaled: 0.0097 (0.0096)  loss_giou_unscaled: 0.1925 (0.1956)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0065 (0.0068)  loss_ce_0_unscaled: 0.0578 (0.0643)  loss_bbox_0_unscaled: 0.0097 (0.0097)  loss_giou_0_unscaled: 0.1946 (0.1979)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0065 (0.0068)  loss_ce_1_unscaled: 0.0520 (0.0550)  loss_bbox_1_unscaled: 0.0096 (0.0096)  loss_giou_1_unscaled: 0.1953 (0.1984)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0065 (0.0068)  loss_ce_2_unscaled: 0.0444 (0.0531)  loss_bbox_2_unscaled: 0.0097 (0.0096)  loss_giou_2_unscaled: 0.1947 (0.1972)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0065 (0.0068)  loss_ce_3_unscaled: 0.0370 (0.0512)  loss_bbox_3_unscaled: 0.0097 (0.0096)  loss_giou_3_unscaled: 0.1945 (0.1964)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0065 (0.0068)  loss_ce_4_unscaled: 0.0365 (0.0492)  loss_bbox_4_unscaled: 0.0097 (0.0096)  loss_giou_4_unscaled: 0.1934 (0.1959)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0065 (0.0068)  loss_ce_interm_unscaled: 0.0807 (0.0850)  loss_bbox_interm_unscaled: 0.0095 (0.0098)  loss_giou_interm_unscaled: 0.1985 (0.1994)  loss_xy_interm_unscaled: 0.0027 (0.0029)  loss_hw_interm_unscaled: 0.0066 (0.0069)  time: 2.9489  data: 0.0156  max mem: 8948\n",
            "Epoch: [6]  [70/90]  eta: 0:00:57  lr: 0.000010  loss: 3.6793 (3.8885)  loss_ce: 0.0664 (0.0941)  loss_bbox: 0.0452 (0.0481)  loss_giou: 0.3849 (0.3897)  loss_ce_0: 0.1123 (0.1265)  loss_bbox_0: 0.0449 (0.0486)  loss_giou_0: 0.3870 (0.3943)  loss_ce_1: 0.0814 (0.1082)  loss_bbox_1: 0.0446 (0.0485)  loss_giou_1: 0.3882 (0.3950)  loss_ce_2: 0.0821 (0.1040)  loss_bbox_2: 0.0450 (0.0482)  loss_giou_2: 0.3886 (0.3927)  loss_ce_3: 0.0730 (0.0998)  loss_bbox_3: 0.0453 (0.0483)  loss_giou_3: 0.3850 (0.3911)  loss_ce_4: 0.0730 (0.0965)  loss_bbox_4: 0.0454 (0.0481)  loss_giou_4: 0.3851 (0.3902)  loss_ce_interm: 0.1612 (0.1687)  loss_bbox_interm: 0.0457 (0.0495)  loss_giou_interm: 0.3805 (0.3982)  loss_ce_unscaled: 0.0332 (0.0470)  loss_bbox_unscaled: 0.0090 (0.0096)  loss_giou_unscaled: 0.1925 (0.1948)  loss_xy_unscaled: 0.0029 (0.0028)  loss_hw_unscaled: 0.0063 (0.0068)  loss_ce_0_unscaled: 0.0561 (0.0633)  loss_bbox_0_unscaled: 0.0090 (0.0097)  loss_giou_0_unscaled: 0.1935 (0.1972)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0065 (0.0068)  loss_ce_1_unscaled: 0.0407 (0.0541)  loss_bbox_1_unscaled: 0.0089 (0.0097)  loss_giou_1_unscaled: 0.1941 (0.1975)  loss_xy_1_unscaled: 0.0029 (0.0029)  loss_hw_1_unscaled: 0.0064 (0.0068)  loss_ce_2_unscaled: 0.0411 (0.0520)  loss_bbox_2_unscaled: 0.0090 (0.0096)  loss_giou_2_unscaled: 0.1943 (0.1964)  loss_xy_2_unscaled: 0.0029 (0.0028)  loss_hw_2_unscaled: 0.0064 (0.0068)  loss_ce_3_unscaled: 0.0365 (0.0499)  loss_bbox_3_unscaled: 0.0091 (0.0097)  loss_giou_3_unscaled: 0.1925 (0.1955)  loss_xy_3_unscaled: 0.0029 (0.0028)  loss_hw_3_unscaled: 0.0064 (0.0068)  loss_ce_4_unscaled: 0.0365 (0.0483)  loss_bbox_4_unscaled: 0.0091 (0.0096)  loss_giou_4_unscaled: 0.1925 (0.1951)  loss_xy_4_unscaled: 0.0029 (0.0028)  loss_hw_4_unscaled: 0.0063 (0.0068)  loss_ce_interm_unscaled: 0.0806 (0.0844)  loss_bbox_interm_unscaled: 0.0091 (0.0099)  loss_giou_interm_unscaled: 0.1902 (0.1991)  loss_xy_interm_unscaled: 0.0028 (0.0030)  loss_hw_interm_unscaled: 0.0065 (0.0069)  time: 3.0014  data: 0.0164  max mem: 8948\n",
            "Epoch: [6]  [80/90]  eta: 0:00:28  lr: 0.000010  loss: 3.7763 (3.8787)  loss_ce: 0.0862 (0.0954)  loss_bbox: 0.0442 (0.0476)  loss_giou: 0.3592 (0.3872)  loss_ce_0: 0.1123 (0.1276)  loss_bbox_0: 0.0443 (0.0482)  loss_giou_0: 0.3642 (0.3922)  loss_ce_1: 0.1031 (0.1090)  loss_bbox_1: 0.0441 (0.0480)  loss_giou_1: 0.3605 (0.3926)  loss_ce_2: 0.0896 (0.1050)  loss_bbox_2: 0.0436 (0.0478)  loss_giou_2: 0.3595 (0.3905)  loss_ce_3: 0.0903 (0.1016)  loss_bbox_3: 0.0447 (0.0478)  loss_giou_3: 0.3570 (0.3886)  loss_ce_4: 0.0880 (0.0984)  loss_bbox_4: 0.0443 (0.0477)  loss_giou_4: 0.3592 (0.3878)  loss_ce_interm: 0.1654 (0.1694)  loss_bbox_interm: 0.0472 (0.0493)  loss_giou_interm: 0.3767 (0.3971)  loss_ce_unscaled: 0.0431 (0.0477)  loss_bbox_unscaled: 0.0088 (0.0095)  loss_giou_unscaled: 0.1796 (0.1936)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0061 (0.0067)  loss_ce_0_unscaled: 0.0561 (0.0638)  loss_bbox_0_unscaled: 0.0089 (0.0096)  loss_giou_0_unscaled: 0.1821 (0.1961)  loss_xy_0_unscaled: 0.0028 (0.0029)  loss_hw_0_unscaled: 0.0063 (0.0068)  loss_ce_1_unscaled: 0.0515 (0.0545)  loss_bbox_1_unscaled: 0.0088 (0.0096)  loss_giou_1_unscaled: 0.1803 (0.1963)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0062 (0.0068)  loss_ce_2_unscaled: 0.0448 (0.0525)  loss_bbox_2_unscaled: 0.0087 (0.0096)  loss_giou_2_unscaled: 0.1797 (0.1952)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0061 (0.0067)  loss_ce_3_unscaled: 0.0451 (0.0508)  loss_bbox_3_unscaled: 0.0089 (0.0096)  loss_giou_3_unscaled: 0.1785 (0.1943)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0063 (0.0067)  loss_ce_4_unscaled: 0.0440 (0.0492)  loss_bbox_4_unscaled: 0.0089 (0.0095)  loss_giou_4_unscaled: 0.1796 (0.1939)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0062 (0.0067)  loss_ce_interm_unscaled: 0.0827 (0.0847)  loss_bbox_interm_unscaled: 0.0094 (0.0099)  loss_giou_interm_unscaled: 0.1883 (0.1985)  loss_xy_interm_unscaled: 0.0028 (0.0030)  loss_hw_interm_unscaled: 0.0065 (0.0069)  time: 2.8547  data: 0.0160  max mem: 8948\n",
            "Epoch: [6]  [89/90]  eta: 0:00:02  lr: 0.000010  loss: 3.7763 (3.8707)  loss_ce: 0.0877 (0.0960)  loss_bbox: 0.0448 (0.0476)  loss_giou: 0.3671 (0.3855)  loss_ce_0: 0.1120 (0.1276)  loss_bbox_0: 0.0461 (0.0482)  loss_giou_0: 0.3801 (0.3910)  loss_ce_1: 0.1044 (0.1096)  loss_bbox_1: 0.0444 (0.0479)  loss_giou_1: 0.3591 (0.3907)  loss_ce_2: 0.0962 (0.1060)  loss_bbox_2: 0.0449 (0.0477)  loss_giou_2: 0.3673 (0.3886)  loss_ce_3: 0.0951 (0.1019)  loss_bbox_3: 0.0449 (0.0477)  loss_giou_3: 0.3688 (0.3868)  loss_ce_4: 0.0921 (0.0992)  loss_bbox_4: 0.0447 (0.0477)  loss_giou_4: 0.3653 (0.3861)  loss_ce_interm: 0.1654 (0.1693)  loss_bbox_interm: 0.0479 (0.0493)  loss_giou_interm: 0.3841 (0.3962)  loss_ce_unscaled: 0.0439 (0.0480)  loss_bbox_unscaled: 0.0090 (0.0095)  loss_giou_unscaled: 0.1835 (0.1928)  loss_xy_unscaled: 0.0026 (0.0028)  loss_hw_unscaled: 0.0061 (0.0067)  loss_ce_0_unscaled: 0.0560 (0.0638)  loss_bbox_0_unscaled: 0.0092 (0.0096)  loss_giou_0_unscaled: 0.1900 (0.1955)  loss_xy_0_unscaled: 0.0026 (0.0029)  loss_hw_0_unscaled: 0.0063 (0.0068)  loss_ce_1_unscaled: 0.0522 (0.0548)  loss_bbox_1_unscaled: 0.0089 (0.0096)  loss_giou_1_unscaled: 0.1795 (0.1953)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0061 (0.0067)  loss_ce_2_unscaled: 0.0481 (0.0530)  loss_bbox_2_unscaled: 0.0090 (0.0095)  loss_giou_2_unscaled: 0.1836 (0.1943)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0061 (0.0067)  loss_ce_3_unscaled: 0.0476 (0.0510)  loss_bbox_3_unscaled: 0.0090 (0.0095)  loss_giou_3_unscaled: 0.1844 (0.1934)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0061 (0.0067)  loss_ce_4_unscaled: 0.0460 (0.0496)  loss_bbox_4_unscaled: 0.0089 (0.0095)  loss_giou_4_unscaled: 0.1827 (0.1930)  loss_xy_4_unscaled: 0.0026 (0.0028)  loss_hw_4_unscaled: 0.0061 (0.0067)  loss_ce_interm_unscaled: 0.0827 (0.0847)  loss_bbox_interm_unscaled: 0.0096 (0.0099)  loss_giou_interm_unscaled: 0.1920 (0.1981)  loss_xy_interm_unscaled: 0.0027 (0.0030)  loss_hw_interm_unscaled: 0.0065 (0.0069)  time: 2.7090  data: 0.0152  max mem: 8948\n",
            "Epoch: [6] Total time: 0:04:16 (2.8542 s / it)\n",
            "Averaged stats: lr: 0.000010  loss: 3.7763 (3.8707)  loss_ce: 0.0877 (0.0960)  loss_bbox: 0.0448 (0.0476)  loss_giou: 0.3671 (0.3855)  loss_ce_0: 0.1120 (0.1276)  loss_bbox_0: 0.0461 (0.0482)  loss_giou_0: 0.3801 (0.3910)  loss_ce_1: 0.1044 (0.1096)  loss_bbox_1: 0.0444 (0.0479)  loss_giou_1: 0.3591 (0.3907)  loss_ce_2: 0.0962 (0.1060)  loss_bbox_2: 0.0449 (0.0477)  loss_giou_2: 0.3673 (0.3886)  loss_ce_3: 0.0951 (0.1019)  loss_bbox_3: 0.0449 (0.0477)  loss_giou_3: 0.3688 (0.3868)  loss_ce_4: 0.0921 (0.0992)  loss_bbox_4: 0.0447 (0.0477)  loss_giou_4: 0.3653 (0.3861)  loss_ce_interm: 0.1654 (0.1693)  loss_bbox_interm: 0.0479 (0.0493)  loss_giou_interm: 0.3841 (0.3962)  loss_ce_unscaled: 0.0439 (0.0480)  loss_bbox_unscaled: 0.0090 (0.0095)  loss_giou_unscaled: 0.1835 (0.1928)  loss_xy_unscaled: 0.0026 (0.0028)  loss_hw_unscaled: 0.0061 (0.0067)  loss_ce_0_unscaled: 0.0560 (0.0638)  loss_bbox_0_unscaled: 0.0092 (0.0096)  loss_giou_0_unscaled: 0.1900 (0.1955)  loss_xy_0_unscaled: 0.0026 (0.0029)  loss_hw_0_unscaled: 0.0063 (0.0068)  loss_ce_1_unscaled: 0.0522 (0.0548)  loss_bbox_1_unscaled: 0.0089 (0.0096)  loss_giou_1_unscaled: 0.1795 (0.1953)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0061 (0.0067)  loss_ce_2_unscaled: 0.0481 (0.0530)  loss_bbox_2_unscaled: 0.0090 (0.0095)  loss_giou_2_unscaled: 0.1836 (0.1943)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0061 (0.0067)  loss_ce_3_unscaled: 0.0476 (0.0510)  loss_bbox_3_unscaled: 0.0090 (0.0095)  loss_giou_3_unscaled: 0.1844 (0.1934)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0061 (0.0067)  loss_ce_4_unscaled: 0.0460 (0.0496)  loss_bbox_4_unscaled: 0.0089 (0.0095)  loss_giou_4_unscaled: 0.1827 (0.1930)  loss_xy_4_unscaled: 0.0026 (0.0028)  loss_hw_4_unscaled: 0.0061 (0.0067)  loss_ce_interm_unscaled: 0.0827 (0.0847)  loss_bbox_interm_unscaled: 0.0096 (0.0099)  loss_giou_interm_unscaled: 0.1920 (0.1981)  loss_xy_interm_unscaled: 0.0027 (0.0030)  loss_hw_interm_unscaled: 0.0065 (0.0069)\n",
            "Input text prompt: bolt . wrong direction . 1 . 2 . 3 . 4 . 5 .\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Test:  [ 0/10]  eta: 0:00:17    time: 1.7342  data: 0.7890  max mem: 8948\n",
            "Test:  [ 9/10]  eta: 0:00:01    time: 1.0249  data: 0.0965  max mem: 8948\n",
            "Test: Total time: 0:00:10 (1.0340 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.059\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.129\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.041\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.060\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.292\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.171\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.270\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.504\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.457\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.653\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.520\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch: [7]  [ 0/90]  eta: 0:06:37  lr: 0.000010  loss: 4.0437 (4.0437)  loss_ce: 0.1250 (0.1250)  loss_bbox: 0.0422 (0.0422)  loss_giou: 0.3838 (0.3838)  loss_ce_0: 0.1656 (0.1656)  loss_bbox_0: 0.0425 (0.0425)  loss_giou_0: 0.3835 (0.3835)  loss_ce_1: 0.1546 (0.1546)  loss_bbox_1: 0.0423 (0.0423)  loss_giou_1: 0.3799 (0.3799)  loss_ce_2: 0.1415 (0.1415)  loss_bbox_2: 0.0417 (0.0417)  loss_giou_2: 0.3825 (0.3825)  loss_ce_3: 0.1251 (0.1251)  loss_bbox_3: 0.0426 (0.0426)  loss_giou_3: 0.3849 (0.3849)  loss_ce_4: 0.1280 (0.1280)  loss_bbox_4: 0.0420 (0.0420)  loss_giou_4: 0.3815 (0.3815)  loss_ce_interm: 0.2216 (0.2216)  loss_bbox_interm: 0.0451 (0.0451)  loss_giou_interm: 0.3878 (0.3878)  loss_ce_unscaled: 0.0625 (0.0625)  loss_bbox_unscaled: 0.0084 (0.0084)  loss_giou_unscaled: 0.1919 (0.1919)  loss_xy_unscaled: 0.0023 (0.0023)  loss_hw_unscaled: 0.0062 (0.0062)  loss_ce_0_unscaled: 0.0828 (0.0828)  loss_bbox_0_unscaled: 0.0085 (0.0085)  loss_giou_0_unscaled: 0.1917 (0.1917)  loss_xy_0_unscaled: 0.0023 (0.0023)  loss_hw_0_unscaled: 0.0062 (0.0062)  loss_ce_1_unscaled: 0.0773 (0.0773)  loss_bbox_1_unscaled: 0.0085 (0.0085)  loss_giou_1_unscaled: 0.1899 (0.1899)  loss_xy_1_unscaled: 0.0022 (0.0022)  loss_hw_1_unscaled: 0.0062 (0.0062)  loss_ce_2_unscaled: 0.0708 (0.0708)  loss_bbox_2_unscaled: 0.0083 (0.0083)  loss_giou_2_unscaled: 0.1912 (0.1912)  loss_xy_2_unscaled: 0.0023 (0.0023)  loss_hw_2_unscaled: 0.0061 (0.0061)  loss_ce_3_unscaled: 0.0625 (0.0625)  loss_bbox_3_unscaled: 0.0085 (0.0085)  loss_giou_3_unscaled: 0.1924 (0.1924)  loss_xy_3_unscaled: 0.0023 (0.0023)  loss_hw_3_unscaled: 0.0062 (0.0062)  loss_ce_4_unscaled: 0.0640 (0.0640)  loss_bbox_4_unscaled: 0.0084 (0.0084)  loss_giou_4_unscaled: 0.1908 (0.1908)  loss_xy_4_unscaled: 0.0023 (0.0023)  loss_hw_4_unscaled: 0.0061 (0.0061)  loss_ce_interm_unscaled: 0.1108 (0.1108)  loss_bbox_interm_unscaled: 0.0090 (0.0090)  loss_giou_interm_unscaled: 0.1939 (0.1939)  loss_xy_interm_unscaled: 0.0023 (0.0023)  loss_hw_interm_unscaled: 0.0067 (0.0067)  time: 4.4178  data: 0.7231  max mem: 8948\n",
            "Epoch: [7]  [10/90]  eta: 0:03:56  lr: 0.000010  loss: 3.6979 (3.8712)  loss_ce: 0.0864 (0.0907)  loss_bbox: 0.0456 (0.0465)  loss_giou: 0.3838 (0.3881)  loss_ce_0: 0.1180 (0.1314)  loss_bbox_0: 0.0461 (0.0476)  loss_giou_0: 0.3835 (0.3905)  loss_ce_1: 0.0894 (0.1074)  loss_bbox_1: 0.0466 (0.0473)  loss_giou_1: 0.3799 (0.3920)  loss_ce_2: 0.0890 (0.1057)  loss_bbox_2: 0.0460 (0.0468)  loss_giou_2: 0.3825 (0.3892)  loss_ce_3: 0.0905 (0.0968)  loss_bbox_3: 0.0460 (0.0467)  loss_giou_3: 0.3849 (0.3895)  loss_ce_4: 0.0887 (0.0947)  loss_bbox_4: 0.0459 (0.0465)  loss_giou_4: 0.3815 (0.3887)  loss_ce_interm: 0.1639 (0.1792)  loss_bbox_interm: 0.0471 (0.0491)  loss_giou_interm: 0.3878 (0.3967)  loss_ce_unscaled: 0.0432 (0.0453)  loss_bbox_unscaled: 0.0091 (0.0093)  loss_giou_unscaled: 0.1919 (0.1941)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0064 (0.0065)  loss_ce_0_unscaled: 0.0590 (0.0657)  loss_bbox_0_unscaled: 0.0092 (0.0095)  loss_giou_0_unscaled: 0.1917 (0.1953)  loss_xy_0_unscaled: 0.0028 (0.0027)  loss_hw_0_unscaled: 0.0064 (0.0068)  loss_ce_1_unscaled: 0.0447 (0.0537)  loss_bbox_1_unscaled: 0.0093 (0.0095)  loss_giou_1_unscaled: 0.1899 (0.1960)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0064 (0.0067)  loss_ce_2_unscaled: 0.0445 (0.0529)  loss_bbox_2_unscaled: 0.0092 (0.0094)  loss_giou_2_unscaled: 0.1912 (0.1946)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0063 (0.0066)  loss_ce_3_unscaled: 0.0452 (0.0484)  loss_bbox_3_unscaled: 0.0092 (0.0093)  loss_giou_3_unscaled: 0.1924 (0.1947)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0063 (0.0066)  loss_ce_4_unscaled: 0.0444 (0.0473)  loss_bbox_4_unscaled: 0.0092 (0.0093)  loss_giou_4_unscaled: 0.1908 (0.1943)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0064 (0.0065)  loss_ce_interm_unscaled: 0.0819 (0.0896)  loss_bbox_interm_unscaled: 0.0094 (0.0098)  loss_giou_interm_unscaled: 0.1939 (0.1984)  loss_xy_interm_unscaled: 0.0029 (0.0029)  loss_hw_interm_unscaled: 0.0066 (0.0069)  time: 2.9516  data: 0.0819  max mem: 8948\n",
            "Epoch: [7]  [20/90]  eta: 0:03:24  lr: 0.000010  loss: 3.7555 (3.9133)  loss_ce: 0.0847 (0.0935)  loss_bbox: 0.0456 (0.0477)  loss_giou: 0.3829 (0.3926)  loss_ce_0: 0.1180 (0.1305)  loss_bbox_0: 0.0461 (0.0483)  loss_giou_0: 0.3915 (0.3960)  loss_ce_1: 0.0963 (0.1111)  loss_bbox_1: 0.0466 (0.0482)  loss_giou_1: 0.3893 (0.3962)  loss_ce_2: 0.0827 (0.1051)  loss_bbox_2: 0.0460 (0.0478)  loss_giou_2: 0.3827 (0.3948)  loss_ce_3: 0.0791 (0.0965)  loss_bbox_3: 0.0460 (0.0477)  loss_giou_3: 0.3832 (0.3951)  loss_ce_4: 0.0867 (0.0956)  loss_bbox_4: 0.0459 (0.0476)  loss_giou_4: 0.3812 (0.3927)  loss_ce_interm: 0.1574 (0.1730)  loss_bbox_interm: 0.0471 (0.0499)  loss_giou_interm: 0.3895 (0.4034)  loss_ce_unscaled: 0.0423 (0.0468)  loss_bbox_unscaled: 0.0091 (0.0095)  loss_giou_unscaled: 0.1914 (0.1963)  loss_xy_unscaled: 0.0028 (0.0029)  loss_hw_unscaled: 0.0064 (0.0067)  loss_ce_0_unscaled: 0.0590 (0.0652)  loss_bbox_0_unscaled: 0.0092 (0.0097)  loss_giou_0_unscaled: 0.1958 (0.1980)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0064 (0.0068)  loss_ce_1_unscaled: 0.0482 (0.0556)  loss_bbox_1_unscaled: 0.0093 (0.0096)  loss_giou_1_unscaled: 0.1946 (0.1981)  loss_xy_1_unscaled: 0.0029 (0.0029)  loss_hw_1_unscaled: 0.0064 (0.0067)  loss_ce_2_unscaled: 0.0414 (0.0526)  loss_bbox_2_unscaled: 0.0092 (0.0096)  loss_giou_2_unscaled: 0.1914 (0.1974)  loss_xy_2_unscaled: 0.0028 (0.0029)  loss_hw_2_unscaled: 0.0063 (0.0067)  loss_ce_3_unscaled: 0.0396 (0.0482)  loss_bbox_3_unscaled: 0.0092 (0.0095)  loss_giou_3_unscaled: 0.1916 (0.1975)  loss_xy_3_unscaled: 0.0029 (0.0029)  loss_hw_3_unscaled: 0.0063 (0.0067)  loss_ce_4_unscaled: 0.0433 (0.0478)  loss_bbox_4_unscaled: 0.0092 (0.0095)  loss_giou_4_unscaled: 0.1906 (0.1964)  loss_xy_4_unscaled: 0.0028 (0.0029)  loss_hw_4_unscaled: 0.0064 (0.0067)  loss_ce_interm_unscaled: 0.0787 (0.0865)  loss_bbox_interm_unscaled: 0.0094 (0.0100)  loss_giou_interm_unscaled: 0.1947 (0.2017)  loss_xy_interm_unscaled: 0.0029 (0.0030)  loss_hw_interm_unscaled: 0.0066 (0.0070)  time: 2.8460  data: 0.0154  max mem: 8948\n",
            "Epoch: [7]  [30/90]  eta: 0:02:52  lr: 0.000010  loss: 3.8690 (3.9814)  loss_ce: 0.0847 (0.0982)  loss_bbox: 0.0451 (0.0475)  loss_giou: 0.3892 (0.3955)  loss_ce_0: 0.1373 (0.1377)  loss_bbox_0: 0.0471 (0.0485)  loss_giou_0: 0.3945 (0.4014)  loss_ce_1: 0.1106 (0.1166)  loss_bbox_1: 0.0458 (0.0481)  loss_giou_1: 0.3950 (0.4004)  loss_ce_2: 0.1013 (0.1131)  loss_bbox_2: 0.0450 (0.0476)  loss_giou_2: 0.3839 (0.3974)  loss_ce_3: 0.0943 (0.1036)  loss_bbox_3: 0.0453 (0.0475)  loss_giou_3: 0.3860 (0.3965)  loss_ce_4: 0.0927 (0.1010)  loss_bbox_4: 0.0450 (0.0475)  loss_giou_4: 0.3828 (0.3954)  loss_ce_interm: 0.1692 (0.1804)  loss_bbox_interm: 0.0479 (0.0501)  loss_giou_interm: 0.4059 (0.4074)  loss_ce_unscaled: 0.0423 (0.0491)  loss_bbox_unscaled: 0.0090 (0.0095)  loss_giou_unscaled: 0.1946 (0.1978)  loss_xy_unscaled: 0.0029 (0.0029)  loss_hw_unscaled: 0.0061 (0.0066)  loss_ce_0_unscaled: 0.0686 (0.0689)  loss_bbox_0_unscaled: 0.0094 (0.0097)  loss_giou_0_unscaled: 0.1973 (0.2007)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0063 (0.0068)  loss_ce_1_unscaled: 0.0553 (0.0583)  loss_bbox_1_unscaled: 0.0092 (0.0096)  loss_giou_1_unscaled: 0.1975 (0.2002)  loss_xy_1_unscaled: 0.0029 (0.0029)  loss_hw_1_unscaled: 0.0063 (0.0067)  loss_ce_2_unscaled: 0.0507 (0.0566)  loss_bbox_2_unscaled: 0.0090 (0.0095)  loss_giou_2_unscaled: 0.1920 (0.1987)  loss_xy_2_unscaled: 0.0029 (0.0029)  loss_hw_2_unscaled: 0.0061 (0.0066)  loss_ce_3_unscaled: 0.0472 (0.0518)  loss_bbox_3_unscaled: 0.0091 (0.0095)  loss_giou_3_unscaled: 0.1930 (0.1982)  loss_xy_3_unscaled: 0.0029 (0.0029)  loss_hw_3_unscaled: 0.0062 (0.0066)  loss_ce_4_unscaled: 0.0463 (0.0505)  loss_bbox_4_unscaled: 0.0090 (0.0095)  loss_giou_4_unscaled: 0.1914 (0.1977)  loss_xy_4_unscaled: 0.0029 (0.0029)  loss_hw_4_unscaled: 0.0062 (0.0066)  loss_ce_interm_unscaled: 0.0846 (0.0902)  loss_bbox_interm_unscaled: 0.0096 (0.0100)  loss_giou_interm_unscaled: 0.2029 (0.2037)  loss_xy_interm_unscaled: 0.0030 (0.0030)  loss_hw_interm_unscaled: 0.0067 (0.0070)  time: 2.8217  data: 0.0149  max mem: 8948\n",
            "Epoch: [7]  [40/90]  eta: 0:02:20  lr: 0.000010  loss: 3.9363 (3.9989)  loss_ce: 0.0801 (0.1013)  loss_bbox: 0.0469 (0.0476)  loss_giou: 0.3944 (0.3958)  loss_ce_0: 0.1357 (0.1415)  loss_bbox_0: 0.0471 (0.0485)  loss_giou_0: 0.4049 (0.4006)  loss_ce_1: 0.1120 (0.1196)  loss_bbox_1: 0.0468 (0.0480)  loss_giou_1: 0.4041 (0.3990)  loss_ce_2: 0.1043 (0.1176)  loss_bbox_2: 0.0471 (0.0477)  loss_giou_2: 0.3895 (0.3965)  loss_ce_3: 0.0943 (0.1076)  loss_bbox_3: 0.0468 (0.0477)  loss_giou_3: 0.3867 (0.3966)  loss_ce_4: 0.0927 (0.1053)  loss_bbox_4: 0.0466 (0.0476)  loss_giou_4: 0.3941 (0.3956)  loss_ce_interm: 0.1692 (0.1779)  loss_bbox_interm: 0.0475 (0.0497)  loss_giou_interm: 0.4078 (0.4073)  loss_ce_unscaled: 0.0400 (0.0507)  loss_bbox_unscaled: 0.0094 (0.0095)  loss_giou_unscaled: 0.1972 (0.1979)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0065 (0.0067)  loss_ce_0_unscaled: 0.0678 (0.0707)  loss_bbox_0_unscaled: 0.0094 (0.0097)  loss_giou_0_unscaled: 0.2025 (0.2003)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0065 (0.0068)  loss_ce_1_unscaled: 0.0560 (0.0598)  loss_bbox_1_unscaled: 0.0094 (0.0096)  loss_giou_1_unscaled: 0.2020 (0.1995)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0064 (0.0068)  loss_ce_2_unscaled: 0.0521 (0.0588)  loss_bbox_2_unscaled: 0.0094 (0.0095)  loss_giou_2_unscaled: 0.1948 (0.1982)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0065 (0.0067)  loss_ce_3_unscaled: 0.0472 (0.0538)  loss_bbox_3_unscaled: 0.0094 (0.0095)  loss_giou_3_unscaled: 0.1934 (0.1983)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0066 (0.0067)  loss_ce_4_unscaled: 0.0463 (0.0527)  loss_bbox_4_unscaled: 0.0093 (0.0095)  loss_giou_4_unscaled: 0.1971 (0.1978)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0065 (0.0067)  loss_ce_interm_unscaled: 0.0846 (0.0890)  loss_bbox_interm_unscaled: 0.0095 (0.0099)  loss_giou_interm_unscaled: 0.2039 (0.2037)  loss_xy_interm_unscaled: 0.0029 (0.0030)  loss_hw_interm_unscaled: 0.0067 (0.0070)  time: 2.7102  data: 0.0158  max mem: 8948\n",
            "Epoch: [7]  [50/90]  eta: 0:01:54  lr: 0.000010  loss: 3.6010 (3.9183)  loss_ce: 0.0575 (0.0921)  loss_bbox: 0.0469 (0.0478)  loss_giou: 0.3847 (0.3934)  loss_ce_0: 0.1194 (0.1326)  loss_bbox_0: 0.0465 (0.0485)  loss_giou_0: 0.3928 (0.3972)  loss_ce_1: 0.0901 (0.1110)  loss_bbox_1: 0.0468 (0.0481)  loss_giou_1: 0.3831 (0.3959)  loss_ce_2: 0.0775 (0.1080)  loss_bbox_2: 0.0471 (0.0479)  loss_giou_2: 0.3813 (0.3938)  loss_ce_3: 0.0703 (0.0987)  loss_bbox_3: 0.0468 (0.0479)  loss_giou_3: 0.3807 (0.3940)  loss_ce_4: 0.0621 (0.0960)  loss_bbox_4: 0.0466 (0.0478)  loss_giou_4: 0.3837 (0.3932)  loss_ce_interm: 0.1445 (0.1721)  loss_bbox_interm: 0.0474 (0.0496)  loss_giou_interm: 0.3824 (0.4026)  loss_ce_unscaled: 0.0288 (0.0461)  loss_bbox_unscaled: 0.0094 (0.0096)  loss_giou_unscaled: 0.1923 (0.1967)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0065 (0.0067)  loss_ce_0_unscaled: 0.0597 (0.0663)  loss_bbox_0_unscaled: 0.0093 (0.0097)  loss_giou_0_unscaled: 0.1964 (0.1986)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0065 (0.0069)  loss_ce_1_unscaled: 0.0450 (0.0555)  loss_bbox_1_unscaled: 0.0094 (0.0096)  loss_giou_1_unscaled: 0.1915 (0.1980)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0064 (0.0068)  loss_ce_2_unscaled: 0.0388 (0.0540)  loss_bbox_2_unscaled: 0.0094 (0.0096)  loss_giou_2_unscaled: 0.1906 (0.1969)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0065 (0.0067)  loss_ce_3_unscaled: 0.0352 (0.0494)  loss_bbox_3_unscaled: 0.0094 (0.0096)  loss_giou_3_unscaled: 0.1903 (0.1970)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0066 (0.0067)  loss_ce_4_unscaled: 0.0311 (0.0480)  loss_bbox_4_unscaled: 0.0093 (0.0096)  loss_giou_4_unscaled: 0.1919 (0.1966)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0065 (0.0067)  loss_ce_interm_unscaled: 0.0723 (0.0861)  loss_bbox_interm_unscaled: 0.0095 (0.0099)  loss_giou_interm_unscaled: 0.1912 (0.2013)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0066 (0.0070)  time: 2.8513  data: 0.0156  max mem: 8948\n",
            "Epoch: [7]  [60/90]  eta: 0:01:26  lr: 0.000010  loss: 3.6317 (3.9173)  loss_ce: 0.0567 (0.0933)  loss_bbox: 0.0471 (0.0479)  loss_giou: 0.3674 (0.3926)  loss_ce_0: 0.1093 (0.1324)  loss_bbox_0: 0.0474 (0.0487)  loss_giou_0: 0.3705 (0.3963)  loss_ce_1: 0.0765 (0.1106)  loss_bbox_1: 0.0469 (0.0483)  loss_giou_1: 0.3726 (0.3954)  loss_ce_2: 0.0615 (0.1081)  loss_bbox_2: 0.0471 (0.0481)  loss_giou_2: 0.3680 (0.3928)  loss_ce_3: 0.0663 (0.0997)  loss_bbox_3: 0.0465 (0.0480)  loss_giou_3: 0.3707 (0.3930)  loss_ce_4: 0.0621 (0.0970)  loss_bbox_4: 0.0469 (0.0480)  loss_giou_4: 0.3673 (0.3923)  loss_ce_interm: 0.1520 (0.1735)  loss_bbox_interm: 0.0467 (0.0497)  loss_giou_interm: 0.3824 (0.4017)  loss_ce_unscaled: 0.0284 (0.0467)  loss_bbox_unscaled: 0.0094 (0.0096)  loss_giou_unscaled: 0.1837 (0.1963)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0065 (0.0068)  loss_ce_0_unscaled: 0.0546 (0.0662)  loss_bbox_0_unscaled: 0.0095 (0.0097)  loss_giou_0_unscaled: 0.1853 (0.1982)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0066 (0.0069)  loss_ce_1_unscaled: 0.0382 (0.0553)  loss_bbox_1_unscaled: 0.0094 (0.0097)  loss_giou_1_unscaled: 0.1863 (0.1977)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0065 (0.0068)  loss_ce_2_unscaled: 0.0307 (0.0541)  loss_bbox_2_unscaled: 0.0094 (0.0096)  loss_giou_2_unscaled: 0.1840 (0.1964)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0065 (0.0068)  loss_ce_3_unscaled: 0.0331 (0.0499)  loss_bbox_3_unscaled: 0.0093 (0.0096)  loss_giou_3_unscaled: 0.1854 (0.1965)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0065 (0.0068)  loss_ce_4_unscaled: 0.0310 (0.0485)  loss_bbox_4_unscaled: 0.0094 (0.0096)  loss_giou_4_unscaled: 0.1836 (0.1961)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0065 (0.0068)  loss_ce_interm_unscaled: 0.0760 (0.0868)  loss_bbox_interm_unscaled: 0.0093 (0.0099)  loss_giou_interm_unscaled: 0.1912 (0.2008)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0065 (0.0070)  time: 3.0168  data: 0.0163  max mem: 8948\n",
            "Epoch: [7]  [70/90]  eta: 0:00:57  lr: 0.000010  loss: 3.7404 (3.8957)  loss_ce: 0.0775 (0.0933)  loss_bbox: 0.0446 (0.0474)  loss_giou: 0.3816 (0.3915)  loss_ce_0: 0.1093 (0.1289)  loss_bbox_0: 0.0445 (0.0481)  loss_giou_0: 0.3788 (0.3953)  loss_ce_1: 0.0841 (0.1093)  loss_bbox_1: 0.0448 (0.0477)  loss_giou_1: 0.3877 (0.3945)  loss_ce_2: 0.0921 (0.1060)  loss_bbox_2: 0.0452 (0.0475)  loss_giou_2: 0.3857 (0.3922)  loss_ce_3: 0.0808 (0.0985)  loss_bbox_3: 0.0454 (0.0475)  loss_giou_3: 0.3848 (0.3923)  loss_ce_4: 0.0867 (0.0964)  loss_bbox_4: 0.0448 (0.0474)  loss_giou_4: 0.3816 (0.3914)  loss_ce_interm: 0.1540 (0.1714)  loss_bbox_interm: 0.0462 (0.0491)  loss_giou_interm: 0.3871 (0.4000)  loss_ce_unscaled: 0.0388 (0.0466)  loss_bbox_unscaled: 0.0089 (0.0095)  loss_giou_unscaled: 0.1908 (0.1957)  loss_xy_unscaled: 0.0026 (0.0028)  loss_hw_unscaled: 0.0063 (0.0067)  loss_ce_0_unscaled: 0.0546 (0.0644)  loss_bbox_0_unscaled: 0.0089 (0.0096)  loss_giou_0_unscaled: 0.1894 (0.1977)  loss_xy_0_unscaled: 0.0026 (0.0028)  loss_hw_0_unscaled: 0.0064 (0.0068)  loss_ce_1_unscaled: 0.0420 (0.0546)  loss_bbox_1_unscaled: 0.0090 (0.0095)  loss_giou_1_unscaled: 0.1939 (0.1973)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0064 (0.0067)  loss_ce_2_unscaled: 0.0460 (0.0530)  loss_bbox_2_unscaled: 0.0090 (0.0095)  loss_giou_2_unscaled: 0.1928 (0.1961)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0065 (0.0067)  loss_ce_3_unscaled: 0.0404 (0.0493)  loss_bbox_3_unscaled: 0.0091 (0.0095)  loss_giou_3_unscaled: 0.1924 (0.1962)  loss_xy_3_unscaled: 0.0026 (0.0028)  loss_hw_3_unscaled: 0.0064 (0.0067)  loss_ce_4_unscaled: 0.0434 (0.0482)  loss_bbox_4_unscaled: 0.0090 (0.0095)  loss_giou_4_unscaled: 0.1908 (0.1957)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0063 (0.0067)  loss_ce_interm_unscaled: 0.0770 (0.0857)  loss_bbox_interm_unscaled: 0.0092 (0.0098)  loss_giou_interm_unscaled: 0.1936 (0.2000)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0064 (0.0069)  time: 2.9349  data: 0.0168  max mem: 8948\n",
            "Epoch: [7]  [80/90]  eta: 0:00:29  lr: 0.000010  loss: 3.6868 (3.9056)  loss_ce: 0.0775 (0.0947)  loss_bbox: 0.0446 (0.0477)  loss_giou: 0.3930 (0.3919)  loss_ce_0: 0.1077 (0.1280)  loss_bbox_0: 0.0452 (0.0485)  loss_giou_0: 0.3972 (0.3961)  loss_ce_1: 0.0947 (0.1110)  loss_bbox_1: 0.0448 (0.0480)  loss_giou_1: 0.3916 (0.3943)  loss_ce_2: 0.0950 (0.1077)  loss_bbox_2: 0.0449 (0.0478)  loss_giou_2: 0.3921 (0.3920)  loss_ce_3: 0.0912 (0.1003)  loss_bbox_3: 0.0454 (0.0478)  loss_giou_3: 0.3923 (0.3925)  loss_ce_4: 0.0816 (0.0982)  loss_bbox_4: 0.0448 (0.0477)  loss_giou_4: 0.3901 (0.3917)  loss_ce_interm: 0.1430 (0.1699)  loss_bbox_interm: 0.0486 (0.0495)  loss_giou_interm: 0.3989 (0.4004)  loss_ce_unscaled: 0.0388 (0.0473)  loss_bbox_unscaled: 0.0089 (0.0095)  loss_giou_unscaled: 0.1965 (0.1960)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0063 (0.0067)  loss_ce_0_unscaled: 0.0539 (0.0640)  loss_bbox_0_unscaled: 0.0090 (0.0097)  loss_giou_0_unscaled: 0.1986 (0.1980)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0063 (0.0068)  loss_ce_1_unscaled: 0.0473 (0.0555)  loss_bbox_1_unscaled: 0.0090 (0.0096)  loss_giou_1_unscaled: 0.1958 (0.1971)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0064 (0.0068)  loss_ce_2_unscaled: 0.0475 (0.0539)  loss_bbox_2_unscaled: 0.0090 (0.0096)  loss_giou_2_unscaled: 0.1960 (0.1960)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0063 (0.0067)  loss_ce_3_unscaled: 0.0456 (0.0502)  loss_bbox_3_unscaled: 0.0091 (0.0096)  loss_giou_3_unscaled: 0.1961 (0.1962)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0064 (0.0067)  loss_ce_4_unscaled: 0.0408 (0.0491)  loss_bbox_4_unscaled: 0.0090 (0.0095)  loss_giou_4_unscaled: 0.1951 (0.1959)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0063 (0.0067)  loss_ce_interm_unscaled: 0.0715 (0.0849)  loss_bbox_interm_unscaled: 0.0097 (0.0099)  loss_giou_interm_unscaled: 0.1994 (0.2002)  loss_xy_interm_unscaled: 0.0030 (0.0030)  loss_hw_interm_unscaled: 0.0065 (0.0069)  time: 2.9692  data: 0.0163  max mem: 8948\n",
            "Epoch: [7]  [89/90]  eta: 0:00:02  lr: 0.000010  loss: 3.6868 (3.8919)  loss_ce: 0.0883 (0.0943)  loss_bbox: 0.0469 (0.0477)  loss_giou: 0.3901 (0.3906)  loss_ce_0: 0.1088 (0.1270)  loss_bbox_0: 0.0475 (0.0485)  loss_giou_0: 0.3920 (0.3947)  loss_ce_1: 0.0976 (0.1103)  loss_bbox_1: 0.0475 (0.0479)  loss_giou_1: 0.3863 (0.3934)  loss_ce_2: 0.0969 (0.1065)  loss_bbox_2: 0.0449 (0.0477)  loss_giou_2: 0.3862 (0.3910)  loss_ce_3: 0.1016 (0.1000)  loss_bbox_3: 0.0454 (0.0477)  loss_giou_3: 0.3887 (0.3912)  loss_ce_4: 0.0920 (0.0977)  loss_bbox_4: 0.0469 (0.0476)  loss_giou_4: 0.3868 (0.3906)  loss_ce_interm: 0.1437 (0.1693)  loss_bbox_interm: 0.0490 (0.0494)  loss_giou_interm: 0.3852 (0.3987)  loss_ce_unscaled: 0.0442 (0.0471)  loss_bbox_unscaled: 0.0094 (0.0095)  loss_giou_unscaled: 0.1951 (0.1953)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0063 (0.0067)  loss_ce_0_unscaled: 0.0544 (0.0635)  loss_bbox_0_unscaled: 0.0095 (0.0097)  loss_giou_0_unscaled: 0.1960 (0.1973)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0066 (0.0068)  loss_ce_1_unscaled: 0.0488 (0.0552)  loss_bbox_1_unscaled: 0.0095 (0.0096)  loss_giou_1_unscaled: 0.1932 (0.1967)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0064 (0.0067)  loss_ce_2_unscaled: 0.0484 (0.0532)  loss_bbox_2_unscaled: 0.0090 (0.0095)  loss_giou_2_unscaled: 0.1931 (0.1955)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0063 (0.0067)  loss_ce_3_unscaled: 0.0508 (0.0500)  loss_bbox_3_unscaled: 0.0091 (0.0095)  loss_giou_3_unscaled: 0.1944 (0.1956)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0064 (0.0067)  loss_ce_4_unscaled: 0.0460 (0.0489)  loss_bbox_4_unscaled: 0.0094 (0.0095)  loss_giou_4_unscaled: 0.1934 (0.1953)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0064 (0.0067)  loss_ce_interm_unscaled: 0.0718 (0.0846)  loss_bbox_interm_unscaled: 0.0098 (0.0099)  loss_giou_interm_unscaled: 0.1926 (0.1994)  loss_xy_interm_unscaled: 0.0029 (0.0029)  loss_hw_interm_unscaled: 0.0066 (0.0069)  time: 2.8520  data: 0.0151  max mem: 8948\n",
            "Epoch: [7] Total time: 0:04:19 (2.8825 s / it)\n",
            "Averaged stats: lr: 0.000010  loss: 3.6868 (3.8919)  loss_ce: 0.0883 (0.0943)  loss_bbox: 0.0469 (0.0477)  loss_giou: 0.3901 (0.3906)  loss_ce_0: 0.1088 (0.1270)  loss_bbox_0: 0.0475 (0.0485)  loss_giou_0: 0.3920 (0.3947)  loss_ce_1: 0.0976 (0.1103)  loss_bbox_1: 0.0475 (0.0479)  loss_giou_1: 0.3863 (0.3934)  loss_ce_2: 0.0969 (0.1065)  loss_bbox_2: 0.0449 (0.0477)  loss_giou_2: 0.3862 (0.3910)  loss_ce_3: 0.1016 (0.1000)  loss_bbox_3: 0.0454 (0.0477)  loss_giou_3: 0.3887 (0.3912)  loss_ce_4: 0.0920 (0.0977)  loss_bbox_4: 0.0469 (0.0476)  loss_giou_4: 0.3868 (0.3906)  loss_ce_interm: 0.1437 (0.1693)  loss_bbox_interm: 0.0490 (0.0494)  loss_giou_interm: 0.3852 (0.3987)  loss_ce_unscaled: 0.0442 (0.0471)  loss_bbox_unscaled: 0.0094 (0.0095)  loss_giou_unscaled: 0.1951 (0.1953)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0063 (0.0067)  loss_ce_0_unscaled: 0.0544 (0.0635)  loss_bbox_0_unscaled: 0.0095 (0.0097)  loss_giou_0_unscaled: 0.1960 (0.1973)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0066 (0.0068)  loss_ce_1_unscaled: 0.0488 (0.0552)  loss_bbox_1_unscaled: 0.0095 (0.0096)  loss_giou_1_unscaled: 0.1932 (0.1967)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0064 (0.0067)  loss_ce_2_unscaled: 0.0484 (0.0532)  loss_bbox_2_unscaled: 0.0090 (0.0095)  loss_giou_2_unscaled: 0.1931 (0.1955)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0063 (0.0067)  loss_ce_3_unscaled: 0.0508 (0.0500)  loss_bbox_3_unscaled: 0.0091 (0.0095)  loss_giou_3_unscaled: 0.1944 (0.1956)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0064 (0.0067)  loss_ce_4_unscaled: 0.0460 (0.0489)  loss_bbox_4_unscaled: 0.0094 (0.0095)  loss_giou_4_unscaled: 0.1934 (0.1953)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0064 (0.0067)  loss_ce_interm_unscaled: 0.0718 (0.0846)  loss_bbox_interm_unscaled: 0.0098 (0.0099)  loss_giou_interm_unscaled: 0.1926 (0.1994)  loss_xy_interm_unscaled: 0.0029 (0.0029)  loss_hw_interm_unscaled: 0.0066 (0.0069)\n",
            "Input text prompt: bolt . wrong direction . 1 . 2 . 3 . 4 . 5 .\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Test:  [ 0/10]  eta: 0:00:18    time: 1.8382  data: 0.9145  max mem: 8948\n",
            "Test:  [ 9/10]  eta: 0:00:01    time: 1.0328  data: 0.1049  max mem: 8948\n",
            "Test: Total time: 0:00:10 (1.0420 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.057\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.121\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.042\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.057\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.281\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.191\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.010\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.251\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.481\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.571\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.407\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch: [8]  [ 0/90]  eta: 0:06:16  lr: 0.000001  loss: 3.5091 (3.5091)  loss_ce: 0.0529 (0.0529)  loss_bbox: 0.0463 (0.0463)  loss_giou: 0.3743 (0.3743)  loss_ce_0: 0.1036 (0.1036)  loss_bbox_0: 0.0465 (0.0465)  loss_giou_0: 0.3650 (0.3650)  loss_ce_1: 0.0891 (0.0891)  loss_bbox_1: 0.0462 (0.0462)  loss_giou_1: 0.3718 (0.3718)  loss_ce_2: 0.0777 (0.0777)  loss_bbox_2: 0.0464 (0.0464)  loss_giou_2: 0.3729 (0.3729)  loss_ce_3: 0.0612 (0.0612)  loss_bbox_3: 0.0466 (0.0466)  loss_giou_3: 0.3734 (0.3734)  loss_ce_4: 0.0577 (0.0577)  loss_bbox_4: 0.0464 (0.0464)  loss_giou_4: 0.3738 (0.3738)  loss_ce_interm: 0.1251 (0.1251)  loss_bbox_interm: 0.0490 (0.0490)  loss_giou_interm: 0.3833 (0.3833)  loss_ce_unscaled: 0.0265 (0.0265)  loss_bbox_unscaled: 0.0093 (0.0093)  loss_giou_unscaled: 0.1871 (0.1871)  loss_xy_unscaled: 0.0027 (0.0027)  loss_hw_unscaled: 0.0065 (0.0065)  loss_ce_0_unscaled: 0.0518 (0.0518)  loss_bbox_0_unscaled: 0.0093 (0.0093)  loss_giou_0_unscaled: 0.1825 (0.1825)  loss_xy_0_unscaled: 0.0027 (0.0027)  loss_hw_0_unscaled: 0.0066 (0.0066)  loss_ce_1_unscaled: 0.0446 (0.0446)  loss_bbox_1_unscaled: 0.0092 (0.0092)  loss_giou_1_unscaled: 0.1859 (0.1859)  loss_xy_1_unscaled: 0.0027 (0.0027)  loss_hw_1_unscaled: 0.0065 (0.0065)  loss_ce_2_unscaled: 0.0389 (0.0389)  loss_bbox_2_unscaled: 0.0093 (0.0093)  loss_giou_2_unscaled: 0.1865 (0.1865)  loss_xy_2_unscaled: 0.0027 (0.0027)  loss_hw_2_unscaled: 0.0066 (0.0066)  loss_ce_3_unscaled: 0.0306 (0.0306)  loss_bbox_3_unscaled: 0.0093 (0.0093)  loss_giou_3_unscaled: 0.1867 (0.1867)  loss_xy_3_unscaled: 0.0027 (0.0027)  loss_hw_3_unscaled: 0.0066 (0.0066)  loss_ce_4_unscaled: 0.0289 (0.0289)  loss_bbox_4_unscaled: 0.0093 (0.0093)  loss_giou_4_unscaled: 0.1869 (0.1869)  loss_xy_4_unscaled: 0.0027 (0.0027)  loss_hw_4_unscaled: 0.0066 (0.0066)  loss_ce_interm_unscaled: 0.0626 (0.0626)  loss_bbox_interm_unscaled: 0.0098 (0.0098)  loss_giou_interm_unscaled: 0.1916 (0.1916)  loss_xy_interm_unscaled: 0.0028 (0.0028)  loss_hw_interm_unscaled: 0.0070 (0.0070)  time: 4.1809  data: 0.8939  max mem: 8948\n",
            "Epoch: [8]  [10/90]  eta: 0:03:44  lr: 0.000001  loss: 4.0684 (3.9903)  loss_ce: 0.0676 (0.1007)  loss_bbox: 0.0461 (0.0483)  loss_giou: 0.3836 (0.3936)  loss_ce_0: 0.1250 (0.1433)  loss_bbox_0: 0.0467 (0.0488)  loss_giou_0: 0.4060 (0.3998)  loss_ce_1: 0.1063 (0.1218)  loss_bbox_1: 0.0462 (0.0488)  loss_giou_1: 0.3947 (0.3965)  loss_ce_2: 0.0939 (0.1107)  loss_bbox_2: 0.0468 (0.0482)  loss_giou_2: 0.3828 (0.3948)  loss_ce_3: 0.0775 (0.1045)  loss_bbox_3: 0.0465 (0.0487)  loss_giou_3: 0.3823 (0.3935)  loss_ce_4: 0.0781 (0.1024)  loss_bbox_4: 0.0463 (0.0483)  loss_giou_4: 0.3819 (0.3932)  loss_ce_interm: 0.1686 (0.1941)  loss_bbox_interm: 0.0489 (0.0505)  loss_giou_interm: 0.4088 (0.3995)  loss_ce_unscaled: 0.0338 (0.0504)  loss_bbox_unscaled: 0.0092 (0.0097)  loss_giou_unscaled: 0.1918 (0.1968)  loss_xy_unscaled: 0.0027 (0.0029)  loss_hw_unscaled: 0.0065 (0.0068)  loss_ce_0_unscaled: 0.0625 (0.0717)  loss_bbox_0_unscaled: 0.0093 (0.0098)  loss_giou_0_unscaled: 0.2030 (0.1999)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0067 (0.0068)  loss_ce_1_unscaled: 0.0532 (0.0609)  loss_bbox_1_unscaled: 0.0092 (0.0098)  loss_giou_1_unscaled: 0.1974 (0.1983)  loss_xy_1_unscaled: 0.0029 (0.0029)  loss_hw_1_unscaled: 0.0066 (0.0068)  loss_ce_2_unscaled: 0.0469 (0.0553)  loss_bbox_2_unscaled: 0.0094 (0.0096)  loss_giou_2_unscaled: 0.1914 (0.1974)  loss_xy_2_unscaled: 0.0028 (0.0029)  loss_hw_2_unscaled: 0.0066 (0.0067)  loss_ce_3_unscaled: 0.0388 (0.0523)  loss_bbox_3_unscaled: 0.0093 (0.0097)  loss_giou_3_unscaled: 0.1911 (0.1967)  loss_xy_3_unscaled: 0.0027 (0.0030)  loss_hw_3_unscaled: 0.0066 (0.0068)  loss_ce_4_unscaled: 0.0391 (0.0512)  loss_bbox_4_unscaled: 0.0093 (0.0097)  loss_giou_4_unscaled: 0.1910 (0.1966)  loss_xy_4_unscaled: 0.0027 (0.0029)  loss_hw_4_unscaled: 0.0066 (0.0068)  loss_ce_interm_unscaled: 0.0843 (0.0971)  loss_bbox_interm_unscaled: 0.0098 (0.0101)  loss_giou_interm_unscaled: 0.2044 (0.1997)  loss_xy_interm_unscaled: 0.0029 (0.0031)  loss_hw_interm_unscaled: 0.0069 (0.0070)  time: 2.8028  data: 0.0939  max mem: 8948\n",
            "Epoch: [8]  [20/90]  eta: 0:03:18  lr: 0.000001  loss: 3.9006 (3.9160)  loss_ce: 0.0676 (0.0898)  loss_bbox: 0.0465 (0.0498)  loss_giou: 0.3898 (0.3955)  loss_ce_0: 0.1145 (0.1296)  loss_bbox_0: 0.0478 (0.0499)  loss_giou_0: 0.4022 (0.3972)  loss_ce_1: 0.0845 (0.1086)  loss_bbox_1: 0.0468 (0.0500)  loss_giou_1: 0.3947 (0.3972)  loss_ce_2: 0.0869 (0.1006)  loss_bbox_2: 0.0470 (0.0496)  loss_giou_2: 0.3859 (0.3955)  loss_ce_3: 0.0775 (0.0931)  loss_bbox_3: 0.0468 (0.0499)  loss_giou_3: 0.3893 (0.3961)  loss_ce_4: 0.0781 (0.0908)  loss_bbox_4: 0.0465 (0.0498)  loss_giou_4: 0.3998 (0.3960)  loss_ce_interm: 0.1655 (0.1749)  loss_bbox_interm: 0.0504 (0.0514)  loss_giou_interm: 0.4088 (0.4008)  loss_ce_unscaled: 0.0338 (0.0449)  loss_bbox_unscaled: 0.0093 (0.0100)  loss_giou_unscaled: 0.1949 (0.1977)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0068 (0.0071)  loss_ce_0_unscaled: 0.0573 (0.0648)  loss_bbox_0_unscaled: 0.0096 (0.0100)  loss_giou_0_unscaled: 0.2011 (0.1986)  loss_xy_0_unscaled: 0.0026 (0.0028)  loss_hw_0_unscaled: 0.0071 (0.0072)  loss_ce_1_unscaled: 0.0423 (0.0543)  loss_bbox_1_unscaled: 0.0094 (0.0100)  loss_giou_1_unscaled: 0.1974 (0.1986)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0071 (0.0072)  loss_ce_2_unscaled: 0.0435 (0.0503)  loss_bbox_2_unscaled: 0.0094 (0.0099)  loss_giou_2_unscaled: 0.1929 (0.1978)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0070 (0.0071)  loss_ce_3_unscaled: 0.0388 (0.0465)  loss_bbox_3_unscaled: 0.0094 (0.0100)  loss_giou_3_unscaled: 0.1947 (0.1981)  loss_xy_3_unscaled: 0.0026 (0.0028)  loss_hw_3_unscaled: 0.0069 (0.0071)  loss_ce_4_unscaled: 0.0391 (0.0454)  loss_bbox_4_unscaled: 0.0093 (0.0100)  loss_giou_4_unscaled: 0.1999 (0.1980)  loss_xy_4_unscaled: 0.0026 (0.0028)  loss_hw_4_unscaled: 0.0068 (0.0071)  loss_ce_interm_unscaled: 0.0827 (0.0875)  loss_bbox_interm_unscaled: 0.0101 (0.0103)  loss_giou_interm_unscaled: 0.2044 (0.2004)  loss_xy_interm_unscaled: 0.0027 (0.0029)  loss_hw_interm_unscaled: 0.0075 (0.0074)  time: 2.7634  data: 0.0133  max mem: 8948\n",
            "Epoch: [8]  [30/90]  eta: 0:02:53  lr: 0.000001  loss: 3.7691 (3.8824)  loss_ce: 0.0674 (0.0900)  loss_bbox: 0.0496 (0.0492)  loss_giou: 0.3968 (0.3940)  loss_ce_0: 0.1117 (0.1252)  loss_bbox_0: 0.0508 (0.0494)  loss_giou_0: 0.3966 (0.3960)  loss_ce_1: 0.0810 (0.1052)  loss_bbox_1: 0.0504 (0.0493)  loss_giou_1: 0.3947 (0.3960)  loss_ce_2: 0.0677 (0.0965)  loss_bbox_2: 0.0503 (0.0491)  loss_giou_2: 0.3878 (0.3948)  loss_ce_3: 0.0719 (0.0918)  loss_bbox_3: 0.0499 (0.0493)  loss_giou_3: 0.3962 (0.3953)  loss_ce_4: 0.0729 (0.0910)  loss_bbox_4: 0.0497 (0.0492)  loss_giou_4: 0.3998 (0.3945)  loss_ce_interm: 0.1501 (0.1657)  loss_bbox_interm: 0.0504 (0.0509)  loss_giou_interm: 0.4042 (0.4000)  loss_ce_unscaled: 0.0337 (0.0450)  loss_bbox_unscaled: 0.0099 (0.0098)  loss_giou_unscaled: 0.1984 (0.1970)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0072 (0.0071)  loss_ce_0_unscaled: 0.0558 (0.0626)  loss_bbox_0_unscaled: 0.0102 (0.0099)  loss_giou_0_unscaled: 0.1983 (0.1980)  loss_xy_0_unscaled: 0.0026 (0.0028)  loss_hw_0_unscaled: 0.0072 (0.0071)  loss_ce_1_unscaled: 0.0405 (0.0526)  loss_bbox_1_unscaled: 0.0101 (0.0099)  loss_giou_1_unscaled: 0.1974 (0.1980)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0072 (0.0071)  loss_ce_2_unscaled: 0.0338 (0.0483)  loss_bbox_2_unscaled: 0.0101 (0.0098)  loss_giou_2_unscaled: 0.1939 (0.1974)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0072 (0.0070)  loss_ce_3_unscaled: 0.0360 (0.0459)  loss_bbox_3_unscaled: 0.0100 (0.0099)  loss_giou_3_unscaled: 0.1981 (0.1977)  loss_xy_3_unscaled: 0.0026 (0.0028)  loss_hw_3_unscaled: 0.0072 (0.0071)  loss_ce_4_unscaled: 0.0365 (0.0455)  loss_bbox_4_unscaled: 0.0099 (0.0098)  loss_giou_4_unscaled: 0.1999 (0.1973)  loss_xy_4_unscaled: 0.0026 (0.0028)  loss_hw_4_unscaled: 0.0072 (0.0071)  loss_ce_interm_unscaled: 0.0751 (0.0829)  loss_bbox_interm_unscaled: 0.0101 (0.0102)  loss_giou_interm_unscaled: 0.2021 (0.2000)  loss_xy_interm_unscaled: 0.0026 (0.0029)  loss_hw_interm_unscaled: 0.0075 (0.0073)  time: 2.9387  data: 0.0150  max mem: 8948\n",
            "Epoch: [8]  [40/90]  eta: 0:02:23  lr: 0.000001  loss: 3.7691 (3.9072)  loss_ce: 0.0732 (0.0908)  loss_bbox: 0.0452 (0.0492)  loss_giou: 0.3968 (0.3964)  loss_ce_0: 0.1094 (0.1249)  loss_bbox_0: 0.0455 (0.0494)  loss_giou_0: 0.3948 (0.3987)  loss_ce_1: 0.0873 (0.1071)  loss_bbox_1: 0.0475 (0.0493)  loss_giou_1: 0.3944 (0.3984)  loss_ce_2: 0.0937 (0.0997)  loss_bbox_2: 0.0461 (0.0491)  loss_giou_2: 0.3894 (0.3959)  loss_ce_3: 0.0760 (0.0937)  loss_bbox_3: 0.0454 (0.0493)  loss_giou_3: 0.3959 (0.3968)  loss_ce_4: 0.0778 (0.0919)  loss_bbox_4: 0.0457 (0.0492)  loss_giou_4: 0.3960 (0.3966)  loss_ce_interm: 0.1559 (0.1678)  loss_bbox_interm: 0.0486 (0.0506)  loss_giou_interm: 0.4037 (0.4026)  loss_ce_unscaled: 0.0366 (0.0454)  loss_bbox_unscaled: 0.0090 (0.0098)  loss_giou_unscaled: 0.1984 (0.1982)  loss_xy_unscaled: 0.0029 (0.0028)  loss_hw_unscaled: 0.0067 (0.0070)  loss_ce_0_unscaled: 0.0547 (0.0624)  loss_bbox_0_unscaled: 0.0091 (0.0099)  loss_giou_0_unscaled: 0.1974 (0.1993)  loss_xy_0_unscaled: 0.0029 (0.0028)  loss_hw_0_unscaled: 0.0068 (0.0071)  loss_ce_1_unscaled: 0.0436 (0.0536)  loss_bbox_1_unscaled: 0.0095 (0.0099)  loss_giou_1_unscaled: 0.1972 (0.1992)  loss_xy_1_unscaled: 0.0029 (0.0028)  loss_hw_1_unscaled: 0.0069 (0.0070)  loss_ce_2_unscaled: 0.0469 (0.0499)  loss_bbox_2_unscaled: 0.0092 (0.0098)  loss_giou_2_unscaled: 0.1947 (0.1980)  loss_xy_2_unscaled: 0.0029 (0.0028)  loss_hw_2_unscaled: 0.0067 (0.0070)  loss_ce_3_unscaled: 0.0380 (0.0468)  loss_bbox_3_unscaled: 0.0091 (0.0099)  loss_giou_3_unscaled: 0.1980 (0.1984)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0068 (0.0070)  loss_ce_4_unscaled: 0.0389 (0.0460)  loss_bbox_4_unscaled: 0.0091 (0.0098)  loss_giou_4_unscaled: 0.1980 (0.1983)  loss_xy_4_unscaled: 0.0029 (0.0028)  loss_hw_4_unscaled: 0.0067 (0.0070)  loss_ce_interm_unscaled: 0.0780 (0.0839)  loss_bbox_interm_unscaled: 0.0097 (0.0101)  loss_giou_interm_unscaled: 0.2018 (0.2013)  loss_xy_interm_unscaled: 0.0029 (0.0029)  loss_hw_interm_unscaled: 0.0070 (0.0072)  time: 2.9154  data: 0.0174  max mem: 8948\n",
            "Epoch: [8]  [50/90]  eta: 0:01:53  lr: 0.000001  loss: 3.8185 (3.9243)  loss_ce: 0.0809 (0.0944)  loss_bbox: 0.0455 (0.0489)  loss_giou: 0.3782 (0.3964)  loss_ce_0: 0.1059 (0.1257)  loss_bbox_0: 0.0467 (0.0494)  loss_giou_0: 0.3801 (0.3998)  loss_ce_1: 0.0939 (0.1090)  loss_bbox_1: 0.0467 (0.0492)  loss_giou_1: 0.3818 (0.3984)  loss_ce_2: 0.0998 (0.1028)  loss_bbox_2: 0.0461 (0.0489)  loss_giou_2: 0.3769 (0.3960)  loss_ce_3: 0.0841 (0.0967)  loss_bbox_3: 0.0465 (0.0491)  loss_giou_3: 0.3765 (0.3969)  loss_ce_4: 0.0807 (0.0954)  loss_bbox_4: 0.0457 (0.0489)  loss_giou_4: 0.3769 (0.3965)  loss_ce_interm: 0.1732 (0.1671)  loss_bbox_interm: 0.0493 (0.0508)  loss_giou_interm: 0.3834 (0.4039)  loss_ce_unscaled: 0.0404 (0.0472)  loss_bbox_unscaled: 0.0091 (0.0098)  loss_giou_unscaled: 0.1891 (0.1982)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0066 (0.0070)  loss_ce_0_unscaled: 0.0530 (0.0628)  loss_bbox_0_unscaled: 0.0093 (0.0099)  loss_giou_0_unscaled: 0.1900 (0.1999)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0066 (0.0070)  loss_ce_1_unscaled: 0.0470 (0.0545)  loss_bbox_1_unscaled: 0.0093 (0.0098)  loss_giou_1_unscaled: 0.1909 (0.1992)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0066 (0.0070)  loss_ce_2_unscaled: 0.0499 (0.0514)  loss_bbox_2_unscaled: 0.0092 (0.0098)  loss_giou_2_unscaled: 0.1884 (0.1980)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0066 (0.0070)  loss_ce_3_unscaled: 0.0420 (0.0483)  loss_bbox_3_unscaled: 0.0093 (0.0098)  loss_giou_3_unscaled: 0.1882 (0.1985)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0066 (0.0070)  loss_ce_4_unscaled: 0.0403 (0.0477)  loss_bbox_4_unscaled: 0.0091 (0.0098)  loss_giou_4_unscaled: 0.1884 (0.1983)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0066 (0.0070)  loss_ce_interm_unscaled: 0.0866 (0.0835)  loss_bbox_interm_unscaled: 0.0099 (0.0102)  loss_giou_interm_unscaled: 0.1917 (0.2019)  loss_xy_interm_unscaled: 0.0029 (0.0029)  loss_hw_interm_unscaled: 0.0070 (0.0073)  time: 2.7644  data: 0.0163  max mem: 8948\n",
            "Epoch: [8]  [60/90]  eta: 0:01:25  lr: 0.000001  loss: 3.5414 (3.8688)  loss_ce: 0.0809 (0.0933)  loss_bbox: 0.0455 (0.0483)  loss_giou: 0.3662 (0.3904)  loss_ce_0: 0.1064 (0.1242)  loss_bbox_0: 0.0454 (0.0487)  loss_giou_0: 0.3714 (0.3945)  loss_ce_1: 0.0916 (0.1070)  loss_bbox_1: 0.0455 (0.0485)  loss_giou_1: 0.3670 (0.3934)  loss_ce_2: 0.0824 (0.1006)  loss_bbox_2: 0.0455 (0.0483)  loss_giou_2: 0.3711 (0.3909)  loss_ce_3: 0.0697 (0.0952)  loss_bbox_3: 0.0456 (0.0484)  loss_giou_3: 0.3648 (0.3913)  loss_ce_4: 0.0698 (0.0935)  loss_bbox_4: 0.0455 (0.0483)  loss_giou_4: 0.3666 (0.3910)  loss_ce_interm: 0.1423 (0.1651)  loss_bbox_interm: 0.0477 (0.0500)  loss_giou_interm: 0.3736 (0.3980)  loss_ce_unscaled: 0.0404 (0.0466)  loss_bbox_unscaled: 0.0091 (0.0097)  loss_giou_unscaled: 0.1831 (0.1952)  loss_xy_unscaled: 0.0025 (0.0028)  loss_hw_unscaled: 0.0063 (0.0069)  loss_ce_0_unscaled: 0.0532 (0.0621)  loss_bbox_0_unscaled: 0.0091 (0.0097)  loss_giou_0_unscaled: 0.1857 (0.1973)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0063 (0.0069)  loss_ce_1_unscaled: 0.0458 (0.0535)  loss_bbox_1_unscaled: 0.0091 (0.0097)  loss_giou_1_unscaled: 0.1835 (0.1967)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0062 (0.0069)  loss_ce_2_unscaled: 0.0412 (0.0503)  loss_bbox_2_unscaled: 0.0091 (0.0097)  loss_giou_2_unscaled: 0.1856 (0.1955)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0064 (0.0069)  loss_ce_3_unscaled: 0.0348 (0.0476)  loss_bbox_3_unscaled: 0.0091 (0.0097)  loss_giou_3_unscaled: 0.1824 (0.1957)  loss_xy_3_unscaled: 0.0026 (0.0028)  loss_hw_3_unscaled: 0.0064 (0.0069)  loss_ce_4_unscaled: 0.0349 (0.0468)  loss_bbox_4_unscaled: 0.0091 (0.0097)  loss_giou_4_unscaled: 0.1833 (0.1955)  loss_xy_4_unscaled: 0.0026 (0.0028)  loss_hw_4_unscaled: 0.0063 (0.0069)  loss_ce_interm_unscaled: 0.0711 (0.0825)  loss_bbox_interm_unscaled: 0.0095 (0.0100)  loss_giou_interm_unscaled: 0.1868 (0.1990)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0065 (0.0071)  time: 2.8037  data: 0.0154  max mem: 8948\n",
            "Epoch: [8]  [70/90]  eta: 0:00:57  lr: 0.000001  loss: 3.5414 (3.8636)  loss_ce: 0.0865 (0.0929)  loss_bbox: 0.0427 (0.0477)  loss_giou: 0.3662 (0.3900)  loss_ce_0: 0.1182 (0.1254)  loss_bbox_0: 0.0433 (0.0480)  loss_giou_0: 0.3714 (0.3934)  loss_ce_1: 0.0961 (0.1080)  loss_bbox_1: 0.0433 (0.0479)  loss_giou_1: 0.3701 (0.3924)  loss_ce_2: 0.0863 (0.1008)  loss_bbox_2: 0.0427 (0.0477)  loss_giou_2: 0.3732 (0.3905)  loss_ce_3: 0.0888 (0.0958)  loss_bbox_3: 0.0429 (0.0478)  loss_giou_3: 0.3648 (0.3906)  loss_ce_4: 0.0883 (0.0934)  loss_bbox_4: 0.0428 (0.0477)  loss_giou_4: 0.3670 (0.3906)  loss_ce_interm: 0.1456 (0.1660)  loss_bbox_interm: 0.0447 (0.0494)  loss_giou_interm: 0.3736 (0.3976)  loss_ce_unscaled: 0.0433 (0.0465)  loss_bbox_unscaled: 0.0085 (0.0095)  loss_giou_unscaled: 0.1831 (0.1950)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0059 (0.0067)  loss_ce_0_unscaled: 0.0591 (0.0627)  loss_bbox_0_unscaled: 0.0087 (0.0096)  loss_giou_0_unscaled: 0.1857 (0.1967)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0060 (0.0068)  loss_ce_1_unscaled: 0.0481 (0.0540)  loss_bbox_1_unscaled: 0.0087 (0.0096)  loss_giou_1_unscaled: 0.1850 (0.1962)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0058 (0.0068)  loss_ce_2_unscaled: 0.0432 (0.0504)  loss_bbox_2_unscaled: 0.0085 (0.0095)  loss_giou_2_unscaled: 0.1866 (0.1952)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0060 (0.0067)  loss_ce_3_unscaled: 0.0444 (0.0479)  loss_bbox_3_unscaled: 0.0086 (0.0096)  loss_giou_3_unscaled: 0.1824 (0.1953)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0058 (0.0067)  loss_ce_4_unscaled: 0.0442 (0.0467)  loss_bbox_4_unscaled: 0.0086 (0.0095)  loss_giou_4_unscaled: 0.1835 (0.1953)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0060 (0.0068)  loss_ce_interm_unscaled: 0.0728 (0.0830)  loss_bbox_interm_unscaled: 0.0089 (0.0099)  loss_giou_interm_unscaled: 0.1868 (0.1988)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0062 (0.0070)  time: 2.9220  data: 0.0153  max mem: 8948\n",
            "Epoch: [8]  [80/90]  eta: 0:00:28  lr: 0.000001  loss: 3.7874 (3.8696)  loss_ce: 0.0818 (0.0925)  loss_bbox: 0.0442 (0.0480)  loss_giou: 0.3761 (0.3909)  loss_ce_0: 0.1080 (0.1241)  loss_bbox_0: 0.0453 (0.0483)  loss_giou_0: 0.3893 (0.3949)  loss_ce_1: 0.0981 (0.1078)  loss_bbox_1: 0.0447 (0.0480)  loss_giou_1: 0.3862 (0.3936)  loss_ce_2: 0.0868 (0.1005)  loss_bbox_2: 0.0437 (0.0479)  loss_giou_2: 0.3834 (0.3917)  loss_ce_3: 0.0888 (0.0951)  loss_bbox_3: 0.0445 (0.0480)  loss_giou_3: 0.3796 (0.3916)  loss_ce_4: 0.0883 (0.0929)  loss_bbox_4: 0.0442 (0.0480)  loss_giou_4: 0.3772 (0.3916)  loss_ce_interm: 0.1513 (0.1653)  loss_bbox_interm: 0.0456 (0.0496)  loss_giou_interm: 0.4026 (0.3993)  loss_ce_unscaled: 0.0409 (0.0463)  loss_bbox_unscaled: 0.0088 (0.0096)  loss_giou_unscaled: 0.1881 (0.1954)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0060 (0.0068)  loss_ce_0_unscaled: 0.0540 (0.0620)  loss_bbox_0_unscaled: 0.0091 (0.0097)  loss_giou_0_unscaled: 0.1946 (0.1975)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0062 (0.0068)  loss_ce_1_unscaled: 0.0490 (0.0539)  loss_bbox_1_unscaled: 0.0089 (0.0096)  loss_giou_1_unscaled: 0.1931 (0.1968)  loss_xy_1_unscaled: 0.0029 (0.0028)  loss_hw_1_unscaled: 0.0062 (0.0068)  loss_ce_2_unscaled: 0.0434 (0.0502)  loss_bbox_2_unscaled: 0.0087 (0.0096)  loss_giou_2_unscaled: 0.1917 (0.1958)  loss_xy_2_unscaled: 0.0029 (0.0028)  loss_hw_2_unscaled: 0.0060 (0.0067)  loss_ce_3_unscaled: 0.0444 (0.0475)  loss_bbox_3_unscaled: 0.0089 (0.0096)  loss_giou_3_unscaled: 0.1898 (0.1958)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0061 (0.0068)  loss_ce_4_unscaled: 0.0442 (0.0464)  loss_bbox_4_unscaled: 0.0088 (0.0096)  loss_giou_4_unscaled: 0.1886 (0.1958)  loss_xy_4_unscaled: 0.0029 (0.0028)  loss_hw_4_unscaled: 0.0061 (0.0068)  loss_ce_interm_unscaled: 0.0756 (0.0826)  loss_bbox_interm_unscaled: 0.0091 (0.0099)  loss_giou_interm_unscaled: 0.2013 (0.1997)  loss_xy_interm_unscaled: 0.0029 (0.0029)  loss_hw_interm_unscaled: 0.0063 (0.0070)  time: 2.8372  data: 0.0150  max mem: 8948\n",
            "Epoch: [8]  [89/90]  eta: 0:00:02  lr: 0.000001  loss: 3.7961 (3.8556)  loss_ce: 0.0703 (0.0906)  loss_bbox: 0.0491 (0.0481)  loss_giou: 0.3967 (0.3903)  loss_ce_0: 0.1042 (0.1227)  loss_bbox_0: 0.0483 (0.0484)  loss_giou_0: 0.3975 (0.3942)  loss_ce_1: 0.0876 (0.1062)  loss_bbox_1: 0.0477 (0.0481)  loss_giou_1: 0.4019 (0.3933)  loss_ce_2: 0.0804 (0.0992)  loss_bbox_2: 0.0479 (0.0481)  loss_giou_2: 0.3888 (0.3909)  loss_ce_3: 0.0741 (0.0934)  loss_bbox_3: 0.0475 (0.0480)  loss_giou_3: 0.3901 (0.3910)  loss_ce_4: 0.0661 (0.0917)  loss_bbox_4: 0.0493 (0.0481)  loss_giou_4: 0.3894 (0.3906)  loss_ce_interm: 0.1510 (0.1650)  loss_bbox_interm: 0.0502 (0.0497)  loss_giou_interm: 0.4023 (0.3981)  loss_ce_unscaled: 0.0352 (0.0453)  loss_bbox_unscaled: 0.0098 (0.0096)  loss_giou_unscaled: 0.1984 (0.1952)  loss_xy_unscaled: 0.0029 (0.0029)  loss_hw_unscaled: 0.0068 (0.0068)  loss_ce_0_unscaled: 0.0521 (0.0614)  loss_bbox_0_unscaled: 0.0097 (0.0097)  loss_giou_0_unscaled: 0.1988 (0.1971)  loss_xy_0_unscaled: 0.0030 (0.0029)  loss_hw_0_unscaled: 0.0069 (0.0068)  loss_ce_1_unscaled: 0.0438 (0.0531)  loss_bbox_1_unscaled: 0.0095 (0.0096)  loss_giou_1_unscaled: 0.2010 (0.1967)  loss_xy_1_unscaled: 0.0029 (0.0029)  loss_hw_1_unscaled: 0.0067 (0.0068)  loss_ce_2_unscaled: 0.0402 (0.0496)  loss_bbox_2_unscaled: 0.0096 (0.0096)  loss_giou_2_unscaled: 0.1944 (0.1955)  loss_xy_2_unscaled: 0.0029 (0.0028)  loss_hw_2_unscaled: 0.0068 (0.0068)  loss_ce_3_unscaled: 0.0370 (0.0467)  loss_bbox_3_unscaled: 0.0095 (0.0096)  loss_giou_3_unscaled: 0.1951 (0.1955)  loss_xy_3_unscaled: 0.0029 (0.0028)  loss_hw_3_unscaled: 0.0066 (0.0068)  loss_ce_4_unscaled: 0.0330 (0.0458)  loss_bbox_4_unscaled: 0.0099 (0.0096)  loss_giou_4_unscaled: 0.1947 (0.1953)  loss_xy_4_unscaled: 0.0029 (0.0029)  loss_hw_4_unscaled: 0.0068 (0.0068)  loss_ce_interm_unscaled: 0.0755 (0.0825)  loss_bbox_interm_unscaled: 0.0100 (0.0099)  loss_giou_interm_unscaled: 0.2012 (0.1990)  loss_xy_interm_unscaled: 0.0031 (0.0030)  loss_hw_interm_unscaled: 0.0071 (0.0070)  time: 2.7803  data: 0.0156  max mem: 8948\n",
            "Epoch: [8] Total time: 0:04:15 (2.8439 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 3.7961 (3.8556)  loss_ce: 0.0703 (0.0906)  loss_bbox: 0.0491 (0.0481)  loss_giou: 0.3967 (0.3903)  loss_ce_0: 0.1042 (0.1227)  loss_bbox_0: 0.0483 (0.0484)  loss_giou_0: 0.3975 (0.3942)  loss_ce_1: 0.0876 (0.1062)  loss_bbox_1: 0.0477 (0.0481)  loss_giou_1: 0.4019 (0.3933)  loss_ce_2: 0.0804 (0.0992)  loss_bbox_2: 0.0479 (0.0481)  loss_giou_2: 0.3888 (0.3909)  loss_ce_3: 0.0741 (0.0934)  loss_bbox_3: 0.0475 (0.0480)  loss_giou_3: 0.3901 (0.3910)  loss_ce_4: 0.0661 (0.0917)  loss_bbox_4: 0.0493 (0.0481)  loss_giou_4: 0.3894 (0.3906)  loss_ce_interm: 0.1510 (0.1650)  loss_bbox_interm: 0.0502 (0.0497)  loss_giou_interm: 0.4023 (0.3981)  loss_ce_unscaled: 0.0352 (0.0453)  loss_bbox_unscaled: 0.0098 (0.0096)  loss_giou_unscaled: 0.1984 (0.1952)  loss_xy_unscaled: 0.0029 (0.0029)  loss_hw_unscaled: 0.0068 (0.0068)  loss_ce_0_unscaled: 0.0521 (0.0614)  loss_bbox_0_unscaled: 0.0097 (0.0097)  loss_giou_0_unscaled: 0.1988 (0.1971)  loss_xy_0_unscaled: 0.0030 (0.0029)  loss_hw_0_unscaled: 0.0069 (0.0068)  loss_ce_1_unscaled: 0.0438 (0.0531)  loss_bbox_1_unscaled: 0.0095 (0.0096)  loss_giou_1_unscaled: 0.2010 (0.1967)  loss_xy_1_unscaled: 0.0029 (0.0029)  loss_hw_1_unscaled: 0.0067 (0.0068)  loss_ce_2_unscaled: 0.0402 (0.0496)  loss_bbox_2_unscaled: 0.0096 (0.0096)  loss_giou_2_unscaled: 0.1944 (0.1955)  loss_xy_2_unscaled: 0.0029 (0.0028)  loss_hw_2_unscaled: 0.0068 (0.0068)  loss_ce_3_unscaled: 0.0370 (0.0467)  loss_bbox_3_unscaled: 0.0095 (0.0096)  loss_giou_3_unscaled: 0.1951 (0.1955)  loss_xy_3_unscaled: 0.0029 (0.0028)  loss_hw_3_unscaled: 0.0066 (0.0068)  loss_ce_4_unscaled: 0.0330 (0.0458)  loss_bbox_4_unscaled: 0.0099 (0.0096)  loss_giou_4_unscaled: 0.1947 (0.1953)  loss_xy_4_unscaled: 0.0029 (0.0029)  loss_hw_4_unscaled: 0.0068 (0.0068)  loss_ce_interm_unscaled: 0.0755 (0.0825)  loss_bbox_interm_unscaled: 0.0100 (0.0099)  loss_giou_interm_unscaled: 0.2012 (0.1990)  loss_xy_interm_unscaled: 0.0031 (0.0030)  loss_hw_interm_unscaled: 0.0071 (0.0070)\n",
            "Input text prompt: bolt . wrong direction . 1 . 2 . 3 . 4 . 5 .\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Test:  [ 0/10]  eta: 0:00:17    time: 1.7275  data: 0.7773  max mem: 8948\n",
            "Test:  [ 9/10]  eta: 0:00:01    time: 1.0238  data: 0.0948  max mem: 8948\n",
            "Test: Total time: 0:00:10 (1.0330 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.058\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.119\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.043\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.056\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.286\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.220\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.011\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.250\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.486\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.461\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.599\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.408\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch: [9]  [ 0/90]  eta: 0:05:59  lr: 0.000001  loss: 3.6254 (3.6254)  loss_ce: 0.0752 (0.0752)  loss_bbox: 0.0356 (0.0356)  loss_giou: 0.3758 (0.3758)  loss_ce_0: 0.1324 (0.1324)  loss_bbox_0: 0.0352 (0.0352)  loss_giou_0: 0.3636 (0.3636)  loss_ce_1: 0.1213 (0.1213)  loss_bbox_1: 0.0352 (0.0352)  loss_giou_1: 0.3755 (0.3755)  loss_ce_2: 0.1096 (0.1096)  loss_bbox_2: 0.0352 (0.0352)  loss_giou_2: 0.3752 (0.3752)  loss_ce_3: 0.0835 (0.0835)  loss_bbox_3: 0.0359 (0.0359)  loss_giou_3: 0.3770 (0.3770)  loss_ce_4: 0.0795 (0.0795)  loss_bbox_4: 0.0355 (0.0355)  loss_giou_4: 0.3732 (0.3732)  loss_ce_interm: 0.1620 (0.1620)  loss_bbox_interm: 0.0388 (0.0388)  loss_giou_interm: 0.3703 (0.3703)  loss_ce_unscaled: 0.0376 (0.0376)  loss_bbox_unscaled: 0.0071 (0.0071)  loss_giou_unscaled: 0.1879 (0.1879)  loss_xy_unscaled: 0.0022 (0.0022)  loss_hw_unscaled: 0.0049 (0.0049)  loss_ce_0_unscaled: 0.0662 (0.0662)  loss_bbox_0_unscaled: 0.0070 (0.0070)  loss_giou_0_unscaled: 0.1818 (0.1818)  loss_xy_0_unscaled: 0.0022 (0.0022)  loss_hw_0_unscaled: 0.0048 (0.0048)  loss_ce_1_unscaled: 0.0607 (0.0607)  loss_bbox_1_unscaled: 0.0070 (0.0070)  loss_giou_1_unscaled: 0.1878 (0.1878)  loss_xy_1_unscaled: 0.0022 (0.0022)  loss_hw_1_unscaled: 0.0049 (0.0049)  loss_ce_2_unscaled: 0.0548 (0.0548)  loss_bbox_2_unscaled: 0.0070 (0.0070)  loss_giou_2_unscaled: 0.1876 (0.1876)  loss_xy_2_unscaled: 0.0022 (0.0022)  loss_hw_2_unscaled: 0.0049 (0.0049)  loss_ce_3_unscaled: 0.0417 (0.0417)  loss_bbox_3_unscaled: 0.0072 (0.0072)  loss_giou_3_unscaled: 0.1885 (0.1885)  loss_xy_3_unscaled: 0.0021 (0.0021)  loss_hw_3_unscaled: 0.0050 (0.0050)  loss_ce_4_unscaled: 0.0397 (0.0397)  loss_bbox_4_unscaled: 0.0071 (0.0071)  loss_giou_4_unscaled: 0.1866 (0.1866)  loss_xy_4_unscaled: 0.0022 (0.0022)  loss_hw_4_unscaled: 0.0049 (0.0049)  loss_ce_interm_unscaled: 0.0810 (0.0810)  loss_bbox_interm_unscaled: 0.0078 (0.0078)  loss_giou_interm_unscaled: 0.1852 (0.1852)  loss_xy_interm_unscaled: 0.0024 (0.0024)  loss_hw_interm_unscaled: 0.0054 (0.0054)  time: 3.9939  data: 0.7274  max mem: 8948\n",
            "Epoch: [9]  [10/90]  eta: 0:03:46  lr: 0.000001  loss: 3.6254 (3.6107)  loss_ce: 0.0638 (0.0630)  loss_bbox: 0.0457 (0.0461)  loss_giou: 0.3789 (0.3794)  loss_ce_0: 0.0928 (0.1026)  loss_bbox_0: 0.0453 (0.0466)  loss_giou_0: 0.3779 (0.3824)  loss_ce_1: 0.0866 (0.0871)  loss_bbox_1: 0.0444 (0.0465)  loss_giou_1: 0.3764 (0.3827)  loss_ce_2: 0.0736 (0.0840)  loss_bbox_2: 0.0439 (0.0459)  loss_giou_2: 0.3752 (0.3794)  loss_ce_3: 0.0719 (0.0698)  loss_bbox_3: 0.0452 (0.0462)  loss_giou_3: 0.3798 (0.3823)  loss_ce_4: 0.0709 (0.0687)  loss_bbox_4: 0.0454 (0.0460)  loss_giou_4: 0.3786 (0.3789)  loss_ce_interm: 0.1315 (0.1441)  loss_bbox_interm: 0.0456 (0.0478)  loss_giou_interm: 0.3802 (0.3812)  loss_ce_unscaled: 0.0319 (0.0315)  loss_bbox_unscaled: 0.0091 (0.0092)  loss_giou_unscaled: 0.1894 (0.1897)  loss_xy_unscaled: 0.0026 (0.0027)  loss_hw_unscaled: 0.0061 (0.0065)  loss_ce_0_unscaled: 0.0464 (0.0513)  loss_bbox_0_unscaled: 0.0091 (0.0093)  loss_giou_0_unscaled: 0.1889 (0.1912)  loss_xy_0_unscaled: 0.0027 (0.0027)  loss_hw_0_unscaled: 0.0061 (0.0066)  loss_ce_1_unscaled: 0.0433 (0.0436)  loss_bbox_1_unscaled: 0.0089 (0.0093)  loss_giou_1_unscaled: 0.1882 (0.1914)  loss_xy_1_unscaled: 0.0026 (0.0027)  loss_hw_1_unscaled: 0.0062 (0.0066)  loss_ce_2_unscaled: 0.0368 (0.0420)  loss_bbox_2_unscaled: 0.0088 (0.0092)  loss_giou_2_unscaled: 0.1876 (0.1897)  loss_xy_2_unscaled: 0.0026 (0.0026)  loss_hw_2_unscaled: 0.0061 (0.0065)  loss_ce_3_unscaled: 0.0359 (0.0349)  loss_bbox_3_unscaled: 0.0090 (0.0092)  loss_giou_3_unscaled: 0.1899 (0.1911)  loss_xy_3_unscaled: 0.0026 (0.0027)  loss_hw_3_unscaled: 0.0062 (0.0066)  loss_ce_4_unscaled: 0.0354 (0.0343)  loss_bbox_4_unscaled: 0.0091 (0.0092)  loss_giou_4_unscaled: 0.1893 (0.1894)  loss_xy_4_unscaled: 0.0026 (0.0027)  loss_hw_4_unscaled: 0.0061 (0.0065)  loss_ce_interm_unscaled: 0.0658 (0.0720)  loss_bbox_interm_unscaled: 0.0091 (0.0096)  loss_giou_interm_unscaled: 0.1901 (0.1906)  loss_xy_interm_unscaled: 0.0027 (0.0028)  loss_hw_interm_unscaled: 0.0065 (0.0068)  time: 2.8318  data: 0.0811  max mem: 8948\n",
            "Epoch: [9]  [20/90]  eta: 0:03:06  lr: 0.000001  loss: 3.6016 (3.6741)  loss_ce: 0.0686 (0.0737)  loss_bbox: 0.0444 (0.0458)  loss_giou: 0.3789 (0.3801)  loss_ce_0: 0.1039 (0.1114)  loss_bbox_0: 0.0453 (0.0464)  loss_giou_0: 0.3721 (0.3826)  loss_ce_1: 0.0866 (0.0940)  loss_bbox_1: 0.0444 (0.0465)  loss_giou_1: 0.3764 (0.3841)  loss_ce_2: 0.0759 (0.0879)  loss_bbox_2: 0.0439 (0.0457)  loss_giou_2: 0.3775 (0.3818)  loss_ce_3: 0.0708 (0.0805)  loss_bbox_3: 0.0452 (0.0460)  loss_giou_3: 0.3798 (0.3828)  loss_ce_4: 0.0709 (0.0791)  loss_bbox_4: 0.0443 (0.0458)  loss_giou_4: 0.3786 (0.3801)  loss_ce_interm: 0.1402 (0.1526)  loss_bbox_interm: 0.0456 (0.0471)  loss_giou_interm: 0.3802 (0.3801)  loss_ce_unscaled: 0.0343 (0.0368)  loss_bbox_unscaled: 0.0089 (0.0092)  loss_giou_unscaled: 0.1894 (0.1900)  loss_xy_unscaled: 0.0026 (0.0027)  loss_hw_unscaled: 0.0062 (0.0065)  loss_ce_0_unscaled: 0.0520 (0.0557)  loss_bbox_0_unscaled: 0.0091 (0.0093)  loss_giou_0_unscaled: 0.1861 (0.1913)  loss_xy_0_unscaled: 0.0027 (0.0027)  loss_hw_0_unscaled: 0.0062 (0.0065)  loss_ce_1_unscaled: 0.0433 (0.0470)  loss_bbox_1_unscaled: 0.0089 (0.0093)  loss_giou_1_unscaled: 0.1882 (0.1921)  loss_xy_1_unscaled: 0.0027 (0.0027)  loss_hw_1_unscaled: 0.0062 (0.0066)  loss_ce_2_unscaled: 0.0380 (0.0440)  loss_bbox_2_unscaled: 0.0088 (0.0091)  loss_giou_2_unscaled: 0.1888 (0.1909)  loss_xy_2_unscaled: 0.0026 (0.0027)  loss_hw_2_unscaled: 0.0061 (0.0065)  loss_ce_3_unscaled: 0.0354 (0.0403)  loss_bbox_3_unscaled: 0.0090 (0.0092)  loss_giou_3_unscaled: 0.1899 (0.1914)  loss_xy_3_unscaled: 0.0027 (0.0027)  loss_hw_3_unscaled: 0.0062 (0.0065)  loss_ce_4_unscaled: 0.0354 (0.0395)  loss_bbox_4_unscaled: 0.0089 (0.0092)  loss_giou_4_unscaled: 0.1893 (0.1901)  loss_xy_4_unscaled: 0.0026 (0.0027)  loss_hw_4_unscaled: 0.0062 (0.0065)  loss_ce_interm_unscaled: 0.0701 (0.0763)  loss_bbox_interm_unscaled: 0.0091 (0.0094)  loss_giou_interm_unscaled: 0.1901 (0.1900)  loss_xy_interm_unscaled: 0.0028 (0.0028)  loss_hw_interm_unscaled: 0.0065 (0.0066)  time: 2.6045  data: 0.0154  max mem: 8948\n",
            "Epoch: [9]  [30/90]  eta: 0:02:40  lr: 0.000001  loss: 3.8859 (3.7753)  loss_ce: 0.0851 (0.0763)  loss_bbox: 0.0472 (0.0478)  loss_giou: 0.3875 (0.3904)  loss_ce_0: 0.1144 (0.1118)  loss_bbox_0: 0.0492 (0.0483)  loss_giou_0: 0.3926 (0.3930)  loss_ce_1: 0.0972 (0.0968)  loss_bbox_1: 0.0486 (0.0481)  loss_giou_1: 0.4020 (0.3935)  loss_ce_2: 0.0816 (0.0907)  loss_bbox_2: 0.0475 (0.0475)  loss_giou_2: 0.3937 (0.3907)  loss_ce_3: 0.0935 (0.0843)  loss_bbox_3: 0.0473 (0.0476)  loss_giou_3: 0.3936 (0.3912)  loss_ce_4: 0.0935 (0.0821)  loss_bbox_4: 0.0471 (0.0476)  loss_giou_4: 0.3883 (0.3903)  loss_ce_interm: 0.1517 (0.1557)  loss_bbox_interm: 0.0496 (0.0492)  loss_giou_interm: 0.3941 (0.3923)  loss_ce_unscaled: 0.0426 (0.0381)  loss_bbox_unscaled: 0.0094 (0.0096)  loss_giou_unscaled: 0.1938 (0.1952)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0068 (0.0068)  loss_ce_0_unscaled: 0.0572 (0.0559)  loss_bbox_0_unscaled: 0.0098 (0.0097)  loss_giou_0_unscaled: 0.1963 (0.1965)  loss_xy_0_unscaled: 0.0029 (0.0028)  loss_hw_0_unscaled: 0.0070 (0.0069)  loss_ce_1_unscaled: 0.0486 (0.0484)  loss_bbox_1_unscaled: 0.0097 (0.0096)  loss_giou_1_unscaled: 0.2010 (0.1967)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0071 (0.0068)  loss_ce_2_unscaled: 0.0408 (0.0453)  loss_bbox_2_unscaled: 0.0095 (0.0095)  loss_giou_2_unscaled: 0.1969 (0.1954)  loss_xy_2_unscaled: 0.0027 (0.0027)  loss_hw_2_unscaled: 0.0068 (0.0068)  loss_ce_3_unscaled: 0.0468 (0.0422)  loss_bbox_3_unscaled: 0.0095 (0.0095)  loss_giou_3_unscaled: 0.1968 (0.1956)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0068 (0.0068)  loss_ce_4_unscaled: 0.0468 (0.0410)  loss_bbox_4_unscaled: 0.0094 (0.0095)  loss_giou_4_unscaled: 0.1941 (0.1952)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0068 (0.0068)  loss_ce_interm_unscaled: 0.0759 (0.0778)  loss_bbox_interm_unscaled: 0.0099 (0.0098)  loss_giou_interm_unscaled: 0.1970 (0.1961)  loss_xy_interm_unscaled: 0.0029 (0.0029)  loss_hw_interm_unscaled: 0.0070 (0.0069)  time: 2.5992  data: 0.0147  max mem: 8948\n",
            "Epoch: [9]  [40/90]  eta: 0:02:13  lr: 0.000001  loss: 3.8577 (3.7564)  loss_ce: 0.0845 (0.0778)  loss_bbox: 0.0472 (0.0478)  loss_giou: 0.3817 (0.3875)  loss_ce_0: 0.1133 (0.1124)  loss_bbox_0: 0.0470 (0.0483)  loss_giou_0: 0.3924 (0.3894)  loss_ce_1: 0.0988 (0.0980)  loss_bbox_1: 0.0475 (0.0481)  loss_giou_1: 0.3846 (0.3896)  loss_ce_2: 0.0885 (0.0909)  loss_bbox_2: 0.0475 (0.0477)  loss_giou_2: 0.3822 (0.3876)  loss_ce_3: 0.0790 (0.0845)  loss_bbox_3: 0.0473 (0.0477)  loss_giou_3: 0.3805 (0.3881)  loss_ce_4: 0.0875 (0.0830)  loss_bbox_4: 0.0472 (0.0477)  loss_giou_4: 0.3806 (0.3873)  loss_ce_interm: 0.1489 (0.1525)  loss_bbox_interm: 0.0496 (0.0493)  loss_giou_interm: 0.3941 (0.3912)  loss_ce_unscaled: 0.0423 (0.0389)  loss_bbox_unscaled: 0.0094 (0.0096)  loss_giou_unscaled: 0.1908 (0.1938)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0067 (0.0068)  loss_ce_0_unscaled: 0.0567 (0.0562)  loss_bbox_0_unscaled: 0.0094 (0.0097)  loss_giou_0_unscaled: 0.1962 (0.1947)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0067 (0.0069)  loss_ce_1_unscaled: 0.0494 (0.0490)  loss_bbox_1_unscaled: 0.0095 (0.0096)  loss_giou_1_unscaled: 0.1923 (0.1948)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0067 (0.0068)  loss_ce_2_unscaled: 0.0443 (0.0454)  loss_bbox_2_unscaled: 0.0095 (0.0095)  loss_giou_2_unscaled: 0.1911 (0.1938)  loss_xy_2_unscaled: 0.0027 (0.0027)  loss_hw_2_unscaled: 0.0068 (0.0068)  loss_ce_3_unscaled: 0.0395 (0.0422)  loss_bbox_3_unscaled: 0.0095 (0.0095)  loss_giou_3_unscaled: 0.1902 (0.1941)  loss_xy_3_unscaled: 0.0027 (0.0027)  loss_hw_3_unscaled: 0.0067 (0.0068)  loss_ce_4_unscaled: 0.0438 (0.0415)  loss_bbox_4_unscaled: 0.0094 (0.0095)  loss_giou_4_unscaled: 0.1903 (0.1936)  loss_xy_4_unscaled: 0.0027 (0.0027)  loss_hw_4_unscaled: 0.0067 (0.0068)  loss_ce_interm_unscaled: 0.0745 (0.0762)  loss_bbox_interm_unscaled: 0.0099 (0.0099)  loss_giou_interm_unscaled: 0.1970 (0.1956)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0070 (0.0070)  time: 2.6852  data: 0.0150  max mem: 8948\n",
            "Epoch: [9]  [50/90]  eta: 0:01:46  lr: 0.000001  loss: 3.6866 (3.8044)  loss_ce: 0.0808 (0.0892)  loss_bbox: 0.0448 (0.0479)  loss_giou: 0.3650 (0.3841)  loss_ce_0: 0.1072 (0.1186)  loss_bbox_0: 0.0453 (0.0484)  loss_giou_0: 0.3731 (0.3876)  loss_ce_1: 0.0988 (0.1063)  loss_bbox_1: 0.0451 (0.0481)  loss_giou_1: 0.3706 (0.3884)  loss_ce_2: 0.0885 (0.0996)  loss_bbox_2: 0.0447 (0.0477)  loss_giou_2: 0.3699 (0.3860)  loss_ce_3: 0.0811 (0.0938)  loss_bbox_3: 0.0445 (0.0478)  loss_giou_3: 0.3690 (0.3864)  loss_ce_4: 0.0801 (0.0916)  loss_bbox_4: 0.0448 (0.0479)  loss_giou_4: 0.3723 (0.3854)  loss_ce_interm: 0.1441 (0.1593)  loss_bbox_interm: 0.0471 (0.0495)  loss_giou_interm: 0.3816 (0.3908)  loss_ce_unscaled: 0.0404 (0.0446)  loss_bbox_unscaled: 0.0090 (0.0096)  loss_giou_unscaled: 0.1825 (0.1920)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0061 (0.0068)  loss_ce_0_unscaled: 0.0536 (0.0593)  loss_bbox_0_unscaled: 0.0091 (0.0097)  loss_giou_0_unscaled: 0.1865 (0.1938)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0063 (0.0069)  loss_ce_1_unscaled: 0.0494 (0.0532)  loss_bbox_1_unscaled: 0.0090 (0.0096)  loss_giou_1_unscaled: 0.1853 (0.1942)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0062 (0.0068)  loss_ce_2_unscaled: 0.0443 (0.0498)  loss_bbox_2_unscaled: 0.0089 (0.0095)  loss_giou_2_unscaled: 0.1849 (0.1930)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0061 (0.0068)  loss_ce_3_unscaled: 0.0406 (0.0469)  loss_bbox_3_unscaled: 0.0089 (0.0096)  loss_giou_3_unscaled: 0.1845 (0.1932)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0061 (0.0068)  loss_ce_4_unscaled: 0.0401 (0.0458)  loss_bbox_4_unscaled: 0.0090 (0.0096)  loss_giou_4_unscaled: 0.1862 (0.1927)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0061 (0.0068)  loss_ce_interm_unscaled: 0.0721 (0.0797)  loss_bbox_interm_unscaled: 0.0094 (0.0099)  loss_giou_interm_unscaled: 0.1908 (0.1954)  loss_xy_interm_unscaled: 0.0029 (0.0029)  loss_hw_interm_unscaled: 0.0063 (0.0070)  time: 2.6359  data: 0.0152  max mem: 8948\n",
            "Epoch: [9]  [60/90]  eta: 0:01:20  lr: 0.000001  loss: 3.6866 (3.8141)  loss_ce: 0.0754 (0.0897)  loss_bbox: 0.0449 (0.0476)  loss_giou: 0.3692 (0.3839)  loss_ce_0: 0.1071 (0.1213)  loss_bbox_0: 0.0463 (0.0484)  loss_giou_0: 0.3739 (0.3876)  loss_ce_1: 0.0987 (0.1081)  loss_bbox_1: 0.0456 (0.0479)  loss_giou_1: 0.3682 (0.3883)  loss_ce_2: 0.0907 (0.1006)  loss_bbox_2: 0.0451 (0.0476)  loss_giou_2: 0.3718 (0.3862)  loss_ce_3: 0.0811 (0.0955)  loss_bbox_3: 0.0449 (0.0476)  loss_giou_3: 0.3693 (0.3863)  loss_ce_4: 0.0795 (0.0919)  loss_bbox_4: 0.0448 (0.0476)  loss_giou_4: 0.3723 (0.3855)  loss_ce_interm: 0.1441 (0.1631)  loss_bbox_interm: 0.0478 (0.0493)  loss_giou_interm: 0.3811 (0.3901)  loss_ce_unscaled: 0.0377 (0.0448)  loss_bbox_unscaled: 0.0090 (0.0095)  loss_giou_unscaled: 0.1846 (0.1920)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0062 (0.0067)  loss_ce_0_unscaled: 0.0536 (0.0607)  loss_bbox_0_unscaled: 0.0093 (0.0097)  loss_giou_0_unscaled: 0.1870 (0.1938)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0063 (0.0068)  loss_ce_1_unscaled: 0.0494 (0.0540)  loss_bbox_1_unscaled: 0.0091 (0.0096)  loss_giou_1_unscaled: 0.1841 (0.1942)  loss_xy_1_unscaled: 0.0029 (0.0028)  loss_hw_1_unscaled: 0.0063 (0.0068)  loss_ce_2_unscaled: 0.0453 (0.0503)  loss_bbox_2_unscaled: 0.0090 (0.0095)  loss_giou_2_unscaled: 0.1859 (0.1931)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0062 (0.0068)  loss_ce_3_unscaled: 0.0406 (0.0477)  loss_bbox_3_unscaled: 0.0090 (0.0095)  loss_giou_3_unscaled: 0.1847 (0.1932)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0062 (0.0068)  loss_ce_4_unscaled: 0.0397 (0.0460)  loss_bbox_4_unscaled: 0.0090 (0.0095)  loss_giou_4_unscaled: 0.1862 (0.1927)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0062 (0.0067)  loss_ce_interm_unscaled: 0.0721 (0.0815)  loss_bbox_interm_unscaled: 0.0096 (0.0099)  loss_giou_interm_unscaled: 0.1906 (0.1951)  loss_xy_interm_unscaled: 0.0030 (0.0029)  loss_hw_interm_unscaled: 0.0065 (0.0070)  time: 2.7002  data: 0.0152  max mem: 8948\n",
            "Epoch: [9]  [70/90]  eta: 0:00:54  lr: 0.000001  loss: 3.7094 (3.8009)  loss_ce: 0.0637 (0.0894)  loss_bbox: 0.0438 (0.0472)  loss_giou: 0.3697 (0.3835)  loss_ce_0: 0.1041 (0.1203)  loss_bbox_0: 0.0454 (0.0479)  loss_giou_0: 0.3739 (0.3873)  loss_ce_1: 0.0866 (0.1064)  loss_bbox_1: 0.0448 (0.0475)  loss_giou_1: 0.3682 (0.3877)  loss_ce_2: 0.0813 (0.0995)  loss_bbox_2: 0.0445 (0.0472)  loss_giou_2: 0.3758 (0.3856)  loss_ce_3: 0.0739 (0.0942)  loss_bbox_3: 0.0440 (0.0472)  loss_giou_3: 0.3783 (0.3857)  loss_ce_4: 0.0716 (0.0916)  loss_bbox_4: 0.0437 (0.0472)  loss_giou_4: 0.3700 (0.3847)  loss_ce_interm: 0.1405 (0.1609)  loss_bbox_interm: 0.0483 (0.0490)  loss_giou_interm: 0.3811 (0.3906)  loss_ce_unscaled: 0.0319 (0.0447)  loss_bbox_unscaled: 0.0088 (0.0094)  loss_giou_unscaled: 0.1848 (0.1917)  loss_xy_unscaled: 0.0026 (0.0028)  loss_hw_unscaled: 0.0060 (0.0067)  loss_ce_0_unscaled: 0.0520 (0.0601)  loss_bbox_0_unscaled: 0.0091 (0.0096)  loss_giou_0_unscaled: 0.1870 (0.1937)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0062 (0.0068)  loss_ce_1_unscaled: 0.0433 (0.0532)  loss_bbox_1_unscaled: 0.0090 (0.0095)  loss_giou_1_unscaled: 0.1841 (0.1939)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0061 (0.0067)  loss_ce_2_unscaled: 0.0407 (0.0497)  loss_bbox_2_unscaled: 0.0089 (0.0094)  loss_giou_2_unscaled: 0.1879 (0.1928)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0061 (0.0067)  loss_ce_3_unscaled: 0.0369 (0.0471)  loss_bbox_3_unscaled: 0.0088 (0.0094)  loss_giou_3_unscaled: 0.1892 (0.1929)  loss_xy_3_unscaled: 0.0026 (0.0028)  loss_hw_3_unscaled: 0.0060 (0.0067)  loss_ce_4_unscaled: 0.0358 (0.0458)  loss_bbox_4_unscaled: 0.0087 (0.0094)  loss_giou_4_unscaled: 0.1850 (0.1924)  loss_xy_4_unscaled: 0.0026 (0.0028)  loss_hw_4_unscaled: 0.0060 (0.0067)  loss_ce_interm_unscaled: 0.0702 (0.0804)  loss_bbox_interm_unscaled: 0.0097 (0.0098)  loss_giou_interm_unscaled: 0.1906 (0.1953)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0067 (0.0069)  time: 2.7973  data: 0.0156  max mem: 8948\n",
            "Epoch: [9]  [80/90]  eta: 0:00:27  lr: 0.000001  loss: 3.6461 (3.7963)  loss_ce: 0.0622 (0.0868)  loss_bbox: 0.0438 (0.0472)  loss_giou: 0.3880 (0.3853)  loss_ce_0: 0.1051 (0.1185)  loss_bbox_0: 0.0454 (0.0478)  loss_giou_0: 0.3825 (0.3884)  loss_ce_1: 0.0800 (0.1039)  loss_bbox_1: 0.0448 (0.0474)  loss_giou_1: 0.3945 (0.3894)  loss_ce_2: 0.0765 (0.0972)  loss_bbox_2: 0.0445 (0.0472)  loss_giou_2: 0.3870 (0.3873)  loss_ce_3: 0.0692 (0.0921)  loss_bbox_3: 0.0440 (0.0472)  loss_giou_3: 0.3894 (0.3875)  loss_ce_4: 0.0716 (0.0891)  loss_bbox_4: 0.0437 (0.0472)  loss_giou_4: 0.3879 (0.3865)  loss_ce_interm: 0.1458 (0.1600)  loss_bbox_interm: 0.0480 (0.0489)  loss_giou_interm: 0.3810 (0.3914)  loss_ce_unscaled: 0.0311 (0.0434)  loss_bbox_unscaled: 0.0088 (0.0094)  loss_giou_unscaled: 0.1940 (0.1926)  loss_xy_unscaled: 0.0026 (0.0027)  loss_hw_unscaled: 0.0060 (0.0067)  loss_ce_0_unscaled: 0.0526 (0.0592)  loss_bbox_0_unscaled: 0.0091 (0.0096)  loss_giou_0_unscaled: 0.1913 (0.1942)  loss_xy_0_unscaled: 0.0026 (0.0028)  loss_hw_0_unscaled: 0.0062 (0.0068)  loss_ce_1_unscaled: 0.0400 (0.0519)  loss_bbox_1_unscaled: 0.0090 (0.0095)  loss_giou_1_unscaled: 0.1972 (0.1947)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0061 (0.0067)  loss_ce_2_unscaled: 0.0382 (0.0486)  loss_bbox_2_unscaled: 0.0089 (0.0094)  loss_giou_2_unscaled: 0.1935 (0.1936)  loss_xy_2_unscaled: 0.0026 (0.0027)  loss_hw_2_unscaled: 0.0061 (0.0067)  loss_ce_3_unscaled: 0.0346 (0.0461)  loss_bbox_3_unscaled: 0.0088 (0.0094)  loss_giou_3_unscaled: 0.1947 (0.1938)  loss_xy_3_unscaled: 0.0026 (0.0027)  loss_hw_3_unscaled: 0.0060 (0.0067)  loss_ce_4_unscaled: 0.0358 (0.0446)  loss_bbox_4_unscaled: 0.0087 (0.0094)  loss_giou_4_unscaled: 0.1940 (0.1933)  loss_xy_4_unscaled: 0.0026 (0.0027)  loss_hw_4_unscaled: 0.0060 (0.0067)  loss_ce_interm_unscaled: 0.0729 (0.0800)  loss_bbox_interm_unscaled: 0.0096 (0.0098)  loss_giou_interm_unscaled: 0.1905 (0.1957)  loss_xy_interm_unscaled: 0.0026 (0.0029)  loss_hw_interm_unscaled: 0.0067 (0.0069)  time: 2.8542  data: 0.0157  max mem: 8948\n",
            "Epoch: [9]  [89/90]  eta: 0:00:02  lr: 0.000001  loss: 3.7258 (3.7930)  loss_ce: 0.0592 (0.0855)  loss_bbox: 0.0460 (0.0471)  loss_giou: 0.3919 (0.3863)  loss_ce_0: 0.1066 (0.1175)  loss_bbox_0: 0.0465 (0.0477)  loss_giou_0: 0.3990 (0.3894)  loss_ce_1: 0.0789 (0.1028)  loss_bbox_1: 0.0463 (0.0473)  loss_giou_1: 0.3948 (0.3899)  loss_ce_2: 0.0742 (0.0961)  loss_bbox_2: 0.0461 (0.0471)  loss_giou_2: 0.3896 (0.3880)  loss_ce_3: 0.0678 (0.0908)  loss_bbox_3: 0.0462 (0.0471)  loss_giou_3: 0.3917 (0.3884)  loss_ce_4: 0.0678 (0.0880)  loss_bbox_4: 0.0460 (0.0471)  loss_giou_4: 0.3914 (0.3874)  loss_ce_interm: 0.1427 (0.1576)  loss_bbox_interm: 0.0482 (0.0488)  loss_giou_interm: 0.4101 (0.3930)  loss_ce_unscaled: 0.0296 (0.0428)  loss_bbox_unscaled: 0.0092 (0.0094)  loss_giou_unscaled: 0.1959 (0.1932)  loss_xy_unscaled: 0.0027 (0.0027)  loss_hw_unscaled: 0.0065 (0.0067)  loss_ce_0_unscaled: 0.0533 (0.0587)  loss_bbox_0_unscaled: 0.0093 (0.0095)  loss_giou_0_unscaled: 0.1995 (0.1947)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0065 (0.0068)  loss_ce_1_unscaled: 0.0395 (0.0514)  loss_bbox_1_unscaled: 0.0093 (0.0095)  loss_giou_1_unscaled: 0.1974 (0.1950)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0065 (0.0067)  loss_ce_2_unscaled: 0.0371 (0.0480)  loss_bbox_2_unscaled: 0.0092 (0.0094)  loss_giou_2_unscaled: 0.1948 (0.1940)  loss_xy_2_unscaled: 0.0027 (0.0027)  loss_hw_2_unscaled: 0.0065 (0.0067)  loss_ce_3_unscaled: 0.0339 (0.0454)  loss_bbox_3_unscaled: 0.0092 (0.0094)  loss_giou_3_unscaled: 0.1958 (0.1942)  loss_xy_3_unscaled: 0.0027 (0.0027)  loss_hw_3_unscaled: 0.0065 (0.0067)  loss_ce_4_unscaled: 0.0339 (0.0440)  loss_bbox_4_unscaled: 0.0092 (0.0094)  loss_giou_4_unscaled: 0.1957 (0.1937)  loss_xy_4_unscaled: 0.0027 (0.0027)  loss_hw_4_unscaled: 0.0065 (0.0067)  loss_ce_interm_unscaled: 0.0713 (0.0788)  loss_bbox_interm_unscaled: 0.0096 (0.0098)  loss_giou_interm_unscaled: 0.2051 (0.1965)  loss_xy_interm_unscaled: 0.0027 (0.0029)  loss_hw_interm_unscaled: 0.0067 (0.0069)  time: 2.8239  data: 0.0149  max mem: 8948\n",
            "Epoch: [9] Total time: 0:04:05 (2.7262 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 3.7258 (3.7930)  loss_ce: 0.0592 (0.0855)  loss_bbox: 0.0460 (0.0471)  loss_giou: 0.3919 (0.3863)  loss_ce_0: 0.1066 (0.1175)  loss_bbox_0: 0.0465 (0.0477)  loss_giou_0: 0.3990 (0.3894)  loss_ce_1: 0.0789 (0.1028)  loss_bbox_1: 0.0463 (0.0473)  loss_giou_1: 0.3948 (0.3899)  loss_ce_2: 0.0742 (0.0961)  loss_bbox_2: 0.0461 (0.0471)  loss_giou_2: 0.3896 (0.3880)  loss_ce_3: 0.0678 (0.0908)  loss_bbox_3: 0.0462 (0.0471)  loss_giou_3: 0.3917 (0.3884)  loss_ce_4: 0.0678 (0.0880)  loss_bbox_4: 0.0460 (0.0471)  loss_giou_4: 0.3914 (0.3874)  loss_ce_interm: 0.1427 (0.1576)  loss_bbox_interm: 0.0482 (0.0488)  loss_giou_interm: 0.4101 (0.3930)  loss_ce_unscaled: 0.0296 (0.0428)  loss_bbox_unscaled: 0.0092 (0.0094)  loss_giou_unscaled: 0.1959 (0.1932)  loss_xy_unscaled: 0.0027 (0.0027)  loss_hw_unscaled: 0.0065 (0.0067)  loss_ce_0_unscaled: 0.0533 (0.0587)  loss_bbox_0_unscaled: 0.0093 (0.0095)  loss_giou_0_unscaled: 0.1995 (0.1947)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0065 (0.0068)  loss_ce_1_unscaled: 0.0395 (0.0514)  loss_bbox_1_unscaled: 0.0093 (0.0095)  loss_giou_1_unscaled: 0.1974 (0.1950)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0065 (0.0067)  loss_ce_2_unscaled: 0.0371 (0.0480)  loss_bbox_2_unscaled: 0.0092 (0.0094)  loss_giou_2_unscaled: 0.1948 (0.1940)  loss_xy_2_unscaled: 0.0027 (0.0027)  loss_hw_2_unscaled: 0.0065 (0.0067)  loss_ce_3_unscaled: 0.0339 (0.0454)  loss_bbox_3_unscaled: 0.0092 (0.0094)  loss_giou_3_unscaled: 0.1958 (0.1942)  loss_xy_3_unscaled: 0.0027 (0.0027)  loss_hw_3_unscaled: 0.0065 (0.0067)  loss_ce_4_unscaled: 0.0339 (0.0440)  loss_bbox_4_unscaled: 0.0092 (0.0094)  loss_giou_4_unscaled: 0.1957 (0.1937)  loss_xy_4_unscaled: 0.0027 (0.0027)  loss_hw_4_unscaled: 0.0065 (0.0067)  loss_ce_interm_unscaled: 0.0713 (0.0788)  loss_bbox_interm_unscaled: 0.0096 (0.0098)  loss_giou_interm_unscaled: 0.2051 (0.1965)  loss_xy_interm_unscaled: 0.0027 (0.0029)  loss_hw_interm_unscaled: 0.0067 (0.0069)\n",
            "Input text prompt: bolt . wrong direction . 1 . 2 . 3 . 4 . 5 .\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Test:  [ 0/10]  eta: 0:00:18    time: 1.8736  data: 0.9501  max mem: 8948\n",
            "Test:  [ 9/10]  eta: 0:00:01    time: 1.0333  data: 0.1093  max mem: 8948\n",
            "Test: Total time: 0:00:10 (1.0425 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.056\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.118\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.038\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.054\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.285\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.223\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.009\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.246\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.458\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.579\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.446\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch: [10]  [ 0/90]  eta: 0:05:48  lr: 0.000001  loss: 4.0959 (4.0959)  loss_ce: 0.1343 (0.1343)  loss_bbox: 0.0477 (0.0477)  loss_giou: 0.3887 (0.3887)  loss_ce_0: 0.1262 (0.1262)  loss_bbox_0: 0.0509 (0.0509)  loss_giou_0: 0.3953 (0.3953)  loss_ce_1: 0.1732 (0.1732)  loss_bbox_1: 0.0518 (0.0518)  loss_giou_1: 0.3887 (0.3887)  loss_ce_2: 0.1463 (0.1463)  loss_bbox_2: 0.0492 (0.0492)  loss_giou_2: 0.3866 (0.3866)  loss_ce_3: 0.1286 (0.1286)  loss_bbox_3: 0.0471 (0.0471)  loss_giou_3: 0.3858 (0.3858)  loss_ce_4: 0.1354 (0.1354)  loss_bbox_4: 0.0475 (0.0475)  loss_giou_4: 0.3859 (0.3859)  loss_ce_interm: 0.1535 (0.1535)  loss_bbox_interm: 0.0540 (0.0540)  loss_giou_interm: 0.4190 (0.4190)  loss_ce_unscaled: 0.0671 (0.0671)  loss_bbox_unscaled: 0.0095 (0.0095)  loss_giou_unscaled: 0.1943 (0.1943)  loss_xy_unscaled: 0.0031 (0.0031)  loss_hw_unscaled: 0.0065 (0.0065)  loss_ce_0_unscaled: 0.0631 (0.0631)  loss_bbox_0_unscaled: 0.0102 (0.0102)  loss_giou_0_unscaled: 0.1977 (0.1977)  loss_xy_0_unscaled: 0.0033 (0.0033)  loss_hw_0_unscaled: 0.0069 (0.0069)  loss_ce_1_unscaled: 0.0866 (0.0866)  loss_bbox_1_unscaled: 0.0104 (0.0104)  loss_giou_1_unscaled: 0.1943 (0.1943)  loss_xy_1_unscaled: 0.0031 (0.0031)  loss_hw_1_unscaled: 0.0073 (0.0073)  loss_ce_2_unscaled: 0.0732 (0.0732)  loss_bbox_2_unscaled: 0.0098 (0.0098)  loss_giou_2_unscaled: 0.1933 (0.1933)  loss_xy_2_unscaled: 0.0030 (0.0030)  loss_hw_2_unscaled: 0.0069 (0.0069)  loss_ce_3_unscaled: 0.0643 (0.0643)  loss_bbox_3_unscaled: 0.0094 (0.0094)  loss_giou_3_unscaled: 0.1929 (0.1929)  loss_xy_3_unscaled: 0.0030 (0.0030)  loss_hw_3_unscaled: 0.0064 (0.0064)  loss_ce_4_unscaled: 0.0677 (0.0677)  loss_bbox_4_unscaled: 0.0095 (0.0095)  loss_giou_4_unscaled: 0.1930 (0.1930)  loss_xy_4_unscaled: 0.0031 (0.0031)  loss_hw_4_unscaled: 0.0064 (0.0064)  loss_ce_interm_unscaled: 0.0767 (0.0767)  loss_bbox_interm_unscaled: 0.0108 (0.0108)  loss_giou_interm_unscaled: 0.2095 (0.2095)  loss_xy_interm_unscaled: 0.0034 (0.0034)  loss_hw_interm_unscaled: 0.0074 (0.0074)  time: 3.8729  data: 0.8340  max mem: 8948\n",
            "Epoch: [10]  [10/90]  eta: 0:03:40  lr: 0.000001  loss: 4.0592 (3.9469)  loss_ce: 0.1068 (0.0975)  loss_bbox: 0.0474 (0.0488)  loss_giou: 0.3887 (0.3943)  loss_ce_0: 0.1262 (0.1395)  loss_bbox_0: 0.0467 (0.0492)  loss_giou_0: 0.3953 (0.3961)  loss_ce_1: 0.0961 (0.1199)  loss_bbox_1: 0.0470 (0.0486)  loss_giou_1: 0.3887 (0.3967)  loss_ce_2: 0.0924 (0.1034)  loss_bbox_2: 0.0473 (0.0498)  loss_giou_2: 0.3866 (0.3983)  loss_ce_3: 0.0996 (0.0960)  loss_bbox_3: 0.0471 (0.0497)  loss_giou_3: 0.3858 (0.3982)  loss_ce_4: 0.1021 (0.0957)  loss_bbox_4: 0.0474 (0.0497)  loss_giou_4: 0.3859 (0.3954)  loss_ce_interm: 0.1578 (0.1637)  loss_bbox_interm: 0.0481 (0.0518)  loss_giou_interm: 0.3909 (0.4045)  loss_ce_unscaled: 0.0534 (0.0488)  loss_bbox_unscaled: 0.0095 (0.0098)  loss_giou_unscaled: 0.1943 (0.1971)  loss_xy_unscaled: 0.0026 (0.0027)  loss_hw_unscaled: 0.0065 (0.0070)  loss_ce_0_unscaled: 0.0631 (0.0697)  loss_bbox_0_unscaled: 0.0093 (0.0098)  loss_giou_0_unscaled: 0.1977 (0.1981)  loss_xy_0_unscaled: 0.0026 (0.0028)  loss_hw_0_unscaled: 0.0069 (0.0070)  loss_ce_1_unscaled: 0.0481 (0.0600)  loss_bbox_1_unscaled: 0.0094 (0.0097)  loss_giou_1_unscaled: 0.1943 (0.1984)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0063 (0.0070)  loss_ce_2_unscaled: 0.0462 (0.0517)  loss_bbox_2_unscaled: 0.0095 (0.0100)  loss_giou_2_unscaled: 0.1933 (0.1992)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0069 (0.0072)  loss_ce_3_unscaled: 0.0498 (0.0480)  loss_bbox_3_unscaled: 0.0094 (0.0099)  loss_giou_3_unscaled: 0.1929 (0.1991)  loss_xy_3_unscaled: 0.0025 (0.0028)  loss_hw_3_unscaled: 0.0064 (0.0072)  loss_ce_4_unscaled: 0.0511 (0.0478)  loss_bbox_4_unscaled: 0.0095 (0.0099)  loss_giou_4_unscaled: 0.1930 (0.1977)  loss_xy_4_unscaled: 0.0025 (0.0028)  loss_hw_4_unscaled: 0.0064 (0.0072)  loss_ce_interm_unscaled: 0.0789 (0.0819)  loss_bbox_interm_unscaled: 0.0096 (0.0104)  loss_giou_interm_unscaled: 0.1955 (0.2023)  loss_xy_interm_unscaled: 0.0026 (0.0028)  loss_hw_interm_unscaled: 0.0072 (0.0076)  time: 2.7580  data: 0.0872  max mem: 8948\n",
            "Epoch: [10]  [20/90]  eta: 0:03:17  lr: 0.000001  loss: 3.6640 (3.9815)  loss_ce: 0.0784 (0.1004)  loss_bbox: 0.0485 (0.0501)  loss_giou: 0.3711 (0.3951)  loss_ce_0: 0.1188 (0.1346)  loss_bbox_0: 0.0467 (0.0503)  loss_giou_0: 0.3808 (0.3986)  loss_ce_1: 0.0961 (0.1192)  loss_bbox_1: 0.0470 (0.0499)  loss_giou_1: 0.3731 (0.3986)  loss_ce_2: 0.0881 (0.1059)  loss_bbox_2: 0.0480 (0.0505)  loss_giou_2: 0.3714 (0.3999)  loss_ce_3: 0.0763 (0.1007)  loss_bbox_3: 0.0483 (0.0505)  loss_giou_3: 0.3738 (0.3982)  loss_ce_4: 0.0766 (0.0977)  loss_bbox_4: 0.0484 (0.0506)  loss_giou_4: 0.3720 (0.3970)  loss_ce_interm: 0.1592 (0.1783)  loss_bbox_interm: 0.0497 (0.0519)  loss_giou_interm: 0.3871 (0.4036)  loss_ce_unscaled: 0.0392 (0.0502)  loss_bbox_unscaled: 0.0097 (0.0100)  loss_giou_unscaled: 0.1856 (0.1975)  loss_xy_unscaled: 0.0028 (0.0030)  loss_hw_unscaled: 0.0070 (0.0071)  loss_ce_0_unscaled: 0.0594 (0.0673)  loss_bbox_0_unscaled: 0.0093 (0.0101)  loss_giou_0_unscaled: 0.1904 (0.1993)  loss_xy_0_unscaled: 0.0027 (0.0030)  loss_hw_0_unscaled: 0.0070 (0.0071)  loss_ce_1_unscaled: 0.0481 (0.0596)  loss_bbox_1_unscaled: 0.0094 (0.0100)  loss_giou_1_unscaled: 0.1865 (0.1993)  loss_xy_1_unscaled: 0.0027 (0.0030)  loss_hw_1_unscaled: 0.0065 (0.0070)  loss_ce_2_unscaled: 0.0441 (0.0529)  loss_bbox_2_unscaled: 0.0096 (0.0101)  loss_giou_2_unscaled: 0.1857 (0.1999)  loss_xy_2_unscaled: 0.0027 (0.0030)  loss_hw_2_unscaled: 0.0070 (0.0071)  loss_ce_3_unscaled: 0.0382 (0.0504)  loss_bbox_3_unscaled: 0.0097 (0.0101)  loss_giou_3_unscaled: 0.1869 (0.1991)  loss_xy_3_unscaled: 0.0027 (0.0030)  loss_hw_3_unscaled: 0.0070 (0.0071)  loss_ce_4_unscaled: 0.0383 (0.0488)  loss_bbox_4_unscaled: 0.0097 (0.0101)  loss_giou_4_unscaled: 0.1860 (0.1985)  loss_xy_4_unscaled: 0.0027 (0.0030)  loss_hw_4_unscaled: 0.0070 (0.0071)  loss_ce_interm_unscaled: 0.0796 (0.0892)  loss_bbox_interm_unscaled: 0.0099 (0.0104)  loss_giou_interm_unscaled: 0.1935 (0.2018)  loss_xy_interm_unscaled: 0.0028 (0.0030)  loss_hw_interm_unscaled: 0.0071 (0.0074)  time: 2.7718  data: 0.0134  max mem: 8948\n",
            "Epoch: [10]  [30/90]  eta: 0:02:48  lr: 0.000001  loss: 4.0000 (4.0044)  loss_ce: 0.0784 (0.1016)  loss_bbox: 0.0491 (0.0500)  loss_giou: 0.3824 (0.3961)  loss_ce_0: 0.1175 (0.1381)  loss_bbox_0: 0.0493 (0.0505)  loss_giou_0: 0.3808 (0.3998)  loss_ce_1: 0.1007 (0.1209)  loss_bbox_1: 0.0491 (0.0504)  loss_giou_1: 0.3855 (0.3988)  loss_ce_2: 0.0923 (0.1096)  loss_bbox_2: 0.0491 (0.0504)  loss_giou_2: 0.3817 (0.3991)  loss_ce_3: 0.0829 (0.1040)  loss_bbox_3: 0.0492 (0.0503)  loss_giou_3: 0.3853 (0.3981)  loss_ce_4: 0.0796 (0.1004)  loss_bbox_4: 0.0491 (0.0503)  loss_giou_4: 0.3818 (0.3971)  loss_ce_interm: 0.1699 (0.1825)  loss_bbox_interm: 0.0511 (0.0522)  loss_giou_interm: 0.4113 (0.4042)  loss_ce_unscaled: 0.0392 (0.0508)  loss_bbox_unscaled: 0.0098 (0.0100)  loss_giou_unscaled: 0.1912 (0.1981)  loss_xy_unscaled: 0.0031 (0.0030)  loss_hw_unscaled: 0.0069 (0.0070)  loss_ce_0_unscaled: 0.0587 (0.0690)  loss_bbox_0_unscaled: 0.0099 (0.0101)  loss_giou_0_unscaled: 0.1904 (0.1999)  loss_xy_0_unscaled: 0.0031 (0.0030)  loss_hw_0_unscaled: 0.0071 (0.0071)  loss_ce_1_unscaled: 0.0504 (0.0605)  loss_bbox_1_unscaled: 0.0098 (0.0101)  loss_giou_1_unscaled: 0.1928 (0.1994)  loss_xy_1_unscaled: 0.0030 (0.0030)  loss_hw_1_unscaled: 0.0071 (0.0071)  loss_ce_2_unscaled: 0.0461 (0.0548)  loss_bbox_2_unscaled: 0.0098 (0.0101)  loss_giou_2_unscaled: 0.1909 (0.1995)  loss_xy_2_unscaled: 0.0030 (0.0030)  loss_hw_2_unscaled: 0.0070 (0.0071)  loss_ce_3_unscaled: 0.0414 (0.0520)  loss_bbox_3_unscaled: 0.0098 (0.0101)  loss_giou_3_unscaled: 0.1927 (0.1990)  loss_xy_3_unscaled: 0.0031 (0.0030)  loss_hw_3_unscaled: 0.0069 (0.0071)  loss_ce_4_unscaled: 0.0398 (0.0502)  loss_bbox_4_unscaled: 0.0098 (0.0101)  loss_giou_4_unscaled: 0.1909 (0.1986)  loss_xy_4_unscaled: 0.0031 (0.0030)  loss_hw_4_unscaled: 0.0069 (0.0070)  loss_ce_interm_unscaled: 0.0849 (0.0912)  loss_bbox_interm_unscaled: 0.0102 (0.0104)  loss_giou_interm_unscaled: 0.2056 (0.2021)  loss_xy_interm_unscaled: 0.0032 (0.0031)  loss_hw_interm_unscaled: 0.0073 (0.0074)  time: 2.8313  data: 0.0153  max mem: 8948\n",
            "Epoch: [10]  [40/90]  eta: 0:02:21  lr: 0.000001  loss: 3.9407 (3.9664)  loss_ce: 0.0779 (0.0990)  loss_bbox: 0.0472 (0.0497)  loss_giou: 0.3824 (0.3952)  loss_ce_0: 0.1175 (0.1321)  loss_bbox_0: 0.0493 (0.0502)  loss_giou_0: 0.3831 (0.3994)  loss_ce_1: 0.0942 (0.1160)  loss_bbox_1: 0.0472 (0.0501)  loss_giou_1: 0.3978 (0.3979)  loss_ce_2: 0.0923 (0.1055)  loss_bbox_2: 0.0481 (0.0501)  loss_giou_2: 0.3897 (0.3982)  loss_ce_3: 0.0829 (0.1009)  loss_bbox_3: 0.0478 (0.0499)  loss_giou_3: 0.3853 (0.3968)  loss_ce_4: 0.0796 (0.0983)  loss_bbox_4: 0.0472 (0.0500)  loss_giou_4: 0.3818 (0.3960)  loss_ce_interm: 0.1699 (0.1773)  loss_bbox_interm: 0.0489 (0.0515)  loss_giou_interm: 0.3918 (0.4025)  loss_ce_unscaled: 0.0389 (0.0495)  loss_bbox_unscaled: 0.0094 (0.0099)  loss_giou_unscaled: 0.1912 (0.1976)  loss_xy_unscaled: 0.0030 (0.0029)  loss_hw_unscaled: 0.0068 (0.0070)  loss_ce_0_unscaled: 0.0587 (0.0660)  loss_bbox_0_unscaled: 0.0099 (0.0100)  loss_giou_0_unscaled: 0.1915 (0.1997)  loss_xy_0_unscaled: 0.0030 (0.0030)  loss_hw_0_unscaled: 0.0070 (0.0071)  loss_ce_1_unscaled: 0.0471 (0.0580)  loss_bbox_1_unscaled: 0.0094 (0.0100)  loss_giou_1_unscaled: 0.1989 (0.1990)  loss_xy_1_unscaled: 0.0030 (0.0030)  loss_hw_1_unscaled: 0.0064 (0.0070)  loss_ce_2_unscaled: 0.0461 (0.0528)  loss_bbox_2_unscaled: 0.0096 (0.0100)  loss_giou_2_unscaled: 0.1949 (0.1991)  loss_xy_2_unscaled: 0.0029 (0.0030)  loss_hw_2_unscaled: 0.0067 (0.0070)  loss_ce_3_unscaled: 0.0414 (0.0505)  loss_bbox_3_unscaled: 0.0096 (0.0100)  loss_giou_3_unscaled: 0.1927 (0.1984)  loss_xy_3_unscaled: 0.0029 (0.0030)  loss_hw_3_unscaled: 0.0067 (0.0070)  loss_ce_4_unscaled: 0.0398 (0.0491)  loss_bbox_4_unscaled: 0.0094 (0.0100)  loss_giou_4_unscaled: 0.1909 (0.1980)  loss_xy_4_unscaled: 0.0030 (0.0030)  loss_hw_4_unscaled: 0.0067 (0.0070)  loss_ce_interm_unscaled: 0.0849 (0.0886)  loss_bbox_interm_unscaled: 0.0098 (0.0103)  loss_giou_interm_unscaled: 0.1959 (0.2012)  loss_xy_interm_unscaled: 0.0032 (0.0031)  loss_hw_interm_unscaled: 0.0070 (0.0073)  time: 2.8342  data: 0.0165  max mem: 8948\n",
            "Epoch: [10]  [50/90]  eta: 0:01:53  lr: 0.000001  loss: 3.6879 (3.9478)  loss_ce: 0.0727 (0.0959)  loss_bbox: 0.0478 (0.0498)  loss_giou: 0.3759 (0.3951)  loss_ce_0: 0.1078 (0.1301)  loss_bbox_0: 0.0496 (0.0503)  loss_giou_0: 0.3831 (0.3997)  loss_ce_1: 0.0858 (0.1131)  loss_bbox_1: 0.0491 (0.0502)  loss_giou_1: 0.3799 (0.3983)  loss_ce_2: 0.0750 (0.1036)  loss_bbox_2: 0.0490 (0.0501)  loss_giou_2: 0.3789 (0.3979)  loss_ce_3: 0.0750 (0.0983)  loss_bbox_3: 0.0486 (0.0499)  loss_giou_3: 0.3751 (0.3965)  loss_ce_4: 0.0733 (0.0957)  loss_bbox_4: 0.0478 (0.0500)  loss_giou_4: 0.3756 (0.3956)  loss_ce_interm: 0.1538 (0.1733)  loss_bbox_interm: 0.0489 (0.0516)  loss_giou_interm: 0.3800 (0.4027)  loss_ce_unscaled: 0.0363 (0.0479)  loss_bbox_unscaled: 0.0096 (0.0100)  loss_giou_unscaled: 0.1880 (0.1975)  loss_xy_unscaled: 0.0027 (0.0029)  loss_hw_unscaled: 0.0069 (0.0070)  loss_ce_0_unscaled: 0.0539 (0.0651)  loss_bbox_0_unscaled: 0.0099 (0.0101)  loss_giou_0_unscaled: 0.1915 (0.1999)  loss_xy_0_unscaled: 0.0027 (0.0030)  loss_hw_0_unscaled: 0.0070 (0.0071)  loss_ce_1_unscaled: 0.0429 (0.0565)  loss_bbox_1_unscaled: 0.0098 (0.0100)  loss_giou_1_unscaled: 0.1900 (0.1991)  loss_xy_1_unscaled: 0.0027 (0.0029)  loss_hw_1_unscaled: 0.0071 (0.0071)  loss_ce_2_unscaled: 0.0375 (0.0518)  loss_bbox_2_unscaled: 0.0098 (0.0100)  loss_giou_2_unscaled: 0.1894 (0.1990)  loss_xy_2_unscaled: 0.0027 (0.0029)  loss_hw_2_unscaled: 0.0069 (0.0071)  loss_ce_3_unscaled: 0.0375 (0.0491)  loss_bbox_3_unscaled: 0.0097 (0.0100)  loss_giou_3_unscaled: 0.1875 (0.1983)  loss_xy_3_unscaled: 0.0027 (0.0029)  loss_hw_3_unscaled: 0.0070 (0.0070)  loss_ce_4_unscaled: 0.0366 (0.0478)  loss_bbox_4_unscaled: 0.0096 (0.0100)  loss_giou_4_unscaled: 0.1878 (0.1978)  loss_xy_4_unscaled: 0.0027 (0.0030)  loss_hw_4_unscaled: 0.0069 (0.0071)  loss_ce_interm_unscaled: 0.0769 (0.0867)  loss_bbox_interm_unscaled: 0.0098 (0.0103)  loss_giou_interm_unscaled: 0.1900 (0.2014)  loss_xy_interm_unscaled: 0.0028 (0.0030)  loss_hw_interm_unscaled: 0.0070 (0.0073)  time: 2.8937  data: 0.0166  max mem: 8948\n",
            "Epoch: [10]  [60/90]  eta: 0:01:26  lr: 0.000001  loss: 3.6936 (3.8987)  loss_ce: 0.0717 (0.0926)  loss_bbox: 0.0464 (0.0491)  loss_giou: 0.3799 (0.3932)  loss_ce_0: 0.1012 (0.1253)  loss_bbox_0: 0.0473 (0.0496)  loss_giou_0: 0.3912 (0.3980)  loss_ce_1: 0.0814 (0.1084)  loss_bbox_1: 0.0474 (0.0494)  loss_giou_1: 0.3916 (0.3963)  loss_ce_2: 0.0750 (0.0996)  loss_bbox_2: 0.0465 (0.0494)  loss_giou_2: 0.3831 (0.3956)  loss_ce_3: 0.0750 (0.0947)  loss_bbox_3: 0.0465 (0.0492)  loss_giou_3: 0.3810 (0.3944)  loss_ce_4: 0.0733 (0.0923)  loss_bbox_4: 0.0465 (0.0493)  loss_giou_4: 0.3784 (0.3935)  loss_ce_interm: 0.1360 (0.1676)  loss_bbox_interm: 0.0471 (0.0508)  loss_giou_interm: 0.3899 (0.4004)  loss_ce_unscaled: 0.0359 (0.0463)  loss_bbox_unscaled: 0.0093 (0.0098)  loss_giou_unscaled: 0.1899 (0.1966)  loss_xy_unscaled: 0.0028 (0.0029)  loss_hw_unscaled: 0.0066 (0.0069)  loss_ce_0_unscaled: 0.0506 (0.0627)  loss_bbox_0_unscaled: 0.0095 (0.0099)  loss_giou_0_unscaled: 0.1956 (0.1990)  loss_xy_0_unscaled: 0.0027 (0.0029)  loss_hw_0_unscaled: 0.0067 (0.0070)  loss_ce_1_unscaled: 0.0407 (0.0542)  loss_bbox_1_unscaled: 0.0095 (0.0099)  loss_giou_1_unscaled: 0.1958 (0.1981)  loss_xy_1_unscaled: 0.0027 (0.0029)  loss_hw_1_unscaled: 0.0068 (0.0070)  loss_ce_2_unscaled: 0.0375 (0.0498)  loss_bbox_2_unscaled: 0.0093 (0.0099)  loss_giou_2_unscaled: 0.1915 (0.1978)  loss_xy_2_unscaled: 0.0027 (0.0029)  loss_hw_2_unscaled: 0.0066 (0.0070)  loss_ce_3_unscaled: 0.0375 (0.0474)  loss_bbox_3_unscaled: 0.0093 (0.0098)  loss_giou_3_unscaled: 0.1905 (0.1972)  loss_xy_3_unscaled: 0.0028 (0.0029)  loss_hw_3_unscaled: 0.0066 (0.0069)  loss_ce_4_unscaled: 0.0366 (0.0461)  loss_bbox_4_unscaled: 0.0093 (0.0099)  loss_giou_4_unscaled: 0.1892 (0.1967)  loss_xy_4_unscaled: 0.0027 (0.0029)  loss_hw_4_unscaled: 0.0066 (0.0069)  loss_ce_interm_unscaled: 0.0680 (0.0838)  loss_bbox_interm_unscaled: 0.0094 (0.0102)  loss_giou_interm_unscaled: 0.1950 (0.2002)  loss_xy_interm_unscaled: 0.0028 (0.0030)  loss_hw_interm_unscaled: 0.0067 (0.0072)  time: 3.0025  data: 0.0177  max mem: 8948\n",
            "Epoch: [10]  [70/90]  eta: 0:00:57  lr: 0.000001  loss: 3.7145 (3.8996)  loss_ce: 0.0657 (0.0917)  loss_bbox: 0.0458 (0.0487)  loss_giou: 0.3867 (0.3938)  loss_ce_0: 0.1107 (0.1257)  loss_bbox_0: 0.0454 (0.0491)  loss_giou_0: 0.3912 (0.3983)  loss_ce_1: 0.0864 (0.1093)  loss_bbox_1: 0.0452 (0.0489)  loss_giou_1: 0.3916 (0.3968)  loss_ce_2: 0.0790 (0.1009)  loss_bbox_2: 0.0448 (0.0489)  loss_giou_2: 0.3879 (0.3958)  loss_ce_3: 0.0742 (0.0947)  loss_bbox_3: 0.0449 (0.0488)  loss_giou_3: 0.3849 (0.3948)  loss_ce_4: 0.0690 (0.0921)  loss_bbox_4: 0.0456 (0.0488)  loss_giou_4: 0.3858 (0.3939)  loss_ce_interm: 0.1424 (0.1680)  loss_bbox_interm: 0.0452 (0.0502)  loss_giou_interm: 0.3902 (0.4005)  loss_ce_unscaled: 0.0329 (0.0458)  loss_bbox_unscaled: 0.0092 (0.0097)  loss_giou_unscaled: 0.1933 (0.1969)  loss_xy_unscaled: 0.0028 (0.0029)  loss_hw_unscaled: 0.0063 (0.0069)  loss_ce_0_unscaled: 0.0553 (0.0629)  loss_bbox_0_unscaled: 0.0091 (0.0098)  loss_giou_0_unscaled: 0.1956 (0.1992)  loss_xy_0_unscaled: 0.0028 (0.0029)  loss_hw_0_unscaled: 0.0063 (0.0069)  loss_ce_1_unscaled: 0.0432 (0.0546)  loss_bbox_1_unscaled: 0.0090 (0.0098)  loss_giou_1_unscaled: 0.1958 (0.1984)  loss_xy_1_unscaled: 0.0027 (0.0029)  loss_hw_1_unscaled: 0.0063 (0.0069)  loss_ce_2_unscaled: 0.0395 (0.0505)  loss_bbox_2_unscaled: 0.0090 (0.0098)  loss_giou_2_unscaled: 0.1939 (0.1979)  loss_xy_2_unscaled: 0.0027 (0.0029)  loss_hw_2_unscaled: 0.0063 (0.0069)  loss_ce_3_unscaled: 0.0371 (0.0474)  loss_bbox_3_unscaled: 0.0090 (0.0098)  loss_giou_3_unscaled: 0.1924 (0.1974)  loss_xy_3_unscaled: 0.0028 (0.0029)  loss_hw_3_unscaled: 0.0063 (0.0069)  loss_ce_4_unscaled: 0.0345 (0.0461)  loss_bbox_4_unscaled: 0.0091 (0.0098)  loss_giou_4_unscaled: 0.1929 (0.1970)  loss_xy_4_unscaled: 0.0027 (0.0029)  loss_hw_4_unscaled: 0.0063 (0.0069)  loss_ce_interm_unscaled: 0.0712 (0.0840)  loss_bbox_interm_unscaled: 0.0090 (0.0100)  loss_giou_interm_unscaled: 0.1951 (0.2002)  loss_xy_interm_unscaled: 0.0027 (0.0030)  loss_hw_interm_unscaled: 0.0062 (0.0071)  time: 2.9290  data: 0.0166  max mem: 8948\n",
            "Epoch: [10]  [80/90]  eta: 0:00:28  lr: 0.000001  loss: 3.7355 (3.8851)  loss_ce: 0.0655 (0.0904)  loss_bbox: 0.0457 (0.0483)  loss_giou: 0.3783 (0.3932)  loss_ce_0: 0.1107 (0.1253)  loss_bbox_0: 0.0454 (0.0487)  loss_giou_0: 0.3831 (0.3969)  loss_ce_1: 0.0864 (0.1084)  loss_bbox_1: 0.0452 (0.0486)  loss_giou_1: 0.3856 (0.3961)  loss_ce_2: 0.0790 (0.0993)  loss_bbox_2: 0.0447 (0.0485)  loss_giou_2: 0.3879 (0.3952)  loss_ce_3: 0.0742 (0.0939)  loss_bbox_3: 0.0448 (0.0484)  loss_giou_3: 0.3809 (0.3942)  loss_ce_4: 0.0683 (0.0913)  loss_bbox_4: 0.0456 (0.0484)  loss_giou_4: 0.3781 (0.3933)  loss_ce_interm: 0.1547 (0.1676)  loss_bbox_interm: 0.0456 (0.0498)  loss_giou_interm: 0.3861 (0.3996)  loss_ce_unscaled: 0.0327 (0.0452)  loss_bbox_unscaled: 0.0091 (0.0097)  loss_giou_unscaled: 0.1891 (0.1966)  loss_xy_unscaled: 0.0028 (0.0029)  loss_hw_unscaled: 0.0065 (0.0068)  loss_ce_0_unscaled: 0.0553 (0.0626)  loss_bbox_0_unscaled: 0.0091 (0.0097)  loss_giou_0_unscaled: 0.1915 (0.1984)  loss_xy_0_unscaled: 0.0028 (0.0029)  loss_hw_0_unscaled: 0.0063 (0.0068)  loss_ce_1_unscaled: 0.0432 (0.0542)  loss_bbox_1_unscaled: 0.0090 (0.0097)  loss_giou_1_unscaled: 0.1928 (0.1980)  loss_xy_1_unscaled: 0.0027 (0.0029)  loss_hw_1_unscaled: 0.0063 (0.0068)  loss_ce_2_unscaled: 0.0395 (0.0497)  loss_bbox_2_unscaled: 0.0089 (0.0097)  loss_giou_2_unscaled: 0.1939 (0.1976)  loss_xy_2_unscaled: 0.0028 (0.0029)  loss_hw_2_unscaled: 0.0064 (0.0068)  loss_ce_3_unscaled: 0.0371 (0.0469)  loss_bbox_3_unscaled: 0.0090 (0.0097)  loss_giou_3_unscaled: 0.1905 (0.1971)  loss_xy_3_unscaled: 0.0028 (0.0029)  loss_hw_3_unscaled: 0.0064 (0.0068)  loss_ce_4_unscaled: 0.0341 (0.0456)  loss_bbox_4_unscaled: 0.0091 (0.0097)  loss_giou_4_unscaled: 0.1890 (0.1966)  loss_xy_4_unscaled: 0.0028 (0.0029)  loss_hw_4_unscaled: 0.0065 (0.0068)  loss_ce_interm_unscaled: 0.0774 (0.0838)  loss_bbox_interm_unscaled: 0.0091 (0.0100)  loss_giou_interm_unscaled: 0.1930 (0.1998)  loss_xy_interm_unscaled: 0.0027 (0.0030)  loss_hw_interm_unscaled: 0.0062 (0.0070)  time: 2.7674  data: 0.0147  max mem: 8948\n",
            "Epoch: [10]  [89/90]  eta: 0:00:02  lr: 0.000001  loss: 3.7553 (3.8676)  loss_ce: 0.0723 (0.0885)  loss_bbox: 0.0460 (0.0485)  loss_giou: 0.3862 (0.3923)  loss_ce_0: 0.1078 (0.1232)  loss_bbox_0: 0.0468 (0.0489)  loss_giou_0: 0.3831 (0.3962)  loss_ce_1: 0.0909 (0.1070)  loss_bbox_1: 0.0463 (0.0486)  loss_giou_1: 0.3868 (0.3948)  loss_ce_2: 0.0825 (0.0971)  loss_bbox_2: 0.0458 (0.0486)  loss_giou_2: 0.3857 (0.3941)  loss_ce_3: 0.0800 (0.0924)  loss_bbox_3: 0.0463 (0.0485)  loss_giou_3: 0.3858 (0.3931)  loss_ce_4: 0.0771 (0.0897)  loss_bbox_4: 0.0461 (0.0485)  loss_giou_4: 0.3860 (0.3924)  loss_ce_interm: 0.1557 (0.1665)  loss_bbox_interm: 0.0479 (0.0500)  loss_giou_interm: 0.3934 (0.3988)  loss_ce_unscaled: 0.0362 (0.0442)  loss_bbox_unscaled: 0.0092 (0.0097)  loss_giou_unscaled: 0.1931 (0.1962)  loss_xy_unscaled: 0.0028 (0.0029)  loss_hw_unscaled: 0.0069 (0.0068)  loss_ce_0_unscaled: 0.0539 (0.0616)  loss_bbox_0_unscaled: 0.0094 (0.0098)  loss_giou_0_unscaled: 0.1915 (0.1981)  loss_xy_0_unscaled: 0.0028 (0.0029)  loss_hw_0_unscaled: 0.0069 (0.0069)  loss_ce_1_unscaled: 0.0455 (0.0535)  loss_bbox_1_unscaled: 0.0093 (0.0097)  loss_giou_1_unscaled: 0.1934 (0.1974)  loss_xy_1_unscaled: 0.0026 (0.0029)  loss_hw_1_unscaled: 0.0070 (0.0069)  loss_ce_2_unscaled: 0.0412 (0.0486)  loss_bbox_2_unscaled: 0.0092 (0.0097)  loss_giou_2_unscaled: 0.1929 (0.1971)  loss_xy_2_unscaled: 0.0028 (0.0029)  loss_hw_2_unscaled: 0.0070 (0.0069)  loss_ce_3_unscaled: 0.0400 (0.0462)  loss_bbox_3_unscaled: 0.0093 (0.0097)  loss_giou_3_unscaled: 0.1929 (0.1966)  loss_xy_3_unscaled: 0.0027 (0.0029)  loss_hw_3_unscaled: 0.0070 (0.0068)  loss_ce_4_unscaled: 0.0386 (0.0449)  loss_bbox_4_unscaled: 0.0092 (0.0097)  loss_giou_4_unscaled: 0.1930 (0.1962)  loss_xy_4_unscaled: 0.0026 (0.0029)  loss_hw_4_unscaled: 0.0069 (0.0068)  loss_ce_interm_unscaled: 0.0778 (0.0832)  loss_bbox_interm_unscaled: 0.0096 (0.0100)  loss_giou_interm_unscaled: 0.1967 (0.1994)  loss_xy_interm_unscaled: 0.0028 (0.0030)  loss_hw_interm_unscaled: 0.0070 (0.0070)  time: 2.8464  data: 0.0150  max mem: 8948\n",
            "Epoch: [10] Total time: 0:04:17 (2.8587 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 3.7553 (3.8676)  loss_ce: 0.0723 (0.0885)  loss_bbox: 0.0460 (0.0485)  loss_giou: 0.3862 (0.3923)  loss_ce_0: 0.1078 (0.1232)  loss_bbox_0: 0.0468 (0.0489)  loss_giou_0: 0.3831 (0.3962)  loss_ce_1: 0.0909 (0.1070)  loss_bbox_1: 0.0463 (0.0486)  loss_giou_1: 0.3868 (0.3948)  loss_ce_2: 0.0825 (0.0971)  loss_bbox_2: 0.0458 (0.0486)  loss_giou_2: 0.3857 (0.3941)  loss_ce_3: 0.0800 (0.0924)  loss_bbox_3: 0.0463 (0.0485)  loss_giou_3: 0.3858 (0.3931)  loss_ce_4: 0.0771 (0.0897)  loss_bbox_4: 0.0461 (0.0485)  loss_giou_4: 0.3860 (0.3924)  loss_ce_interm: 0.1557 (0.1665)  loss_bbox_interm: 0.0479 (0.0500)  loss_giou_interm: 0.3934 (0.3988)  loss_ce_unscaled: 0.0362 (0.0442)  loss_bbox_unscaled: 0.0092 (0.0097)  loss_giou_unscaled: 0.1931 (0.1962)  loss_xy_unscaled: 0.0028 (0.0029)  loss_hw_unscaled: 0.0069 (0.0068)  loss_ce_0_unscaled: 0.0539 (0.0616)  loss_bbox_0_unscaled: 0.0094 (0.0098)  loss_giou_0_unscaled: 0.1915 (0.1981)  loss_xy_0_unscaled: 0.0028 (0.0029)  loss_hw_0_unscaled: 0.0069 (0.0069)  loss_ce_1_unscaled: 0.0455 (0.0535)  loss_bbox_1_unscaled: 0.0093 (0.0097)  loss_giou_1_unscaled: 0.1934 (0.1974)  loss_xy_1_unscaled: 0.0026 (0.0029)  loss_hw_1_unscaled: 0.0070 (0.0069)  loss_ce_2_unscaled: 0.0412 (0.0486)  loss_bbox_2_unscaled: 0.0092 (0.0097)  loss_giou_2_unscaled: 0.1929 (0.1971)  loss_xy_2_unscaled: 0.0028 (0.0029)  loss_hw_2_unscaled: 0.0070 (0.0069)  loss_ce_3_unscaled: 0.0400 (0.0462)  loss_bbox_3_unscaled: 0.0093 (0.0097)  loss_giou_3_unscaled: 0.1929 (0.1966)  loss_xy_3_unscaled: 0.0027 (0.0029)  loss_hw_3_unscaled: 0.0070 (0.0068)  loss_ce_4_unscaled: 0.0386 (0.0449)  loss_bbox_4_unscaled: 0.0092 (0.0097)  loss_giou_4_unscaled: 0.1930 (0.1962)  loss_xy_4_unscaled: 0.0026 (0.0029)  loss_hw_4_unscaled: 0.0069 (0.0068)  loss_ce_interm_unscaled: 0.0778 (0.0832)  loss_bbox_interm_unscaled: 0.0096 (0.0100)  loss_giou_interm_unscaled: 0.1967 (0.1994)  loss_xy_interm_unscaled: 0.0028 (0.0030)  loss_hw_interm_unscaled: 0.0070 (0.0070)\n",
            "Input text prompt: bolt . wrong direction . 1 . 2 . 3 . 4 . 5 .\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Test:  [ 0/10]  eta: 0:00:18    time: 1.8223  data: 0.8972  max mem: 8948\n",
            "Test:  [ 9/10]  eta: 0:00:01    time: 1.0252  data: 0.1035  max mem: 8948\n",
            "Test: Total time: 0:00:10 (1.0345 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.056\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.117\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.042\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.054\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.298\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.232\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.006\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.247\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.491\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.465\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.593\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.441\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch: [11]  [ 0/90]  eta: 0:05:10  lr: 0.000001  loss: 3.6079 (3.6079)  loss_ce: 0.0539 (0.0539)  loss_bbox: 0.0446 (0.0446)  loss_giou: 0.3926 (0.3926)  loss_ce_0: 0.0940 (0.0940)  loss_bbox_0: 0.0466 (0.0466)  loss_giou_0: 0.3911 (0.3911)  loss_ce_1: 0.0783 (0.0783)  loss_bbox_1: 0.0434 (0.0434)  loss_giou_1: 0.3995 (0.3995)  loss_ce_2: 0.0731 (0.0731)  loss_bbox_2: 0.0438 (0.0438)  loss_giou_2: 0.3909 (0.3909)  loss_ce_3: 0.0480 (0.0480)  loss_bbox_3: 0.0458 (0.0458)  loss_giou_3: 0.4061 (0.4061)  loss_ce_4: 0.0573 (0.0573)  loss_bbox_4: 0.0443 (0.0443)  loss_giou_4: 0.3947 (0.3947)  loss_ce_interm: 0.1019 (0.1019)  loss_bbox_interm: 0.0492 (0.0492)  loss_giou_interm: 0.4087 (0.4087)  loss_ce_unscaled: 0.0270 (0.0270)  loss_bbox_unscaled: 0.0089 (0.0089)  loss_giou_unscaled: 0.1963 (0.1963)  loss_xy_unscaled: 0.0027 (0.0027)  loss_hw_unscaled: 0.0062 (0.0062)  loss_ce_0_unscaled: 0.0470 (0.0470)  loss_bbox_0_unscaled: 0.0093 (0.0093)  loss_giou_0_unscaled: 0.1955 (0.1955)  loss_xy_0_unscaled: 0.0030 (0.0030)  loss_hw_0_unscaled: 0.0063 (0.0063)  loss_ce_1_unscaled: 0.0392 (0.0392)  loss_bbox_1_unscaled: 0.0087 (0.0087)  loss_giou_1_unscaled: 0.1998 (0.1998)  loss_xy_1_unscaled: 0.0027 (0.0027)  loss_hw_1_unscaled: 0.0060 (0.0060)  loss_ce_2_unscaled: 0.0366 (0.0366)  loss_bbox_2_unscaled: 0.0088 (0.0088)  loss_giou_2_unscaled: 0.1955 (0.1955)  loss_xy_2_unscaled: 0.0027 (0.0027)  loss_hw_2_unscaled: 0.0061 (0.0061)  loss_ce_3_unscaled: 0.0240 (0.0240)  loss_bbox_3_unscaled: 0.0092 (0.0092)  loss_giou_3_unscaled: 0.2030 (0.2030)  loss_xy_3_unscaled: 0.0027 (0.0027)  loss_hw_3_unscaled: 0.0064 (0.0064)  loss_ce_4_unscaled: 0.0287 (0.0287)  loss_bbox_4_unscaled: 0.0089 (0.0089)  loss_giou_4_unscaled: 0.1974 (0.1974)  loss_xy_4_unscaled: 0.0027 (0.0027)  loss_hw_4_unscaled: 0.0061 (0.0061)  loss_ce_interm_unscaled: 0.0510 (0.0510)  loss_bbox_interm_unscaled: 0.0098 (0.0098)  loss_giou_interm_unscaled: 0.2044 (0.2044)  loss_xy_interm_unscaled: 0.0031 (0.0031)  loss_hw_interm_unscaled: 0.0068 (0.0068)  time: 3.4481  data: 0.6885  max mem: 8948\n",
            "Epoch: [11]  [10/90]  eta: 0:03:58  lr: 0.000001  loss: 3.6388 (3.6708)  loss_ce: 0.0800 (0.0847)  loss_bbox: 0.0446 (0.0478)  loss_giou: 0.3871 (0.3745)  loss_ce_0: 0.1114 (0.1117)  loss_bbox_0: 0.0466 (0.0480)  loss_giou_0: 0.3911 (0.3763)  loss_ce_1: 0.0938 (0.0937)  loss_bbox_1: 0.0434 (0.0477)  loss_giou_1: 0.3906 (0.3767)  loss_ce_2: 0.0839 (0.0889)  loss_bbox_2: 0.0438 (0.0474)  loss_giou_2: 0.3909 (0.3738)  loss_ce_3: 0.0838 (0.0851)  loss_bbox_3: 0.0458 (0.0478)  loss_giou_3: 0.3899 (0.3759)  loss_ce_4: 0.0844 (0.0851)  loss_bbox_4: 0.0443 (0.0477)  loss_giou_4: 0.3882 (0.3760)  loss_ce_interm: 0.1552 (0.1498)  loss_bbox_interm: 0.0492 (0.0494)  loss_giou_interm: 0.3899 (0.3829)  loss_ce_unscaled: 0.0400 (0.0424)  loss_bbox_unscaled: 0.0089 (0.0096)  loss_giou_unscaled: 0.1936 (0.1873)  loss_xy_unscaled: 0.0027 (0.0027)  loss_hw_unscaled: 0.0062 (0.0069)  loss_ce_0_unscaled: 0.0557 (0.0559)  loss_bbox_0_unscaled: 0.0093 (0.0096)  loss_giou_0_unscaled: 0.1955 (0.1881)  loss_xy_0_unscaled: 0.0027 (0.0027)  loss_hw_0_unscaled: 0.0063 (0.0069)  loss_ce_1_unscaled: 0.0469 (0.0469)  loss_bbox_1_unscaled: 0.0087 (0.0095)  loss_giou_1_unscaled: 0.1953 (0.1883)  loss_xy_1_unscaled: 0.0027 (0.0026)  loss_hw_1_unscaled: 0.0062 (0.0069)  loss_ce_2_unscaled: 0.0419 (0.0444)  loss_bbox_2_unscaled: 0.0088 (0.0095)  loss_giou_2_unscaled: 0.1955 (0.1869)  loss_xy_2_unscaled: 0.0027 (0.0026)  loss_hw_2_unscaled: 0.0061 (0.0068)  loss_ce_3_unscaled: 0.0419 (0.0426)  loss_bbox_3_unscaled: 0.0092 (0.0096)  loss_giou_3_unscaled: 0.1949 (0.1879)  loss_xy_3_unscaled: 0.0027 (0.0027)  loss_hw_3_unscaled: 0.0063 (0.0069)  loss_ce_4_unscaled: 0.0422 (0.0425)  loss_bbox_4_unscaled: 0.0089 (0.0095)  loss_giou_4_unscaled: 0.1941 (0.1880)  loss_xy_4_unscaled: 0.0027 (0.0027)  loss_hw_4_unscaled: 0.0061 (0.0069)  loss_ce_interm_unscaled: 0.0776 (0.0749)  loss_bbox_interm_unscaled: 0.0098 (0.0099)  loss_giou_interm_unscaled: 0.1950 (0.1914)  loss_xy_interm_unscaled: 0.0027 (0.0028)  loss_hw_interm_unscaled: 0.0068 (0.0071)  time: 2.9863  data: 0.0789  max mem: 8948\n",
            "Epoch: [11]  [20/90]  eta: 0:03:32  lr: 0.000001  loss: 3.6388 (3.7375)  loss_ce: 0.0800 (0.0926)  loss_bbox: 0.0471 (0.0480)  loss_giou: 0.3690 (0.3742)  loss_ce_0: 0.1173 (0.1186)  loss_bbox_0: 0.0470 (0.0485)  loss_giou_0: 0.3720 (0.3780)  loss_ce_1: 0.0994 (0.1052)  loss_bbox_1: 0.0473 (0.0481)  loss_giou_1: 0.3721 (0.3769)  loss_ce_2: 0.0880 (0.0994)  loss_bbox_2: 0.0465 (0.0478)  loss_giou_2: 0.3703 (0.3754)  loss_ce_3: 0.0838 (0.0931)  loss_bbox_3: 0.0458 (0.0480)  loss_giou_3: 0.3788 (0.3755)  loss_ce_4: 0.0844 (0.0927)  loss_bbox_4: 0.0471 (0.0479)  loss_giou_4: 0.3810 (0.3757)  loss_ce_interm: 0.1552 (0.1612)  loss_bbox_interm: 0.0484 (0.0497)  loss_giou_interm: 0.3821 (0.3810)  loss_ce_unscaled: 0.0400 (0.0463)  loss_bbox_unscaled: 0.0094 (0.0096)  loss_giou_unscaled: 0.1845 (0.1871)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0064 (0.0068)  loss_ce_0_unscaled: 0.0586 (0.0593)  loss_bbox_0_unscaled: 0.0094 (0.0097)  loss_giou_0_unscaled: 0.1860 (0.1890)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0064 (0.0069)  loss_ce_1_unscaled: 0.0497 (0.0526)  loss_bbox_1_unscaled: 0.0095 (0.0096)  loss_giou_1_unscaled: 0.1861 (0.1884)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0064 (0.0068)  loss_ce_2_unscaled: 0.0440 (0.0497)  loss_bbox_2_unscaled: 0.0093 (0.0096)  loss_giou_2_unscaled: 0.1852 (0.1877)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0064 (0.0068)  loss_ce_3_unscaled: 0.0419 (0.0466)  loss_bbox_3_unscaled: 0.0092 (0.0096)  loss_giou_3_unscaled: 0.1894 (0.1878)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0063 (0.0068)  loss_ce_4_unscaled: 0.0422 (0.0464)  loss_bbox_4_unscaled: 0.0094 (0.0096)  loss_giou_4_unscaled: 0.1905 (0.1878)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0064 (0.0068)  loss_ce_interm_unscaled: 0.0776 (0.0806)  loss_bbox_interm_unscaled: 0.0097 (0.0099)  loss_giou_interm_unscaled: 0.1911 (0.1905)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0066 (0.0071)  time: 3.0184  data: 0.0167  max mem: 8948\n",
            "Epoch: [11]  [30/90]  eta: 0:03:00  lr: 0.000001  loss: 3.6226 (3.7141)  loss_ce: 0.0743 (0.0947)  loss_bbox: 0.0469 (0.0466)  loss_giou: 0.3638 (0.3692)  loss_ce_0: 0.1117 (0.1205)  loss_bbox_0: 0.0461 (0.0471)  loss_giou_0: 0.3647 (0.3749)  loss_ce_1: 0.0921 (0.1078)  loss_bbox_1: 0.0464 (0.0469)  loss_giou_1: 0.3640 (0.3733)  loss_ce_2: 0.0888 (0.1013)  loss_bbox_2: 0.0465 (0.0466)  loss_giou_2: 0.3646 (0.3716)  loss_ce_3: 0.0785 (0.0976)  loss_bbox_3: 0.0456 (0.0468)  loss_giou_3: 0.3660 (0.3704)  loss_ce_4: 0.0771 (0.0954)  loss_bbox_4: 0.0471 (0.0466)  loss_giou_4: 0.3642 (0.3705)  loss_ce_interm: 0.1529 (0.1619)  loss_bbox_interm: 0.0462 (0.0482)  loss_giou_interm: 0.3698 (0.3762)  loss_ce_unscaled: 0.0371 (0.0473)  loss_bbox_unscaled: 0.0094 (0.0093)  loss_giou_unscaled: 0.1819 (0.1846)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0060 (0.0066)  loss_ce_0_unscaled: 0.0558 (0.0603)  loss_bbox_0_unscaled: 0.0092 (0.0094)  loss_giou_0_unscaled: 0.1824 (0.1874)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0060 (0.0066)  loss_ce_1_unscaled: 0.0461 (0.0539)  loss_bbox_1_unscaled: 0.0093 (0.0094)  loss_giou_1_unscaled: 0.1820 (0.1866)  loss_xy_1_unscaled: 0.0028 (0.0027)  loss_hw_1_unscaled: 0.0064 (0.0066)  loss_ce_2_unscaled: 0.0444 (0.0506)  loss_bbox_2_unscaled: 0.0093 (0.0093)  loss_giou_2_unscaled: 0.1823 (0.1858)  loss_xy_2_unscaled: 0.0028 (0.0027)  loss_hw_2_unscaled: 0.0064 (0.0066)  loss_ce_3_unscaled: 0.0392 (0.0488)  loss_bbox_3_unscaled: 0.0091 (0.0094)  loss_giou_3_unscaled: 0.1830 (0.1852)  loss_xy_3_unscaled: 0.0028 (0.0027)  loss_hw_3_unscaled: 0.0063 (0.0066)  loss_ce_4_unscaled: 0.0385 (0.0477)  loss_bbox_4_unscaled: 0.0094 (0.0093)  loss_giou_4_unscaled: 0.1821 (0.1852)  loss_xy_4_unscaled: 0.0027 (0.0027)  loss_hw_4_unscaled: 0.0064 (0.0066)  loss_ce_interm_unscaled: 0.0765 (0.0810)  loss_bbox_interm_unscaled: 0.0092 (0.0096)  loss_giou_interm_unscaled: 0.1849 (0.1881)  loss_xy_interm_unscaled: 0.0028 (0.0028)  loss_hw_interm_unscaled: 0.0064 (0.0068)  time: 3.0089  data: 0.0162  max mem: 8948\n",
            "Epoch: [11]  [40/90]  eta: 0:02:28  lr: 0.000001  loss: 3.5324 (3.7789)  loss_ce: 0.0743 (0.0984)  loss_bbox: 0.0469 (0.0477)  loss_giou: 0.3638 (0.3728)  loss_ce_0: 0.1116 (0.1261)  loss_bbox_0: 0.0463 (0.0478)  loss_giou_0: 0.3638 (0.3782)  loss_ce_1: 0.0826 (0.1124)  loss_bbox_1: 0.0449 (0.0479)  loss_giou_1: 0.3640 (0.3772)  loss_ce_2: 0.0866 (0.1061)  loss_bbox_2: 0.0471 (0.0477)  loss_giou_2: 0.3646 (0.3751)  loss_ce_3: 0.0799 (0.0996)  loss_bbox_3: 0.0459 (0.0476)  loss_giou_3: 0.3648 (0.3751)  loss_ce_4: 0.0796 (0.0989)  loss_bbox_4: 0.0473 (0.0476)  loss_giou_4: 0.3642 (0.3746)  loss_ce_interm: 0.1545 (0.1674)  loss_bbox_interm: 0.0450 (0.0493)  loss_giou_interm: 0.3680 (0.3814)  loss_ce_unscaled: 0.0371 (0.0492)  loss_bbox_unscaled: 0.0094 (0.0095)  loss_giou_unscaled: 0.1819 (0.1864)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0061 (0.0067)  loss_ce_0_unscaled: 0.0558 (0.0631)  loss_bbox_0_unscaled: 0.0093 (0.0096)  loss_giou_0_unscaled: 0.1819 (0.1891)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0061 (0.0067)  loss_ce_1_unscaled: 0.0413 (0.0562)  loss_bbox_1_unscaled: 0.0090 (0.0096)  loss_giou_1_unscaled: 0.1820 (0.1886)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0061 (0.0068)  loss_ce_2_unscaled: 0.0433 (0.0531)  loss_bbox_2_unscaled: 0.0094 (0.0095)  loss_giou_2_unscaled: 0.1823 (0.1875)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0064 (0.0067)  loss_ce_3_unscaled: 0.0400 (0.0498)  loss_bbox_3_unscaled: 0.0092 (0.0095)  loss_giou_3_unscaled: 0.1824 (0.1875)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0064 (0.0067)  loss_ce_4_unscaled: 0.0398 (0.0494)  loss_bbox_4_unscaled: 0.0095 (0.0095)  loss_giou_4_unscaled: 0.1821 (0.1873)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0064 (0.0067)  loss_ce_interm_unscaled: 0.0773 (0.0837)  loss_bbox_interm_unscaled: 0.0090 (0.0099)  loss_giou_interm_unscaled: 0.1840 (0.1907)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0061 (0.0070)  time: 2.9173  data: 0.0164  max mem: 8948\n",
            "Epoch: [11]  [50/90]  eta: 0:01:57  lr: 0.000001  loss: 3.7524 (3.8107)  loss_ce: 0.0672 (0.0944)  loss_bbox: 0.0476 (0.0479)  loss_giou: 0.3822 (0.3803)  loss_ce_0: 0.0997 (0.1241)  loss_bbox_0: 0.0479 (0.0481)  loss_giou_0: 0.3796 (0.3844)  loss_ce_1: 0.0830 (0.1101)  loss_bbox_1: 0.0472 (0.0480)  loss_giou_1: 0.3829 (0.3834)  loss_ce_2: 0.0771 (0.1048)  loss_bbox_2: 0.0484 (0.0479)  loss_giou_2: 0.3847 (0.3819)  loss_ce_3: 0.0791 (0.0981)  loss_bbox_3: 0.0459 (0.0478)  loss_giou_3: 0.3854 (0.3816)  loss_ce_4: 0.0796 (0.0957)  loss_bbox_4: 0.0476 (0.0478)  loss_giou_4: 0.3802 (0.3816)  loss_ce_interm: 0.1556 (0.1676)  loss_bbox_interm: 0.0488 (0.0494)  loss_giou_interm: 0.3854 (0.3860)  loss_ce_unscaled: 0.0336 (0.0472)  loss_bbox_unscaled: 0.0095 (0.0096)  loss_giou_unscaled: 0.1911 (0.1901)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0064 (0.0068)  loss_ce_0_unscaled: 0.0499 (0.0620)  loss_bbox_0_unscaled: 0.0096 (0.0096)  loss_giou_0_unscaled: 0.1898 (0.1922)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0065 (0.0068)  loss_ce_1_unscaled: 0.0415 (0.0550)  loss_bbox_1_unscaled: 0.0094 (0.0096)  loss_giou_1_unscaled: 0.1915 (0.1917)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0065 (0.0068)  loss_ce_2_unscaled: 0.0385 (0.0524)  loss_bbox_2_unscaled: 0.0097 (0.0096)  loss_giou_2_unscaled: 0.1923 (0.1910)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0064 (0.0068)  loss_ce_3_unscaled: 0.0396 (0.0491)  loss_bbox_3_unscaled: 0.0092 (0.0096)  loss_giou_3_unscaled: 0.1927 (0.1908)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0064 (0.0068)  loss_ce_4_unscaled: 0.0398 (0.0478)  loss_bbox_4_unscaled: 0.0095 (0.0096)  loss_giou_4_unscaled: 0.1901 (0.1908)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0064 (0.0068)  loss_ce_interm_unscaled: 0.0778 (0.0838)  loss_bbox_interm_unscaled: 0.0098 (0.0099)  loss_giou_interm_unscaled: 0.1927 (0.1930)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0065 (0.0070)  time: 2.8165  data: 0.0160  max mem: 8948\n",
            "Epoch: [11]  [60/90]  eta: 0:01:27  lr: 0.000001  loss: 3.7749 (3.8192)  loss_ce: 0.0806 (0.0934)  loss_bbox: 0.0456 (0.0483)  loss_giou: 0.3980 (0.3820)  loss_ce_0: 0.1136 (0.1234)  loss_bbox_0: 0.0465 (0.0486)  loss_giou_0: 0.3954 (0.3865)  loss_ce_1: 0.1049 (0.1101)  loss_bbox_1: 0.0458 (0.0484)  loss_giou_1: 0.4021 (0.3841)  loss_ce_2: 0.0934 (0.1037)  loss_bbox_2: 0.0460 (0.0483)  loss_giou_2: 0.3988 (0.3829)  loss_ce_3: 0.0823 (0.0965)  loss_bbox_3: 0.0461 (0.0482)  loss_giou_3: 0.4002 (0.3833)  loss_ce_4: 0.0834 (0.0963)  loss_bbox_4: 0.0455 (0.0482)  loss_giou_4: 0.3986 (0.3823)  loss_ce_interm: 0.1615 (0.1657)  loss_bbox_interm: 0.0458 (0.0502)  loss_giou_interm: 0.4014 (0.3888)  loss_ce_unscaled: 0.0403 (0.0467)  loss_bbox_unscaled: 0.0091 (0.0097)  loss_giou_unscaled: 0.1990 (0.1910)  loss_xy_unscaled: 0.0026 (0.0028)  loss_hw_unscaled: 0.0062 (0.0068)  loss_ce_0_unscaled: 0.0568 (0.0617)  loss_bbox_0_unscaled: 0.0093 (0.0097)  loss_giou_0_unscaled: 0.1977 (0.1932)  loss_xy_0_unscaled: 0.0028 (0.0029)  loss_hw_0_unscaled: 0.0063 (0.0069)  loss_ce_1_unscaled: 0.0524 (0.0551)  loss_bbox_1_unscaled: 0.0092 (0.0097)  loss_giou_1_unscaled: 0.2010 (0.1921)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0063 (0.0068)  loss_ce_2_unscaled: 0.0467 (0.0518)  loss_bbox_2_unscaled: 0.0092 (0.0097)  loss_giou_2_unscaled: 0.1994 (0.1914)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0062 (0.0068)  loss_ce_3_unscaled: 0.0412 (0.0482)  loss_bbox_3_unscaled: 0.0092 (0.0096)  loss_giou_3_unscaled: 0.2001 (0.1917)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0062 (0.0068)  loss_ce_4_unscaled: 0.0417 (0.0481)  loss_bbox_4_unscaled: 0.0091 (0.0096)  loss_giou_4_unscaled: 0.1993 (0.1912)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0062 (0.0068)  loss_ce_interm_unscaled: 0.0808 (0.0828)  loss_bbox_interm_unscaled: 0.0092 (0.0100)  loss_giou_interm_unscaled: 0.2007 (0.1944)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0065 (0.0071)  time: 2.7646  data: 0.0158  max mem: 8948\n",
            "Epoch: [11]  [70/90]  eta: 0:00:57  lr: 0.000001  loss: 3.8094 (3.8450)  loss_ce: 0.0901 (0.0941)  loss_bbox: 0.0475 (0.0487)  loss_giou: 0.3937 (0.3843)  loss_ce_0: 0.1243 (0.1244)  loss_bbox_0: 0.0495 (0.0492)  loss_giou_0: 0.4052 (0.3894)  loss_ce_1: 0.1082 (0.1110)  loss_bbox_1: 0.0477 (0.0490)  loss_giou_1: 0.4021 (0.3874)  loss_ce_2: 0.0937 (0.1040)  loss_bbox_2: 0.0488 (0.0487)  loss_giou_2: 0.3936 (0.3851)  loss_ce_3: 0.0880 (0.0971)  loss_bbox_3: 0.0476 (0.0486)  loss_giou_3: 0.3951 (0.3855)  loss_ce_4: 0.0925 (0.0967)  loss_bbox_4: 0.0484 (0.0485)  loss_giou_4: 0.3934 (0.3845)  loss_ce_interm: 0.1625 (0.1673)  loss_bbox_interm: 0.0526 (0.0506)  loss_giou_interm: 0.4014 (0.3909)  loss_ce_unscaled: 0.0450 (0.0470)  loss_bbox_unscaled: 0.0095 (0.0097)  loss_giou_unscaled: 0.1969 (0.1921)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0069 (0.0069)  loss_ce_0_unscaled: 0.0621 (0.0622)  loss_bbox_0_unscaled: 0.0099 (0.0098)  loss_giou_0_unscaled: 0.2026 (0.1947)  loss_xy_0_unscaled: 0.0029 (0.0029)  loss_hw_0_unscaled: 0.0068 (0.0070)  loss_ce_1_unscaled: 0.0541 (0.0555)  loss_bbox_1_unscaled: 0.0095 (0.0098)  loss_giou_1_unscaled: 0.2010 (0.1937)  loss_xy_1_unscaled: 0.0028 (0.0029)  loss_hw_1_unscaled: 0.0069 (0.0069)  loss_ce_2_unscaled: 0.0468 (0.0520)  loss_bbox_2_unscaled: 0.0098 (0.0097)  loss_giou_2_unscaled: 0.1968 (0.1926)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0069 (0.0069)  loss_ce_3_unscaled: 0.0440 (0.0485)  loss_bbox_3_unscaled: 0.0095 (0.0097)  loss_giou_3_unscaled: 0.1975 (0.1928)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0069 (0.0069)  loss_ce_4_unscaled: 0.0462 (0.0484)  loss_bbox_4_unscaled: 0.0097 (0.0097)  loss_giou_4_unscaled: 0.1967 (0.1922)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0070 (0.0069)  loss_ce_interm_unscaled: 0.0813 (0.0836)  loss_bbox_interm_unscaled: 0.0105 (0.0101)  loss_giou_interm_unscaled: 0.2007 (0.1955)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0076 (0.0072)  time: 2.7826  data: 0.0149  max mem: 8948\n",
            "Epoch: [11]  [80/90]  eta: 0:00:28  lr: 0.000001  loss: 3.6163 (3.8220)  loss_ce: 0.0691 (0.0910)  loss_bbox: 0.0471 (0.0482)  loss_giou: 0.3895 (0.3837)  loss_ce_0: 0.1063 (0.1223)  loss_bbox_0: 0.0505 (0.0490)  loss_giou_0: 0.3936 (0.3895)  loss_ce_1: 0.0969 (0.1086)  loss_bbox_1: 0.0478 (0.0486)  loss_giou_1: 0.3871 (0.3868)  loss_ce_2: 0.0838 (0.1012)  loss_bbox_2: 0.0470 (0.0483)  loss_giou_2: 0.3868 (0.3846)  loss_ce_3: 0.0766 (0.0942)  loss_bbox_3: 0.0471 (0.0482)  loss_giou_3: 0.3857 (0.3850)  loss_ce_4: 0.0721 (0.0935)  loss_bbox_4: 0.0472 (0.0481)  loss_giou_4: 0.3849 (0.3838)  loss_ce_interm: 0.1576 (0.1656)  loss_bbox_interm: 0.0526 (0.0504)  loss_giou_interm: 0.3956 (0.3916)  loss_ce_unscaled: 0.0346 (0.0455)  loss_bbox_unscaled: 0.0094 (0.0096)  loss_giou_unscaled: 0.1948 (0.1919)  loss_xy_unscaled: 0.0026 (0.0028)  loss_hw_unscaled: 0.0068 (0.0068)  loss_ce_0_unscaled: 0.0531 (0.0611)  loss_bbox_0_unscaled: 0.0101 (0.0098)  loss_giou_0_unscaled: 0.1968 (0.1947)  loss_xy_0_unscaled: 0.0027 (0.0029)  loss_hw_0_unscaled: 0.0069 (0.0069)  loss_ce_1_unscaled: 0.0484 (0.0543)  loss_bbox_1_unscaled: 0.0096 (0.0097)  loss_giou_1_unscaled: 0.1935 (0.1934)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0069 (0.0069)  loss_ce_2_unscaled: 0.0419 (0.0506)  loss_bbox_2_unscaled: 0.0094 (0.0097)  loss_giou_2_unscaled: 0.1934 (0.1923)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0069 (0.0069)  loss_ce_3_unscaled: 0.0383 (0.0471)  loss_bbox_3_unscaled: 0.0094 (0.0096)  loss_giou_3_unscaled: 0.1928 (0.1925)  loss_xy_3_unscaled: 0.0026 (0.0028)  loss_hw_3_unscaled: 0.0067 (0.0068)  loss_ce_4_unscaled: 0.0361 (0.0467)  loss_bbox_4_unscaled: 0.0094 (0.0096)  loss_giou_4_unscaled: 0.1924 (0.1919)  loss_xy_4_unscaled: 0.0025 (0.0028)  loss_hw_4_unscaled: 0.0068 (0.0068)  loss_ce_interm_unscaled: 0.0788 (0.0828)  loss_bbox_interm_unscaled: 0.0105 (0.0101)  loss_giou_interm_unscaled: 0.1978 (0.1958)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0070 (0.0072)  time: 2.7955  data: 0.0153  max mem: 8948\n",
            "Epoch: [11]  [89/90]  eta: 0:00:02  lr: 0.000001  loss: 3.6224 (3.8335)  loss_ce: 0.0588 (0.0892)  loss_bbox: 0.0435 (0.0483)  loss_giou: 0.3885 (0.3866)  loss_ce_0: 0.1006 (0.1220)  loss_bbox_0: 0.0445 (0.0490)  loss_giou_0: 0.3936 (0.3921)  loss_ce_1: 0.0804 (0.1075)  loss_bbox_1: 0.0455 (0.0487)  loss_giou_1: 0.3952 (0.3896)  loss_ce_2: 0.0774 (0.1001)  loss_bbox_2: 0.0435 (0.0484)  loss_giou_2: 0.3868 (0.3873)  loss_ce_3: 0.0725 (0.0934)  loss_bbox_3: 0.0447 (0.0483)  loss_giou_3: 0.3901 (0.3878)  loss_ce_4: 0.0614 (0.0919)  loss_bbox_4: 0.0436 (0.0482)  loss_giou_4: 0.3882 (0.3866)  loss_ce_interm: 0.1427 (0.1647)  loss_bbox_interm: 0.0444 (0.0502)  loss_giou_interm: 0.3940 (0.3936)  loss_ce_unscaled: 0.0294 (0.0446)  loss_bbox_unscaled: 0.0087 (0.0097)  loss_giou_unscaled: 0.1942 (0.1933)  loss_xy_unscaled: 0.0026 (0.0028)  loss_hw_unscaled: 0.0062 (0.0069)  loss_ce_0_unscaled: 0.0503 (0.0610)  loss_bbox_0_unscaled: 0.0089 (0.0098)  loss_giou_0_unscaled: 0.1968 (0.1961)  loss_xy_0_unscaled: 0.0026 (0.0029)  loss_hw_0_unscaled: 0.0063 (0.0069)  loss_ce_1_unscaled: 0.0402 (0.0537)  loss_bbox_1_unscaled: 0.0091 (0.0097)  loss_giou_1_unscaled: 0.1976 (0.1948)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0064 (0.0069)  loss_ce_2_unscaled: 0.0387 (0.0500)  loss_bbox_2_unscaled: 0.0087 (0.0097)  loss_giou_2_unscaled: 0.1934 (0.1936)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0063 (0.0069)  loss_ce_3_unscaled: 0.0362 (0.0467)  loss_bbox_3_unscaled: 0.0089 (0.0097)  loss_giou_3_unscaled: 0.1951 (0.1939)  loss_xy_3_unscaled: 0.0026 (0.0028)  loss_hw_3_unscaled: 0.0063 (0.0069)  loss_ce_4_unscaled: 0.0307 (0.0459)  loss_bbox_4_unscaled: 0.0087 (0.0096)  loss_giou_4_unscaled: 0.1941 (0.1933)  loss_xy_4_unscaled: 0.0025 (0.0028)  loss_hw_4_unscaled: 0.0062 (0.0068)  loss_ce_interm_unscaled: 0.0713 (0.0823)  loss_bbox_interm_unscaled: 0.0089 (0.0100)  loss_giou_interm_unscaled: 0.1970 (0.1968)  loss_xy_interm_unscaled: 0.0026 (0.0029)  loss_hw_interm_unscaled: 0.0064 (0.0071)  time: 3.0611  data: 0.0162  max mem: 8948\n",
            "Epoch: [11] Total time: 0:04:23 (2.9238 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 3.6224 (3.8335)  loss_ce: 0.0588 (0.0892)  loss_bbox: 0.0435 (0.0483)  loss_giou: 0.3885 (0.3866)  loss_ce_0: 0.1006 (0.1220)  loss_bbox_0: 0.0445 (0.0490)  loss_giou_0: 0.3936 (0.3921)  loss_ce_1: 0.0804 (0.1075)  loss_bbox_1: 0.0455 (0.0487)  loss_giou_1: 0.3952 (0.3896)  loss_ce_2: 0.0774 (0.1001)  loss_bbox_2: 0.0435 (0.0484)  loss_giou_2: 0.3868 (0.3873)  loss_ce_3: 0.0725 (0.0934)  loss_bbox_3: 0.0447 (0.0483)  loss_giou_3: 0.3901 (0.3878)  loss_ce_4: 0.0614 (0.0919)  loss_bbox_4: 0.0436 (0.0482)  loss_giou_4: 0.3882 (0.3866)  loss_ce_interm: 0.1427 (0.1647)  loss_bbox_interm: 0.0444 (0.0502)  loss_giou_interm: 0.3940 (0.3936)  loss_ce_unscaled: 0.0294 (0.0446)  loss_bbox_unscaled: 0.0087 (0.0097)  loss_giou_unscaled: 0.1942 (0.1933)  loss_xy_unscaled: 0.0026 (0.0028)  loss_hw_unscaled: 0.0062 (0.0069)  loss_ce_0_unscaled: 0.0503 (0.0610)  loss_bbox_0_unscaled: 0.0089 (0.0098)  loss_giou_0_unscaled: 0.1968 (0.1961)  loss_xy_0_unscaled: 0.0026 (0.0029)  loss_hw_0_unscaled: 0.0063 (0.0069)  loss_ce_1_unscaled: 0.0402 (0.0537)  loss_bbox_1_unscaled: 0.0091 (0.0097)  loss_giou_1_unscaled: 0.1976 (0.1948)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0064 (0.0069)  loss_ce_2_unscaled: 0.0387 (0.0500)  loss_bbox_2_unscaled: 0.0087 (0.0097)  loss_giou_2_unscaled: 0.1934 (0.1936)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0063 (0.0069)  loss_ce_3_unscaled: 0.0362 (0.0467)  loss_bbox_3_unscaled: 0.0089 (0.0097)  loss_giou_3_unscaled: 0.1951 (0.1939)  loss_xy_3_unscaled: 0.0026 (0.0028)  loss_hw_3_unscaled: 0.0063 (0.0069)  loss_ce_4_unscaled: 0.0307 (0.0459)  loss_bbox_4_unscaled: 0.0087 (0.0096)  loss_giou_4_unscaled: 0.1941 (0.1933)  loss_xy_4_unscaled: 0.0025 (0.0028)  loss_hw_4_unscaled: 0.0062 (0.0068)  loss_ce_interm_unscaled: 0.0713 (0.0823)  loss_bbox_interm_unscaled: 0.0089 (0.0100)  loss_giou_interm_unscaled: 0.1970 (0.1968)  loss_xy_interm_unscaled: 0.0026 (0.0029)  loss_hw_interm_unscaled: 0.0064 (0.0071)\n",
            "Input text prompt: bolt . wrong direction . 1 . 2 . 3 . 4 . 5 .\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Test:  [ 0/10]  eta: 0:00:17    time: 1.7968  data: 0.8432  max mem: 8948\n",
            "Test:  [ 9/10]  eta: 0:00:01    time: 1.0294  data: 0.1004  max mem: 8948\n",
            "Test: Total time: 0:00:10 (1.0385 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.055\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.115\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.041\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.305\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.223\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.009\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.232\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.490\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.597\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.440\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch: [12]  [ 0/90]  eta: 0:05:35  lr: 0.000000  loss: 4.5151 (4.5151)  loss_ce: 0.1327 (0.1327)  loss_bbox: 0.0590 (0.0590)  loss_giou: 0.4280 (0.4280)  loss_ce_0: 0.2064 (0.2064)  loss_bbox_0: 0.0580 (0.0580)  loss_giou_0: 0.4278 (0.4278)  loss_ce_1: 0.1744 (0.1744)  loss_bbox_1: 0.0566 (0.0566)  loss_giou_1: 0.4072 (0.4072)  loss_ce_2: 0.1836 (0.1836)  loss_bbox_2: 0.0565 (0.0565)  loss_giou_2: 0.4015 (0.4015)  loss_ce_3: 0.1218 (0.1218)  loss_bbox_3: 0.0565 (0.0565)  loss_giou_3: 0.4134 (0.4134)  loss_ce_4: 0.1384 (0.1384)  loss_bbox_4: 0.0586 (0.0586)  loss_giou_4: 0.4270 (0.4270)  loss_ce_interm: 0.2236 (0.2236)  loss_bbox_interm: 0.0595 (0.0595)  loss_giou_interm: 0.4246 (0.4246)  loss_ce_unscaled: 0.0663 (0.0663)  loss_bbox_unscaled: 0.0118 (0.0118)  loss_giou_unscaled: 0.2140 (0.2140)  loss_xy_unscaled: 0.0039 (0.0039)  loss_hw_unscaled: 0.0079 (0.0079)  loss_ce_0_unscaled: 0.1032 (0.1032)  loss_bbox_0_unscaled: 0.0116 (0.0116)  loss_giou_0_unscaled: 0.2139 (0.2139)  loss_xy_0_unscaled: 0.0038 (0.0038)  loss_hw_0_unscaled: 0.0078 (0.0078)  loss_ce_1_unscaled: 0.0872 (0.0872)  loss_bbox_1_unscaled: 0.0113 (0.0113)  loss_giou_1_unscaled: 0.2036 (0.2036)  loss_xy_1_unscaled: 0.0039 (0.0039)  loss_hw_1_unscaled: 0.0075 (0.0075)  loss_ce_2_unscaled: 0.0918 (0.0918)  loss_bbox_2_unscaled: 0.0113 (0.0113)  loss_giou_2_unscaled: 0.2007 (0.2007)  loss_xy_2_unscaled: 0.0037 (0.0037)  loss_hw_2_unscaled: 0.0076 (0.0076)  loss_ce_3_unscaled: 0.0609 (0.0609)  loss_bbox_3_unscaled: 0.0113 (0.0113)  loss_giou_3_unscaled: 0.2067 (0.2067)  loss_xy_3_unscaled: 0.0038 (0.0038)  loss_hw_3_unscaled: 0.0075 (0.0075)  loss_ce_4_unscaled: 0.0692 (0.0692)  loss_bbox_4_unscaled: 0.0117 (0.0117)  loss_giou_4_unscaled: 0.2135 (0.2135)  loss_xy_4_unscaled: 0.0038 (0.0038)  loss_hw_4_unscaled: 0.0079 (0.0079)  loss_ce_interm_unscaled: 0.1118 (0.1118)  loss_bbox_interm_unscaled: 0.0119 (0.0119)  loss_giou_interm_unscaled: 0.2123 (0.2123)  loss_xy_interm_unscaled: 0.0040 (0.0040)  loss_hw_interm_unscaled: 0.0079 (0.0079)  time: 3.7316  data: 0.8641  max mem: 8948\n",
            "Epoch: [12]  [10/90]  eta: 0:03:44  lr: 0.000000  loss: 3.7053 (3.9642)  loss_ce: 0.0628 (0.0834)  loss_bbox: 0.0461 (0.0477)  loss_giou: 0.3938 (0.4110)  loss_ce_0: 0.1031 (0.1226)  loss_bbox_0: 0.0465 (0.0485)  loss_giou_0: 0.3991 (0.4143)  loss_ce_1: 0.0706 (0.1004)  loss_bbox_1: 0.0460 (0.0488)  loss_giou_1: 0.3964 (0.4100)  loss_ce_2: 0.0709 (0.0991)  loss_bbox_2: 0.0462 (0.0478)  loss_giou_2: 0.3961 (0.4057)  loss_ce_3: 0.0736 (0.0896)  loss_bbox_3: 0.0458 (0.0473)  loss_giou_3: 0.3918 (0.4077)  loss_ce_4: 0.0667 (0.0851)  loss_bbox_4: 0.0461 (0.0477)  loss_giou_4: 0.3946 (0.4122)  loss_ce_interm: 0.1430 (0.1702)  loss_bbox_interm: 0.0470 (0.0494)  loss_giou_interm: 0.3980 (0.4158)  loss_ce_unscaled: 0.0314 (0.0417)  loss_bbox_unscaled: 0.0092 (0.0095)  loss_giou_unscaled: 0.1969 (0.2055)  loss_xy_unscaled: 0.0028 (0.0030)  loss_hw_unscaled: 0.0065 (0.0066)  loss_ce_0_unscaled: 0.0516 (0.0613)  loss_bbox_0_unscaled: 0.0093 (0.0097)  loss_giou_0_unscaled: 0.1995 (0.2072)  loss_xy_0_unscaled: 0.0029 (0.0030)  loss_hw_0_unscaled: 0.0066 (0.0067)  loss_ce_1_unscaled: 0.0353 (0.0502)  loss_bbox_1_unscaled: 0.0092 (0.0098)  loss_giou_1_unscaled: 0.1982 (0.2050)  loss_xy_1_unscaled: 0.0028 (0.0030)  loss_hw_1_unscaled: 0.0065 (0.0068)  loss_ce_2_unscaled: 0.0355 (0.0495)  loss_bbox_2_unscaled: 0.0092 (0.0096)  loss_giou_2_unscaled: 0.1980 (0.2028)  loss_xy_2_unscaled: 0.0028 (0.0029)  loss_hw_2_unscaled: 0.0065 (0.0066)  loss_ce_3_unscaled: 0.0368 (0.0448)  loss_bbox_3_unscaled: 0.0092 (0.0095)  loss_giou_3_unscaled: 0.1959 (0.2038)  loss_xy_3_unscaled: 0.0028 (0.0030)  loss_hw_3_unscaled: 0.0065 (0.0065)  loss_ce_4_unscaled: 0.0333 (0.0426)  loss_bbox_4_unscaled: 0.0092 (0.0095)  loss_giou_4_unscaled: 0.1973 (0.2061)  loss_xy_4_unscaled: 0.0028 (0.0030)  loss_hw_4_unscaled: 0.0065 (0.0066)  loss_ce_interm_unscaled: 0.0715 (0.0851)  loss_bbox_interm_unscaled: 0.0094 (0.0099)  loss_giou_interm_unscaled: 0.1990 (0.2079)  loss_xy_interm_unscaled: 0.0030 (0.0030)  loss_hw_interm_unscaled: 0.0068 (0.0068)  time: 2.8115  data: 0.0923  max mem: 8948\n",
            "Epoch: [12]  [20/90]  eta: 0:03:14  lr: 0.000000  loss: 3.6672 (3.8899)  loss_ce: 0.0699 (0.0876)  loss_bbox: 0.0452 (0.0476)  loss_giou: 0.3822 (0.3986)  loss_ce_0: 0.1031 (0.1200)  loss_bbox_0: 0.0465 (0.0483)  loss_giou_0: 0.3927 (0.4029)  loss_ce_1: 0.0918 (0.1026)  loss_bbox_1: 0.0460 (0.0484)  loss_giou_1: 0.3826 (0.3986)  loss_ce_2: 0.0709 (0.0970)  loss_bbox_2: 0.0453 (0.0478)  loss_giou_2: 0.3866 (0.3970)  loss_ce_3: 0.0766 (0.0929)  loss_bbox_3: 0.0455 (0.0474)  loss_giou_3: 0.3835 (0.3976)  loss_ce_4: 0.0744 (0.0886)  loss_bbox_4: 0.0451 (0.0475)  loss_giou_4: 0.3848 (0.3996)  loss_ce_interm: 0.1411 (0.1665)  loss_bbox_interm: 0.0472 (0.0493)  loss_giou_interm: 0.3870 (0.4040)  loss_ce_unscaled: 0.0349 (0.0438)  loss_bbox_unscaled: 0.0090 (0.0095)  loss_giou_unscaled: 0.1911 (0.1993)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0065 (0.0067)  loss_ce_0_unscaled: 0.0516 (0.0600)  loss_bbox_0_unscaled: 0.0093 (0.0097)  loss_giou_0_unscaled: 0.1963 (0.2014)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0066 (0.0068)  loss_ce_1_unscaled: 0.0459 (0.0513)  loss_bbox_1_unscaled: 0.0092 (0.0097)  loss_giou_1_unscaled: 0.1913 (0.1993)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0067 (0.0068)  loss_ce_2_unscaled: 0.0355 (0.0485)  loss_bbox_2_unscaled: 0.0091 (0.0096)  loss_giou_2_unscaled: 0.1933 (0.1985)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0065 (0.0068)  loss_ce_3_unscaled: 0.0383 (0.0464)  loss_bbox_3_unscaled: 0.0091 (0.0095)  loss_giou_3_unscaled: 0.1918 (0.1988)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0065 (0.0067)  loss_ce_4_unscaled: 0.0372 (0.0443)  loss_bbox_4_unscaled: 0.0090 (0.0095)  loss_giou_4_unscaled: 0.1924 (0.1998)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0065 (0.0067)  loss_ce_interm_unscaled: 0.0706 (0.0832)  loss_bbox_interm_unscaled: 0.0094 (0.0099)  loss_giou_interm_unscaled: 0.1935 (0.2020)  loss_xy_interm_unscaled: 0.0027 (0.0029)  loss_hw_interm_unscaled: 0.0068 (0.0070)  time: 2.7360  data: 0.0139  max mem: 8948\n",
            "Epoch: [12]  [30/90]  eta: 0:02:48  lr: 0.000000  loss: 3.7123 (3.8858)  loss_ce: 0.0730 (0.0909)  loss_bbox: 0.0445 (0.0471)  loss_giou: 0.3806 (0.3954)  loss_ce_0: 0.1079 (0.1262)  loss_bbox_0: 0.0457 (0.0473)  loss_giou_0: 0.3799 (0.3954)  loss_ce_1: 0.1022 (0.1080)  loss_bbox_1: 0.0453 (0.0474)  loss_giou_1: 0.3826 (0.3943)  loss_ce_2: 0.0864 (0.1016)  loss_bbox_2: 0.0450 (0.0471)  loss_giou_2: 0.3822 (0.3935)  loss_ce_3: 0.0836 (0.0985)  loss_bbox_3: 0.0446 (0.0469)  loss_giou_3: 0.3813 (0.3940)  loss_ce_4: 0.0834 (0.0962)  loss_bbox_4: 0.0445 (0.0468)  loss_giou_4: 0.3806 (0.3946)  loss_ce_interm: 0.1476 (0.1650)  loss_bbox_interm: 0.0479 (0.0485)  loss_giou_interm: 0.3886 (0.4013)  loss_ce_unscaled: 0.0365 (0.0454)  loss_bbox_unscaled: 0.0089 (0.0094)  loss_giou_unscaled: 0.1903 (0.1977)  loss_xy_unscaled: 0.0025 (0.0028)  loss_hw_unscaled: 0.0063 (0.0067)  loss_ce_0_unscaled: 0.0540 (0.0631)  loss_bbox_0_unscaled: 0.0091 (0.0095)  loss_giou_0_unscaled: 0.1899 (0.1977)  loss_xy_0_unscaled: 0.0025 (0.0027)  loss_hw_0_unscaled: 0.0063 (0.0067)  loss_ce_1_unscaled: 0.0511 (0.0540)  loss_bbox_1_unscaled: 0.0091 (0.0095)  loss_giou_1_unscaled: 0.1913 (0.1971)  loss_xy_1_unscaled: 0.0025 (0.0027)  loss_hw_1_unscaled: 0.0062 (0.0067)  loss_ce_2_unscaled: 0.0432 (0.0508)  loss_bbox_2_unscaled: 0.0090 (0.0094)  loss_giou_2_unscaled: 0.1911 (0.1967)  loss_xy_2_unscaled: 0.0025 (0.0027)  loss_hw_2_unscaled: 0.0063 (0.0067)  loss_ce_3_unscaled: 0.0418 (0.0492)  loss_bbox_3_unscaled: 0.0089 (0.0094)  loss_giou_3_unscaled: 0.1907 (0.1970)  loss_xy_3_unscaled: 0.0025 (0.0027)  loss_hw_3_unscaled: 0.0063 (0.0066)  loss_ce_4_unscaled: 0.0417 (0.0481)  loss_bbox_4_unscaled: 0.0089 (0.0094)  loss_giou_4_unscaled: 0.1903 (0.1973)  loss_xy_4_unscaled: 0.0025 (0.0027)  loss_hw_4_unscaled: 0.0063 (0.0066)  loss_ce_interm_unscaled: 0.0738 (0.0825)  loss_bbox_interm_unscaled: 0.0096 (0.0097)  loss_giou_interm_unscaled: 0.1943 (0.2007)  loss_xy_interm_unscaled: 0.0026 (0.0028)  loss_hw_interm_unscaled: 0.0066 (0.0069)  time: 2.7955  data: 0.0141  max mem: 8948\n",
            "Epoch: [12]  [40/90]  eta: 0:02:21  lr: 0.000000  loss: 3.9327 (3.8740)  loss_ce: 0.0703 (0.0900)  loss_bbox: 0.0432 (0.0466)  loss_giou: 0.3815 (0.3959)  loss_ce_0: 0.1079 (0.1250)  loss_bbox_0: 0.0427 (0.0468)  loss_giou_0: 0.3776 (0.3965)  loss_ce_1: 0.0883 (0.1052)  loss_bbox_1: 0.0432 (0.0468)  loss_giou_1: 0.3894 (0.3960)  loss_ce_2: 0.0828 (0.1002)  loss_bbox_2: 0.0427 (0.0466)  loss_giou_2: 0.3818 (0.3943)  loss_ce_3: 0.0803 (0.0968)  loss_bbox_3: 0.0434 (0.0465)  loss_giou_3: 0.3881 (0.3945)  loss_ce_4: 0.0722 (0.0940)  loss_bbox_4: 0.0431 (0.0463)  loss_giou_4: 0.3776 (0.3954)  loss_ce_interm: 0.1511 (0.1624)  loss_bbox_interm: 0.0444 (0.0479)  loss_giou_interm: 0.3914 (0.4003)  loss_ce_unscaled: 0.0352 (0.0450)  loss_bbox_unscaled: 0.0086 (0.0093)  loss_giou_unscaled: 0.1908 (0.1980)  loss_xy_unscaled: 0.0025 (0.0027)  loss_hw_unscaled: 0.0062 (0.0066)  loss_ce_0_unscaled: 0.0540 (0.0625)  loss_bbox_0_unscaled: 0.0085 (0.0094)  loss_giou_0_unscaled: 0.1888 (0.1983)  loss_xy_0_unscaled: 0.0025 (0.0028)  loss_hw_0_unscaled: 0.0060 (0.0066)  loss_ce_1_unscaled: 0.0441 (0.0526)  loss_bbox_1_unscaled: 0.0086 (0.0094)  loss_giou_1_unscaled: 0.1947 (0.1980)  loss_xy_1_unscaled: 0.0025 (0.0027)  loss_hw_1_unscaled: 0.0062 (0.0066)  loss_ce_2_unscaled: 0.0414 (0.0501)  loss_bbox_2_unscaled: 0.0085 (0.0093)  loss_giou_2_unscaled: 0.1909 (0.1972)  loss_xy_2_unscaled: 0.0024 (0.0027)  loss_hw_2_unscaled: 0.0061 (0.0066)  loss_ce_3_unscaled: 0.0402 (0.0484)  loss_bbox_3_unscaled: 0.0087 (0.0093)  loss_giou_3_unscaled: 0.1940 (0.1972)  loss_xy_3_unscaled: 0.0025 (0.0027)  loss_hw_3_unscaled: 0.0062 (0.0066)  loss_ce_4_unscaled: 0.0361 (0.0470)  loss_bbox_4_unscaled: 0.0086 (0.0093)  loss_giou_4_unscaled: 0.1888 (0.1977)  loss_xy_4_unscaled: 0.0025 (0.0027)  loss_hw_4_unscaled: 0.0062 (0.0065)  loss_ce_interm_unscaled: 0.0755 (0.0812)  loss_bbox_interm_unscaled: 0.0089 (0.0096)  loss_giou_interm_unscaled: 0.1957 (0.2002)  loss_xy_interm_unscaled: 0.0025 (0.0028)  loss_hw_interm_unscaled: 0.0063 (0.0068)  time: 2.8656  data: 0.0152  max mem: 8948\n",
            "Epoch: [12]  [50/90]  eta: 0:01:53  lr: 0.000000  loss: 3.7524 (3.8614)  loss_ce: 0.0730 (0.0886)  loss_bbox: 0.0464 (0.0476)  loss_giou: 0.3781 (0.3935)  loss_ce_0: 0.1088 (0.1235)  loss_bbox_0: 0.0466 (0.0477)  loss_giou_0: 0.3880 (0.3947)  loss_ce_1: 0.0845 (0.1049)  loss_bbox_1: 0.0465 (0.0476)  loss_giou_1: 0.3874 (0.3936)  loss_ce_2: 0.0816 (0.0997)  loss_bbox_2: 0.0467 (0.0473)  loss_giou_2: 0.3760 (0.3922)  loss_ce_3: 0.0803 (0.0971)  loss_bbox_3: 0.0463 (0.0473)  loss_giou_3: 0.3749 (0.3921)  loss_ce_4: 0.0722 (0.0922)  loss_bbox_4: 0.0463 (0.0474)  loss_giou_4: 0.3776 (0.3935)  loss_ce_interm: 0.1522 (0.1619)  loss_bbox_interm: 0.0488 (0.0490)  loss_giou_interm: 0.3839 (0.4001)  loss_ce_unscaled: 0.0365 (0.0443)  loss_bbox_unscaled: 0.0093 (0.0095)  loss_giou_unscaled: 0.1890 (0.1968)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0064 (0.0067)  loss_ce_0_unscaled: 0.0544 (0.0618)  loss_bbox_0_unscaled: 0.0093 (0.0095)  loss_giou_0_unscaled: 0.1940 (0.1974)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0064 (0.0067)  loss_ce_1_unscaled: 0.0422 (0.0525)  loss_bbox_1_unscaled: 0.0093 (0.0095)  loss_giou_1_unscaled: 0.1937 (0.1968)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0065 (0.0067)  loss_ce_2_unscaled: 0.0408 (0.0499)  loss_bbox_2_unscaled: 0.0093 (0.0095)  loss_giou_2_unscaled: 0.1880 (0.1961)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0063 (0.0067)  loss_ce_3_unscaled: 0.0402 (0.0485)  loss_bbox_3_unscaled: 0.0093 (0.0095)  loss_giou_3_unscaled: 0.1874 (0.1961)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0066 (0.0067)  loss_ce_4_unscaled: 0.0361 (0.0461)  loss_bbox_4_unscaled: 0.0093 (0.0095)  loss_giou_4_unscaled: 0.1888 (0.1967)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0064 (0.0067)  loss_ce_interm_unscaled: 0.0761 (0.0810)  loss_bbox_interm_unscaled: 0.0098 (0.0098)  loss_giou_interm_unscaled: 0.1920 (0.2001)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0064 (0.0069)  time: 2.8814  data: 0.0148  max mem: 8948\n",
            "Epoch: [12]  [60/90]  eta: 0:01:26  lr: 0.000000  loss: 3.6488 (3.8255)  loss_ce: 0.0818 (0.0862)  loss_bbox: 0.0429 (0.0471)  loss_giou: 0.3696 (0.3911)  loss_ce_0: 0.1096 (0.1209)  loss_bbox_0: 0.0435 (0.0472)  loss_giou_0: 0.3758 (0.3928)  loss_ce_1: 0.0849 (0.1023)  loss_bbox_1: 0.0436 (0.0471)  loss_giou_1: 0.3715 (0.3913)  loss_ce_2: 0.0898 (0.0970)  loss_bbox_2: 0.0450 (0.0470)  loss_giou_2: 0.3694 (0.3901)  loss_ce_3: 0.0952 (0.0941)  loss_bbox_3: 0.0435 (0.0469)  loss_giou_3: 0.3721 (0.3901)  loss_ce_4: 0.0745 (0.0896)  loss_bbox_4: 0.0430 (0.0469)  loss_giou_4: 0.3706 (0.3913)  loss_ce_interm: 0.1637 (0.1612)  loss_bbox_interm: 0.0454 (0.0485)  loss_giou_interm: 0.3686 (0.3968)  loss_ce_unscaled: 0.0409 (0.0431)  loss_bbox_unscaled: 0.0086 (0.0094)  loss_giou_unscaled: 0.1848 (0.1956)  loss_xy_unscaled: 0.0026 (0.0027)  loss_hw_unscaled: 0.0062 (0.0067)  loss_ce_0_unscaled: 0.0548 (0.0604)  loss_bbox_0_unscaled: 0.0087 (0.0094)  loss_giou_0_unscaled: 0.1879 (0.1964)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0063 (0.0067)  loss_ce_1_unscaled: 0.0425 (0.0512)  loss_bbox_1_unscaled: 0.0087 (0.0094)  loss_giou_1_unscaled: 0.1857 (0.1957)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0062 (0.0067)  loss_ce_2_unscaled: 0.0449 (0.0485)  loss_bbox_2_unscaled: 0.0090 (0.0094)  loss_giou_2_unscaled: 0.1847 (0.1951)  loss_xy_2_unscaled: 0.0026 (0.0027)  loss_hw_2_unscaled: 0.0063 (0.0067)  loss_ce_3_unscaled: 0.0476 (0.0470)  loss_bbox_3_unscaled: 0.0087 (0.0094)  loss_giou_3_unscaled: 0.1860 (0.1951)  loss_xy_3_unscaled: 0.0026 (0.0027)  loss_hw_3_unscaled: 0.0063 (0.0067)  loss_ce_4_unscaled: 0.0372 (0.0448)  loss_bbox_4_unscaled: 0.0086 (0.0094)  loss_giou_4_unscaled: 0.1853 (0.1956)  loss_xy_4_unscaled: 0.0026 (0.0027)  loss_hw_4_unscaled: 0.0062 (0.0066)  loss_ce_interm_unscaled: 0.0818 (0.0806)  loss_bbox_interm_unscaled: 0.0091 (0.0097)  loss_giou_interm_unscaled: 0.1843 (0.1984)  loss_xy_interm_unscaled: 0.0027 (0.0028)  loss_hw_interm_unscaled: 0.0064 (0.0069)  time: 2.9697  data: 0.0155  max mem: 8948\n",
            "Epoch: [12]  [70/90]  eta: 0:00:56  lr: 0.000000  loss: 3.5952 (3.8356)  loss_ce: 0.0659 (0.0871)  loss_bbox: 0.0449 (0.0481)  loss_giou: 0.3888 (0.3909)  loss_ce_0: 0.0948 (0.1219)  loss_bbox_0: 0.0453 (0.0480)  loss_giou_0: 0.3883 (0.3930)  loss_ce_1: 0.0799 (0.1027)  loss_bbox_1: 0.0451 (0.0481)  loss_giou_1: 0.3872 (0.3914)  loss_ce_2: 0.0763 (0.0980)  loss_bbox_2: 0.0450 (0.0480)  loss_giou_2: 0.3895 (0.3897)  loss_ce_3: 0.0673 (0.0946)  loss_bbox_3: 0.0448 (0.0479)  loss_giou_3: 0.3907 (0.3897)  loss_ce_4: 0.0700 (0.0899)  loss_bbox_4: 0.0449 (0.0479)  loss_giou_4: 0.3892 (0.3911)  loss_ce_interm: 0.1539 (0.1617)  loss_bbox_interm: 0.0466 (0.0493)  loss_giou_interm: 0.3925 (0.3968)  loss_ce_unscaled: 0.0329 (0.0436)  loss_bbox_unscaled: 0.0090 (0.0096)  loss_giou_unscaled: 0.1944 (0.1954)  loss_xy_unscaled: 0.0026 (0.0028)  loss_hw_unscaled: 0.0064 (0.0068)  loss_ce_0_unscaled: 0.0474 (0.0609)  loss_bbox_0_unscaled: 0.0091 (0.0096)  loss_giou_0_unscaled: 0.1942 (0.1965)  loss_xy_0_unscaled: 0.0026 (0.0028)  loss_hw_0_unscaled: 0.0066 (0.0068)  loss_ce_1_unscaled: 0.0400 (0.0514)  loss_bbox_1_unscaled: 0.0090 (0.0096)  loss_giou_1_unscaled: 0.1936 (0.1957)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0064 (0.0068)  loss_ce_2_unscaled: 0.0381 (0.0490)  loss_bbox_2_unscaled: 0.0090 (0.0096)  loss_giou_2_unscaled: 0.1947 (0.1948)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0064 (0.0068)  loss_ce_3_unscaled: 0.0336 (0.0473)  loss_bbox_3_unscaled: 0.0090 (0.0096)  loss_giou_3_unscaled: 0.1953 (0.1949)  loss_xy_3_unscaled: 0.0026 (0.0028)  loss_hw_3_unscaled: 0.0064 (0.0068)  loss_ce_4_unscaled: 0.0350 (0.0449)  loss_bbox_4_unscaled: 0.0090 (0.0096)  loss_giou_4_unscaled: 0.1946 (0.1955)  loss_xy_4_unscaled: 0.0026 (0.0028)  loss_hw_4_unscaled: 0.0064 (0.0068)  loss_ce_interm_unscaled: 0.0769 (0.0808)  loss_bbox_interm_unscaled: 0.0093 (0.0099)  loss_giou_interm_unscaled: 0.1963 (0.1984)  loss_xy_interm_unscaled: 0.0027 (0.0029)  loss_hw_interm_unscaled: 0.0067 (0.0070)  time: 2.8349  data: 0.0152  max mem: 8948\n",
            "Epoch: [12]  [80/90]  eta: 0:00:28  lr: 0.000000  loss: 3.7504 (3.8350)  loss_ce: 0.0865 (0.0878)  loss_bbox: 0.0449 (0.0476)  loss_giou: 0.3887 (0.3905)  loss_ce_0: 0.1110 (0.1224)  loss_bbox_0: 0.0459 (0.0476)  loss_giou_0: 0.3883 (0.3928)  loss_ce_1: 0.0963 (0.1030)  loss_bbox_1: 0.0454 (0.0477)  loss_giou_1: 0.3881 (0.3916)  loss_ce_2: 0.0832 (0.0983)  loss_bbox_2: 0.0460 (0.0475)  loss_giou_2: 0.3895 (0.3897)  loss_ce_3: 0.0839 (0.0946)  loss_bbox_3: 0.0455 (0.0474)  loss_giou_3: 0.3890 (0.3897)  loss_ce_4: 0.0864 (0.0905)  loss_bbox_4: 0.0449 (0.0474)  loss_giou_4: 0.3874 (0.3907)  loss_ce_interm: 0.1505 (0.1619)  loss_bbox_interm: 0.0470 (0.0489)  loss_giou_interm: 0.4000 (0.3975)  loss_ce_unscaled: 0.0433 (0.0439)  loss_bbox_unscaled: 0.0090 (0.0095)  loss_giou_unscaled: 0.1944 (0.1953)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0064 (0.0067)  loss_ce_0_unscaled: 0.0555 (0.0612)  loss_bbox_0_unscaled: 0.0092 (0.0095)  loss_giou_0_unscaled: 0.1942 (0.1964)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0066 (0.0067)  loss_ce_1_unscaled: 0.0481 (0.0515)  loss_bbox_1_unscaled: 0.0091 (0.0095)  loss_giou_1_unscaled: 0.1940 (0.1958)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0064 (0.0067)  loss_ce_2_unscaled: 0.0416 (0.0492)  loss_bbox_2_unscaled: 0.0092 (0.0095)  loss_giou_2_unscaled: 0.1948 (0.1949)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0064 (0.0067)  loss_ce_3_unscaled: 0.0419 (0.0473)  loss_bbox_3_unscaled: 0.0091 (0.0095)  loss_giou_3_unscaled: 0.1945 (0.1949)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0064 (0.0067)  loss_ce_4_unscaled: 0.0432 (0.0453)  loss_bbox_4_unscaled: 0.0090 (0.0095)  loss_giou_4_unscaled: 0.1937 (0.1954)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0064 (0.0067)  loss_ce_interm_unscaled: 0.0753 (0.0809)  loss_bbox_interm_unscaled: 0.0094 (0.0098)  loss_giou_interm_unscaled: 0.2000 (0.1987)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0068 (0.0069)  time: 2.6970  data: 0.0150  max mem: 8948\n",
            "Epoch: [12]  [89/90]  eta: 0:00:02  lr: 0.000000  loss: 3.6179 (3.8147)  loss_ce: 0.0762 (0.0870)  loss_bbox: 0.0439 (0.0474)  loss_giou: 0.3760 (0.3894)  loss_ce_0: 0.0964 (0.1197)  loss_bbox_0: 0.0442 (0.0474)  loss_giou_0: 0.3841 (0.3918)  loss_ce_1: 0.0896 (0.1016)  loss_bbox_1: 0.0443 (0.0475)  loss_giou_1: 0.3810 (0.3905)  loss_ce_2: 0.0806 (0.0972)  loss_bbox_2: 0.0443 (0.0473)  loss_giou_2: 0.3801 (0.3888)  loss_ce_3: 0.0775 (0.0937)  loss_bbox_3: 0.0442 (0.0472)  loss_giou_3: 0.3751 (0.3885)  loss_ce_4: 0.0819 (0.0895)  loss_bbox_4: 0.0438 (0.0472)  loss_giou_4: 0.3760 (0.3896)  loss_ce_interm: 0.1360 (0.1584)  loss_bbox_interm: 0.0461 (0.0486)  loss_giou_interm: 0.3890 (0.3964)  loss_ce_unscaled: 0.0381 (0.0435)  loss_bbox_unscaled: 0.0088 (0.0095)  loss_giou_unscaled: 0.1880 (0.1947)  loss_xy_unscaled: 0.0026 (0.0028)  loss_hw_unscaled: 0.0061 (0.0067)  loss_ce_0_unscaled: 0.0482 (0.0599)  loss_bbox_0_unscaled: 0.0088 (0.0095)  loss_giou_0_unscaled: 0.1921 (0.1959)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0061 (0.0067)  loss_ce_1_unscaled: 0.0448 (0.0508)  loss_bbox_1_unscaled: 0.0089 (0.0095)  loss_giou_1_unscaled: 0.1905 (0.1952)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0062 (0.0067)  loss_ce_2_unscaled: 0.0403 (0.0486)  loss_bbox_2_unscaled: 0.0089 (0.0095)  loss_giou_2_unscaled: 0.1900 (0.1944)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0061 (0.0067)  loss_ce_3_unscaled: 0.0387 (0.0468)  loss_bbox_3_unscaled: 0.0088 (0.0094)  loss_giou_3_unscaled: 0.1876 (0.1943)  loss_xy_3_unscaled: 0.0026 (0.0028)  loss_hw_3_unscaled: 0.0061 (0.0067)  loss_ce_4_unscaled: 0.0410 (0.0447)  loss_bbox_4_unscaled: 0.0088 (0.0094)  loss_giou_4_unscaled: 0.1880 (0.1948)  loss_xy_4_unscaled: 0.0026 (0.0028)  loss_hw_4_unscaled: 0.0061 (0.0067)  loss_ce_interm_unscaled: 0.0680 (0.0792)  loss_bbox_interm_unscaled: 0.0092 (0.0097)  loss_giou_interm_unscaled: 0.1945 (0.1982)  loss_xy_interm_unscaled: 0.0027 (0.0028)  loss_hw_interm_unscaled: 0.0064 (0.0069)  time: 2.7481  data: 0.0149  max mem: 8948\n",
            "Epoch: [12] Total time: 0:04:13 (2.8124 s / it)\n",
            "Averaged stats: lr: 0.000000  loss: 3.6179 (3.8147)  loss_ce: 0.0762 (0.0870)  loss_bbox: 0.0439 (0.0474)  loss_giou: 0.3760 (0.3894)  loss_ce_0: 0.0964 (0.1197)  loss_bbox_0: 0.0442 (0.0474)  loss_giou_0: 0.3841 (0.3918)  loss_ce_1: 0.0896 (0.1016)  loss_bbox_1: 0.0443 (0.0475)  loss_giou_1: 0.3810 (0.3905)  loss_ce_2: 0.0806 (0.0972)  loss_bbox_2: 0.0443 (0.0473)  loss_giou_2: 0.3801 (0.3888)  loss_ce_3: 0.0775 (0.0937)  loss_bbox_3: 0.0442 (0.0472)  loss_giou_3: 0.3751 (0.3885)  loss_ce_4: 0.0819 (0.0895)  loss_bbox_4: 0.0438 (0.0472)  loss_giou_4: 0.3760 (0.3896)  loss_ce_interm: 0.1360 (0.1584)  loss_bbox_interm: 0.0461 (0.0486)  loss_giou_interm: 0.3890 (0.3964)  loss_ce_unscaled: 0.0381 (0.0435)  loss_bbox_unscaled: 0.0088 (0.0095)  loss_giou_unscaled: 0.1880 (0.1947)  loss_xy_unscaled: 0.0026 (0.0028)  loss_hw_unscaled: 0.0061 (0.0067)  loss_ce_0_unscaled: 0.0482 (0.0599)  loss_bbox_0_unscaled: 0.0088 (0.0095)  loss_giou_0_unscaled: 0.1921 (0.1959)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0061 (0.0067)  loss_ce_1_unscaled: 0.0448 (0.0508)  loss_bbox_1_unscaled: 0.0089 (0.0095)  loss_giou_1_unscaled: 0.1905 (0.1952)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0062 (0.0067)  loss_ce_2_unscaled: 0.0403 (0.0486)  loss_bbox_2_unscaled: 0.0089 (0.0095)  loss_giou_2_unscaled: 0.1900 (0.1944)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0061 (0.0067)  loss_ce_3_unscaled: 0.0387 (0.0468)  loss_bbox_3_unscaled: 0.0088 (0.0094)  loss_giou_3_unscaled: 0.1876 (0.1943)  loss_xy_3_unscaled: 0.0026 (0.0028)  loss_hw_3_unscaled: 0.0061 (0.0067)  loss_ce_4_unscaled: 0.0410 (0.0447)  loss_bbox_4_unscaled: 0.0088 (0.0094)  loss_giou_4_unscaled: 0.1880 (0.1948)  loss_xy_4_unscaled: 0.0026 (0.0028)  loss_hw_4_unscaled: 0.0061 (0.0067)  loss_ce_interm_unscaled: 0.0680 (0.0792)  loss_bbox_interm_unscaled: 0.0092 (0.0097)  loss_giou_interm_unscaled: 0.1945 (0.1982)  loss_xy_interm_unscaled: 0.0027 (0.0028)  loss_hw_interm_unscaled: 0.0064 (0.0069)\n",
            "Input text prompt: bolt . wrong direction . 1 . 2 . 3 . 4 . 5 .\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Test:  [ 0/10]  eta: 0:00:17    time: 1.7937  data: 0.8513  max mem: 8948\n",
            "Test:  [ 9/10]  eta: 0:00:01    time: 1.0260  data: 0.1017  max mem: 8948\n",
            "Test: Total time: 0:00:10 (1.0356 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.055\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.114\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.042\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.290\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.205\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.006\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.242\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.483\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.462\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.581\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch: [13]  [ 0/90]  eta: 0:04:53  lr: 0.000000  loss: 2.9346 (2.9346)  loss_ce: 0.0420 (0.0420)  loss_bbox: 0.0533 (0.0533)  loss_giou: 0.3046 (0.3046)  loss_ce_0: 0.0567 (0.0567)  loss_bbox_0: 0.0513 (0.0513)  loss_giou_0: 0.3072 (0.3072)  loss_ce_1: 0.0779 (0.0779)  loss_bbox_1: 0.0521 (0.0521)  loss_giou_1: 0.2997 (0.2997)  loss_ce_2: 0.0458 (0.0458)  loss_bbox_2: 0.0537 (0.0537)  loss_giou_2: 0.3096 (0.3096)  loss_ce_3: 0.0470 (0.0470)  loss_bbox_3: 0.0523 (0.0523)  loss_giou_3: 0.3043 (0.3043)  loss_ce_4: 0.0457 (0.0457)  loss_bbox_4: 0.0533 (0.0533)  loss_giou_4: 0.3054 (0.3054)  loss_ce_interm: 0.1092 (0.1092)  loss_bbox_interm: 0.0523 (0.0523)  loss_giou_interm: 0.3114 (0.3114)  loss_ce_unscaled: 0.0210 (0.0210)  loss_bbox_unscaled: 0.0107 (0.0107)  loss_giou_unscaled: 0.1523 (0.1523)  loss_xy_unscaled: 0.0035 (0.0035)  loss_hw_unscaled: 0.0071 (0.0071)  loss_ce_0_unscaled: 0.0283 (0.0283)  loss_bbox_0_unscaled: 0.0103 (0.0103)  loss_giou_0_unscaled: 0.1536 (0.1536)  loss_xy_0_unscaled: 0.0035 (0.0035)  loss_hw_0_unscaled: 0.0068 (0.0068)  loss_ce_1_unscaled: 0.0389 (0.0389)  loss_bbox_1_unscaled: 0.0104 (0.0104)  loss_giou_1_unscaled: 0.1498 (0.1498)  loss_xy_1_unscaled: 0.0036 (0.0036)  loss_hw_1_unscaled: 0.0068 (0.0068)  loss_ce_2_unscaled: 0.0229 (0.0229)  loss_bbox_2_unscaled: 0.0107 (0.0107)  loss_giou_2_unscaled: 0.1548 (0.1548)  loss_xy_2_unscaled: 0.0036 (0.0036)  loss_hw_2_unscaled: 0.0071 (0.0071)  loss_ce_3_unscaled: 0.0235 (0.0235)  loss_bbox_3_unscaled: 0.0105 (0.0105)  loss_giou_3_unscaled: 0.1522 (0.1522)  loss_xy_3_unscaled: 0.0036 (0.0036)  loss_hw_3_unscaled: 0.0069 (0.0069)  loss_ce_4_unscaled: 0.0229 (0.0229)  loss_bbox_4_unscaled: 0.0107 (0.0107)  loss_giou_4_unscaled: 0.1527 (0.1527)  loss_xy_4_unscaled: 0.0035 (0.0035)  loss_hw_4_unscaled: 0.0071 (0.0071)  loss_ce_interm_unscaled: 0.0546 (0.0546)  loss_bbox_interm_unscaled: 0.0105 (0.0105)  loss_giou_interm_unscaled: 0.1557 (0.1557)  loss_xy_interm_unscaled: 0.0036 (0.0036)  loss_hw_interm_unscaled: 0.0069 (0.0069)  time: 3.2623  data: 0.8366  max mem: 8948\n",
            "Epoch: [13]  [10/90]  eta: 0:03:50  lr: 0.000000  loss: 3.7532 (3.7009)  loss_ce: 0.0621 (0.0689)  loss_bbox: 0.0501 (0.0476)  loss_giou: 0.3780 (0.3843)  loss_ce_0: 0.1094 (0.1109)  loss_bbox_0: 0.0513 (0.0483)  loss_giou_0: 0.3953 (0.3881)  loss_ce_1: 0.0914 (0.0958)  loss_bbox_1: 0.0476 (0.0475)  loss_giou_1: 0.3934 (0.3860)  loss_ce_2: 0.0792 (0.0874)  loss_bbox_2: 0.0484 (0.0475)  loss_giou_2: 0.3820 (0.3823)  loss_ce_3: 0.0701 (0.0776)  loss_bbox_3: 0.0486 (0.0474)  loss_giou_3: 0.3799 (0.3838)  loss_ce_4: 0.0641 (0.0759)  loss_bbox_4: 0.0503 (0.0476)  loss_giou_4: 0.3779 (0.3843)  loss_ce_interm: 0.1615 (0.1512)  loss_bbox_interm: 0.0493 (0.0488)  loss_giou_interm: 0.4029 (0.3897)  loss_ce_unscaled: 0.0311 (0.0345)  loss_bbox_unscaled: 0.0100 (0.0095)  loss_giou_unscaled: 0.1890 (0.1921)  loss_xy_unscaled: 0.0029 (0.0029)  loss_hw_unscaled: 0.0070 (0.0066)  loss_ce_0_unscaled: 0.0547 (0.0555)  loss_bbox_0_unscaled: 0.0103 (0.0097)  loss_giou_0_unscaled: 0.1976 (0.1941)  loss_xy_0_unscaled: 0.0030 (0.0030)  loss_hw_0_unscaled: 0.0068 (0.0067)  loss_ce_1_unscaled: 0.0457 (0.0479)  loss_bbox_1_unscaled: 0.0095 (0.0095)  loss_giou_1_unscaled: 0.1967 (0.1930)  loss_xy_1_unscaled: 0.0028 (0.0029)  loss_hw_1_unscaled: 0.0068 (0.0066)  loss_ce_2_unscaled: 0.0396 (0.0437)  loss_bbox_2_unscaled: 0.0097 (0.0095)  loss_giou_2_unscaled: 0.1910 (0.1912)  loss_xy_2_unscaled: 0.0028 (0.0029)  loss_hw_2_unscaled: 0.0070 (0.0066)  loss_ce_3_unscaled: 0.0351 (0.0388)  loss_bbox_3_unscaled: 0.0097 (0.0095)  loss_giou_3_unscaled: 0.1900 (0.1919)  loss_xy_3_unscaled: 0.0028 (0.0029)  loss_hw_3_unscaled: 0.0069 (0.0066)  loss_ce_4_unscaled: 0.0321 (0.0380)  loss_bbox_4_unscaled: 0.0101 (0.0095)  loss_giou_4_unscaled: 0.1890 (0.1921)  loss_xy_4_unscaled: 0.0029 (0.0029)  loss_hw_4_unscaled: 0.0070 (0.0066)  loss_ce_interm_unscaled: 0.0807 (0.0756)  loss_bbox_interm_unscaled: 0.0099 (0.0098)  loss_giou_interm_unscaled: 0.2014 (0.1948)  loss_xy_interm_unscaled: 0.0030 (0.0030)  loss_hw_interm_unscaled: 0.0069 (0.0067)  time: 2.8764  data: 0.0907  max mem: 8948\n",
            "Epoch: [13]  [20/90]  eta: 0:03:16  lr: 0.000000  loss: 3.7386 (3.7119)  loss_ce: 0.0623 (0.0760)  loss_bbox: 0.0442 (0.0464)  loss_giou: 0.3734 (0.3804)  loss_ce_0: 0.1094 (0.1135)  loss_bbox_0: 0.0462 (0.0471)  loss_giou_0: 0.3898 (0.3853)  loss_ce_1: 0.0914 (0.1025)  loss_bbox_1: 0.0438 (0.0461)  loss_giou_1: 0.3843 (0.3811)  loss_ce_2: 0.0792 (0.0907)  loss_bbox_2: 0.0440 (0.0462)  loss_giou_2: 0.3697 (0.3799)  loss_ce_3: 0.0714 (0.0870)  loss_bbox_3: 0.0433 (0.0462)  loss_giou_3: 0.3778 (0.3804)  loss_ce_4: 0.0659 (0.0839)  loss_bbox_4: 0.0442 (0.0463)  loss_giou_4: 0.3727 (0.3794)  loss_ce_interm: 0.1590 (0.1557)  loss_bbox_interm: 0.0470 (0.0480)  loss_giou_interm: 0.3876 (0.3900)  loss_ce_unscaled: 0.0312 (0.0380)  loss_bbox_unscaled: 0.0088 (0.0093)  loss_giou_unscaled: 0.1867 (0.1902)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0061 (0.0065)  loss_ce_0_unscaled: 0.0547 (0.0567)  loss_bbox_0_unscaled: 0.0092 (0.0094)  loss_giou_0_unscaled: 0.1949 (0.1927)  loss_xy_0_unscaled: 0.0028 (0.0029)  loss_hw_0_unscaled: 0.0065 (0.0066)  loss_ce_1_unscaled: 0.0457 (0.0512)  loss_bbox_1_unscaled: 0.0088 (0.0092)  loss_giou_1_unscaled: 0.1921 (0.1905)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0062 (0.0064)  loss_ce_2_unscaled: 0.0396 (0.0453)  loss_bbox_2_unscaled: 0.0088 (0.0092)  loss_giou_2_unscaled: 0.1849 (0.1900)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0060 (0.0065)  loss_ce_3_unscaled: 0.0357 (0.0435)  loss_bbox_3_unscaled: 0.0087 (0.0092)  loss_giou_3_unscaled: 0.1889 (0.1902)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0059 (0.0064)  loss_ce_4_unscaled: 0.0329 (0.0419)  loss_bbox_4_unscaled: 0.0088 (0.0093)  loss_giou_4_unscaled: 0.1863 (0.1897)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0061 (0.0065)  loss_ce_interm_unscaled: 0.0795 (0.0779)  loss_bbox_interm_unscaled: 0.0094 (0.0096)  loss_giou_interm_unscaled: 0.1938 (0.1950)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0066 (0.0067)  time: 2.7832  data: 0.0148  max mem: 8948\n",
            "Epoch: [13]  [30/90]  eta: 0:02:53  lr: 0.000000  loss: 3.5985 (3.6798)  loss_ce: 0.0653 (0.0762)  loss_bbox: 0.0442 (0.0454)  loss_giou: 0.3645 (0.3781)  loss_ce_0: 0.1017 (0.1118)  loss_bbox_0: 0.0454 (0.0460)  loss_giou_0: 0.3684 (0.3818)  loss_ce_1: 0.0828 (0.1011)  loss_bbox_1: 0.0438 (0.0453)  loss_giou_1: 0.3638 (0.3788)  loss_ce_2: 0.0773 (0.0896)  loss_bbox_2: 0.0440 (0.0453)  loss_giou_2: 0.3649 (0.3780)  loss_ce_3: 0.0721 (0.0854)  loss_bbox_3: 0.0433 (0.0453)  loss_giou_3: 0.3625 (0.3776)  loss_ce_4: 0.0687 (0.0820)  loss_bbox_4: 0.0442 (0.0454)  loss_giou_4: 0.3637 (0.3772)  loss_ce_interm: 0.1506 (0.1548)  loss_bbox_interm: 0.0466 (0.0469)  loss_giou_interm: 0.3761 (0.3879)  loss_ce_unscaled: 0.0326 (0.0381)  loss_bbox_unscaled: 0.0088 (0.0091)  loss_giou_unscaled: 0.1823 (0.1890)  loss_xy_unscaled: 0.0026 (0.0027)  loss_hw_unscaled: 0.0063 (0.0064)  loss_ce_0_unscaled: 0.0508 (0.0559)  loss_bbox_0_unscaled: 0.0091 (0.0092)  loss_giou_0_unscaled: 0.1842 (0.1909)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0064 (0.0064)  loss_ce_1_unscaled: 0.0414 (0.0505)  loss_bbox_1_unscaled: 0.0088 (0.0091)  loss_giou_1_unscaled: 0.1819 (0.1894)  loss_xy_1_unscaled: 0.0026 (0.0027)  loss_hw_1_unscaled: 0.0063 (0.0063)  loss_ce_2_unscaled: 0.0386 (0.0448)  loss_bbox_2_unscaled: 0.0088 (0.0091)  loss_giou_2_unscaled: 0.1825 (0.1890)  loss_xy_2_unscaled: 0.0026 (0.0027)  loss_hw_2_unscaled: 0.0063 (0.0063)  loss_ce_3_unscaled: 0.0361 (0.0427)  loss_bbox_3_unscaled: 0.0087 (0.0091)  loss_giou_3_unscaled: 0.1813 (0.1888)  loss_xy_3_unscaled: 0.0026 (0.0027)  loss_hw_3_unscaled: 0.0063 (0.0063)  loss_ce_4_unscaled: 0.0343 (0.0410)  loss_bbox_4_unscaled: 0.0088 (0.0091)  loss_giou_4_unscaled: 0.1819 (0.1886)  loss_xy_4_unscaled: 0.0026 (0.0027)  loss_hw_4_unscaled: 0.0063 (0.0064)  loss_ce_interm_unscaled: 0.0753 (0.0774)  loss_bbox_interm_unscaled: 0.0093 (0.0094)  loss_giou_interm_unscaled: 0.1881 (0.1940)  loss_xy_interm_unscaled: 0.0027 (0.0028)  loss_hw_interm_unscaled: 0.0066 (0.0065)  time: 2.8989  data: 0.0145  max mem: 8948\n",
            "Epoch: [13]  [40/90]  eta: 0:02:23  lr: 0.000000  loss: 3.7670 (3.7474)  loss_ce: 0.0650 (0.0776)  loss_bbox: 0.0467 (0.0464)  loss_giou: 0.3752 (0.3846)  loss_ce_0: 0.1087 (0.1150)  loss_bbox_0: 0.0466 (0.0469)  loss_giou_0: 0.3803 (0.3885)  loss_ce_1: 0.0933 (0.1026)  loss_bbox_1: 0.0463 (0.0463)  loss_giou_1: 0.3808 (0.3858)  loss_ce_2: 0.0837 (0.0920)  loss_bbox_2: 0.0465 (0.0461)  loss_giou_2: 0.3814 (0.3844)  loss_ce_3: 0.0727 (0.0874)  loss_bbox_3: 0.0462 (0.0462)  loss_giou_3: 0.3775 (0.3845)  loss_ce_4: 0.0656 (0.0829)  loss_bbox_4: 0.0467 (0.0463)  loss_giou_4: 0.3768 (0.3841)  loss_ce_interm: 0.1506 (0.1575)  loss_bbox_interm: 0.0468 (0.0477)  loss_giou_interm: 0.3931 (0.3946)  loss_ce_unscaled: 0.0325 (0.0388)  loss_bbox_unscaled: 0.0093 (0.0093)  loss_giou_unscaled: 0.1876 (0.1923)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0065 (0.0065)  loss_ce_0_unscaled: 0.0543 (0.0575)  loss_bbox_0_unscaled: 0.0093 (0.0094)  loss_giou_0_unscaled: 0.1901 (0.1942)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0066 (0.0066)  loss_ce_1_unscaled: 0.0466 (0.0513)  loss_bbox_1_unscaled: 0.0093 (0.0093)  loss_giou_1_unscaled: 0.1904 (0.1929)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0064 (0.0065)  loss_ce_2_unscaled: 0.0418 (0.0460)  loss_bbox_2_unscaled: 0.0093 (0.0092)  loss_giou_2_unscaled: 0.1907 (0.1922)  loss_xy_2_unscaled: 0.0028 (0.0027)  loss_hw_2_unscaled: 0.0065 (0.0065)  loss_ce_3_unscaled: 0.0364 (0.0437)  loss_bbox_3_unscaled: 0.0092 (0.0092)  loss_giou_3_unscaled: 0.1888 (0.1923)  loss_xy_3_unscaled: 0.0028 (0.0027)  loss_hw_3_unscaled: 0.0065 (0.0065)  loss_ce_4_unscaled: 0.0328 (0.0415)  loss_bbox_4_unscaled: 0.0093 (0.0093)  loss_giou_4_unscaled: 0.1884 (0.1921)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0065 (0.0065)  loss_ce_interm_unscaled: 0.0753 (0.0787)  loss_bbox_interm_unscaled: 0.0094 (0.0095)  loss_giou_interm_unscaled: 0.1966 (0.1973)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0066 (0.0067)  time: 2.9540  data: 0.0157  max mem: 8948\n",
            "Epoch: [13]  [50/90]  eta: 0:01:55  lr: 0.000000  loss: 4.0126 (3.7909)  loss_ce: 0.0796 (0.0822)  loss_bbox: 0.0487 (0.0469)  loss_giou: 0.3841 (0.3858)  loss_ce_0: 0.1135 (0.1203)  loss_bbox_0: 0.0478 (0.0473)  loss_giou_0: 0.3945 (0.3884)  loss_ce_1: 0.1004 (0.1086)  loss_bbox_1: 0.0475 (0.0467)  loss_giou_1: 0.3887 (0.3862)  loss_ce_2: 0.0949 (0.0972)  loss_bbox_2: 0.0478 (0.0467)  loss_giou_2: 0.3930 (0.3856)  loss_ce_3: 0.0886 (0.0921)  loss_bbox_3: 0.0482 (0.0467)  loss_giou_3: 0.3821 (0.3853)  loss_ce_4: 0.0815 (0.0881)  loss_bbox_4: 0.0480 (0.0468)  loss_giou_4: 0.3823 (0.3849)  loss_ce_interm: 0.1591 (0.1621)  loss_bbox_interm: 0.0496 (0.0481)  loss_giou_interm: 0.3967 (0.3951)  loss_ce_unscaled: 0.0398 (0.0411)  loss_bbox_unscaled: 0.0097 (0.0094)  loss_giou_unscaled: 0.1921 (0.1929)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0066 (0.0066)  loss_ce_0_unscaled: 0.0568 (0.0601)  loss_bbox_0_unscaled: 0.0096 (0.0095)  loss_giou_0_unscaled: 0.1973 (0.1942)  loss_xy_0_unscaled: 0.0029 (0.0028)  loss_hw_0_unscaled: 0.0067 (0.0066)  loss_ce_1_unscaled: 0.0502 (0.0543)  loss_bbox_1_unscaled: 0.0095 (0.0093)  loss_giou_1_unscaled: 0.1943 (0.1931)  loss_xy_1_unscaled: 0.0029 (0.0028)  loss_hw_1_unscaled: 0.0066 (0.0066)  loss_ce_2_unscaled: 0.0475 (0.0486)  loss_bbox_2_unscaled: 0.0096 (0.0093)  loss_giou_2_unscaled: 0.1965 (0.1928)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0066 (0.0066)  loss_ce_3_unscaled: 0.0443 (0.0460)  loss_bbox_3_unscaled: 0.0096 (0.0093)  loss_giou_3_unscaled: 0.1910 (0.1927)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0067 (0.0066)  loss_ce_4_unscaled: 0.0408 (0.0441)  loss_bbox_4_unscaled: 0.0096 (0.0094)  loss_giou_4_unscaled: 0.1912 (0.1924)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0066 (0.0066)  loss_ce_interm_unscaled: 0.0795 (0.0810)  loss_bbox_interm_unscaled: 0.0099 (0.0096)  loss_giou_interm_unscaled: 0.1984 (0.1975)  loss_xy_interm_unscaled: 0.0030 (0.0029)  loss_hw_interm_unscaled: 0.0069 (0.0067)  time: 2.9085  data: 0.0172  max mem: 8948\n",
            "Epoch: [13]  [60/90]  eta: 0:01:26  lr: 0.000000  loss: 3.9396 (3.8073)  loss_ce: 0.0820 (0.0833)  loss_bbox: 0.0481 (0.0474)  loss_giou: 0.3804 (0.3863)  loss_ce_0: 0.1218 (0.1209)  loss_bbox_0: 0.0491 (0.0482)  loss_giou_0: 0.3854 (0.3891)  loss_ce_1: 0.1144 (0.1093)  loss_bbox_1: 0.0475 (0.0476)  loss_giou_1: 0.3808 (0.3872)  loss_ce_2: 0.1012 (0.0978)  loss_bbox_2: 0.0488 (0.0474)  loss_giou_2: 0.3926 (0.3864)  loss_ce_3: 0.0886 (0.0923)  loss_bbox_3: 0.0482 (0.0474)  loss_giou_3: 0.3762 (0.3862)  loss_ce_4: 0.0873 (0.0890)  loss_bbox_4: 0.0475 (0.0474)  loss_giou_4: 0.3766 (0.3855)  loss_ce_interm: 0.1615 (0.1641)  loss_bbox_interm: 0.0508 (0.0489)  loss_giou_interm: 0.3932 (0.3955)  loss_ce_unscaled: 0.0410 (0.0417)  loss_bbox_unscaled: 0.0096 (0.0095)  loss_giou_unscaled: 0.1902 (0.1932)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0065 (0.0067)  loss_ce_0_unscaled: 0.0609 (0.0604)  loss_bbox_0_unscaled: 0.0098 (0.0096)  loss_giou_0_unscaled: 0.1927 (0.1945)  loss_xy_0_unscaled: 0.0028 (0.0029)  loss_hw_0_unscaled: 0.0067 (0.0068)  loss_ce_1_unscaled: 0.0572 (0.0546)  loss_bbox_1_unscaled: 0.0095 (0.0095)  loss_giou_1_unscaled: 0.1904 (0.1936)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0066 (0.0067)  loss_ce_2_unscaled: 0.0506 (0.0489)  loss_bbox_2_unscaled: 0.0098 (0.0095)  loss_giou_2_unscaled: 0.1963 (0.1932)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0066 (0.0067)  loss_ce_3_unscaled: 0.0443 (0.0462)  loss_bbox_3_unscaled: 0.0096 (0.0095)  loss_giou_3_unscaled: 0.1881 (0.1931)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0065 (0.0067)  loss_ce_4_unscaled: 0.0437 (0.0445)  loss_bbox_4_unscaled: 0.0095 (0.0095)  loss_giou_4_unscaled: 0.1883 (0.1927)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0065 (0.0067)  loss_ce_interm_unscaled: 0.0808 (0.0820)  loss_bbox_interm_unscaled: 0.0102 (0.0098)  loss_giou_interm_unscaled: 0.1966 (0.1978)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0071 (0.0069)  time: 2.8575  data: 0.0164  max mem: 8948\n",
            "Epoch: [13]  [70/90]  eta: 0:00:58  lr: 0.000000  loss: 3.6372 (3.7838)  loss_ce: 0.0759 (0.0828)  loss_bbox: 0.0473 (0.0470)  loss_giou: 0.3738 (0.3849)  loss_ce_0: 0.1136 (0.1195)  loss_bbox_0: 0.0472 (0.0478)  loss_giou_0: 0.3717 (0.3870)  loss_ce_1: 0.0942 (0.1069)  loss_bbox_1: 0.0478 (0.0473)  loss_giou_1: 0.3647 (0.3860)  loss_ce_2: 0.0845 (0.0963)  loss_bbox_2: 0.0491 (0.0471)  loss_giou_2: 0.3741 (0.3852)  loss_ce_3: 0.0773 (0.0909)  loss_bbox_3: 0.0476 (0.0471)  loss_giou_3: 0.3729 (0.3850)  loss_ce_4: 0.0780 (0.0878)  loss_bbox_4: 0.0475 (0.0471)  loss_giou_4: 0.3747 (0.3842)  loss_ce_interm: 0.1583 (0.1616)  loss_bbox_interm: 0.0491 (0.0486)  loss_giou_interm: 0.3800 (0.3937)  loss_ce_unscaled: 0.0380 (0.0414)  loss_bbox_unscaled: 0.0095 (0.0094)  loss_giou_unscaled: 0.1869 (0.1925)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0065 (0.0066)  loss_ce_0_unscaled: 0.0568 (0.0597)  loss_bbox_0_unscaled: 0.0094 (0.0096)  loss_giou_0_unscaled: 0.1859 (0.1935)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0066 (0.0067)  loss_ce_1_unscaled: 0.0471 (0.0535)  loss_bbox_1_unscaled: 0.0096 (0.0095)  loss_giou_1_unscaled: 0.1823 (0.1930)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0065 (0.0067)  loss_ce_2_unscaled: 0.0423 (0.0481)  loss_bbox_2_unscaled: 0.0098 (0.0094)  loss_giou_2_unscaled: 0.1871 (0.1926)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0065 (0.0066)  loss_ce_3_unscaled: 0.0387 (0.0455)  loss_bbox_3_unscaled: 0.0095 (0.0094)  loss_giou_3_unscaled: 0.1865 (0.1925)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0064 (0.0066)  loss_ce_4_unscaled: 0.0390 (0.0439)  loss_bbox_4_unscaled: 0.0095 (0.0094)  loss_giou_4_unscaled: 0.1874 (0.1921)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0066 (0.0066)  loss_ce_interm_unscaled: 0.0791 (0.0808)  loss_bbox_interm_unscaled: 0.0098 (0.0097)  loss_giou_interm_unscaled: 0.1900 (0.1968)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0067 (0.0068)  time: 2.9113  data: 0.0150  max mem: 8948\n",
            "Epoch: [13]  [80/90]  eta: 0:00:29  lr: 0.000000  loss: 3.4759 (3.7675)  loss_ce: 0.0653 (0.0817)  loss_bbox: 0.0458 (0.0470)  loss_giou: 0.3738 (0.3844)  loss_ce_0: 0.1037 (0.1183)  loss_bbox_0: 0.0457 (0.0477)  loss_giou_0: 0.3734 (0.3863)  loss_ce_1: 0.0814 (0.1052)  loss_bbox_1: 0.0457 (0.0473)  loss_giou_1: 0.3712 (0.3856)  loss_ce_2: 0.0772 (0.0950)  loss_bbox_2: 0.0457 (0.0470)  loss_giou_2: 0.3718 (0.3845)  loss_ce_3: 0.0765 (0.0895)  loss_bbox_3: 0.0458 (0.0470)  loss_giou_3: 0.3692 (0.3842)  loss_ce_4: 0.0663 (0.0864)  loss_bbox_4: 0.0457 (0.0470)  loss_giou_4: 0.3747 (0.3836)  loss_ce_interm: 0.1352 (0.1589)  loss_bbox_interm: 0.0455 (0.0485)  loss_giou_interm: 0.3736 (0.3924)  loss_ce_unscaled: 0.0327 (0.0409)  loss_bbox_unscaled: 0.0092 (0.0094)  loss_giou_unscaled: 0.1869 (0.1922)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0065 (0.0066)  loss_ce_0_unscaled: 0.0518 (0.0592)  loss_bbox_0_unscaled: 0.0091 (0.0095)  loss_giou_0_unscaled: 0.1867 (0.1932)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0065 (0.0067)  loss_ce_1_unscaled: 0.0407 (0.0526)  loss_bbox_1_unscaled: 0.0091 (0.0095)  loss_giou_1_unscaled: 0.1856 (0.1928)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0065 (0.0067)  loss_ce_2_unscaled: 0.0386 (0.0475)  loss_bbox_2_unscaled: 0.0091 (0.0094)  loss_giou_2_unscaled: 0.1859 (0.1922)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0064 (0.0066)  loss_ce_3_unscaled: 0.0382 (0.0448)  loss_bbox_3_unscaled: 0.0092 (0.0094)  loss_giou_3_unscaled: 0.1846 (0.1921)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0064 (0.0066)  loss_ce_4_unscaled: 0.0332 (0.0432)  loss_bbox_4_unscaled: 0.0091 (0.0094)  loss_giou_4_unscaled: 0.1874 (0.1918)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0065 (0.0066)  loss_ce_interm_unscaled: 0.0676 (0.0794)  loss_bbox_interm_unscaled: 0.0091 (0.0097)  loss_giou_interm_unscaled: 0.1868 (0.1962)  loss_xy_interm_unscaled: 0.0027 (0.0029)  loss_hw_interm_unscaled: 0.0063 (0.0068)  time: 2.9973  data: 0.0157  max mem: 8948\n",
            "Epoch: [13]  [89/90]  eta: 0:00:02  lr: 0.000000  loss: 3.6515 (3.7685)  loss_ce: 0.0721 (0.0830)  loss_bbox: 0.0468 (0.0469)  loss_giou: 0.3767 (0.3845)  loss_ce_0: 0.1010 (0.1183)  loss_bbox_0: 0.0459 (0.0475)  loss_giou_0: 0.3740 (0.3866)  loss_ce_1: 0.0831 (0.1049)  loss_bbox_1: 0.0469 (0.0471)  loss_giou_1: 0.3742 (0.3857)  loss_ce_2: 0.0829 (0.0950)  loss_bbox_2: 0.0465 (0.0468)  loss_giou_2: 0.3764 (0.3845)  loss_ce_3: 0.0840 (0.0903)  loss_bbox_3: 0.0466 (0.0468)  loss_giou_3: 0.3762 (0.3841)  loss_ce_4: 0.0782 (0.0869)  loss_bbox_4: 0.0467 (0.0469)  loss_giou_4: 0.3776 (0.3839)  loss_ce_interm: 0.1331 (0.1579)  loss_bbox_interm: 0.0455 (0.0483)  loss_giou_interm: 0.3861 (0.3925)  loss_ce_unscaled: 0.0360 (0.0415)  loss_bbox_unscaled: 0.0094 (0.0094)  loss_giou_unscaled: 0.1883 (0.1922)  loss_xy_unscaled: 0.0026 (0.0028)  loss_hw_unscaled: 0.0067 (0.0066)  loss_ce_0_unscaled: 0.0505 (0.0592)  loss_bbox_0_unscaled: 0.0092 (0.0095)  loss_giou_0_unscaled: 0.1870 (0.1933)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0067 (0.0067)  loss_ce_1_unscaled: 0.0415 (0.0525)  loss_bbox_1_unscaled: 0.0094 (0.0094)  loss_giou_1_unscaled: 0.1871 (0.1928)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0067 (0.0067)  loss_ce_2_unscaled: 0.0414 (0.0475)  loss_bbox_2_unscaled: 0.0093 (0.0094)  loss_giou_2_unscaled: 0.1882 (0.1923)  loss_xy_2_unscaled: 0.0026 (0.0027)  loss_hw_2_unscaled: 0.0067 (0.0066)  loss_ce_3_unscaled: 0.0420 (0.0452)  loss_bbox_3_unscaled: 0.0093 (0.0094)  loss_giou_3_unscaled: 0.1881 (0.1920)  loss_xy_3_unscaled: 0.0026 (0.0027)  loss_hw_3_unscaled: 0.0067 (0.0066)  loss_ce_4_unscaled: 0.0391 (0.0435)  loss_bbox_4_unscaled: 0.0093 (0.0094)  loss_giou_4_unscaled: 0.1888 (0.1919)  loss_xy_4_unscaled: 0.0026 (0.0028)  loss_hw_4_unscaled: 0.0067 (0.0066)  loss_ce_interm_unscaled: 0.0665 (0.0789)  loss_bbox_interm_unscaled: 0.0091 (0.0097)  loss_giou_interm_unscaled: 0.1930 (0.1963)  loss_xy_interm_unscaled: 0.0026 (0.0029)  loss_hw_interm_unscaled: 0.0065 (0.0068)  time: 2.9271  data: 0.0153  max mem: 8948\n",
            "Epoch: [13] Total time: 0:04:21 (2.9082 s / it)\n",
            "Averaged stats: lr: 0.000000  loss: 3.6515 (3.7685)  loss_ce: 0.0721 (0.0830)  loss_bbox: 0.0468 (0.0469)  loss_giou: 0.3767 (0.3845)  loss_ce_0: 0.1010 (0.1183)  loss_bbox_0: 0.0459 (0.0475)  loss_giou_0: 0.3740 (0.3866)  loss_ce_1: 0.0831 (0.1049)  loss_bbox_1: 0.0469 (0.0471)  loss_giou_1: 0.3742 (0.3857)  loss_ce_2: 0.0829 (0.0950)  loss_bbox_2: 0.0465 (0.0468)  loss_giou_2: 0.3764 (0.3845)  loss_ce_3: 0.0840 (0.0903)  loss_bbox_3: 0.0466 (0.0468)  loss_giou_3: 0.3762 (0.3841)  loss_ce_4: 0.0782 (0.0869)  loss_bbox_4: 0.0467 (0.0469)  loss_giou_4: 0.3776 (0.3839)  loss_ce_interm: 0.1331 (0.1579)  loss_bbox_interm: 0.0455 (0.0483)  loss_giou_interm: 0.3861 (0.3925)  loss_ce_unscaled: 0.0360 (0.0415)  loss_bbox_unscaled: 0.0094 (0.0094)  loss_giou_unscaled: 0.1883 (0.1922)  loss_xy_unscaled: 0.0026 (0.0028)  loss_hw_unscaled: 0.0067 (0.0066)  loss_ce_0_unscaled: 0.0505 (0.0592)  loss_bbox_0_unscaled: 0.0092 (0.0095)  loss_giou_0_unscaled: 0.1870 (0.1933)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0067 (0.0067)  loss_ce_1_unscaled: 0.0415 (0.0525)  loss_bbox_1_unscaled: 0.0094 (0.0094)  loss_giou_1_unscaled: 0.1871 (0.1928)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0067 (0.0067)  loss_ce_2_unscaled: 0.0414 (0.0475)  loss_bbox_2_unscaled: 0.0093 (0.0094)  loss_giou_2_unscaled: 0.1882 (0.1923)  loss_xy_2_unscaled: 0.0026 (0.0027)  loss_hw_2_unscaled: 0.0067 (0.0066)  loss_ce_3_unscaled: 0.0420 (0.0452)  loss_bbox_3_unscaled: 0.0093 (0.0094)  loss_giou_3_unscaled: 0.1881 (0.1920)  loss_xy_3_unscaled: 0.0026 (0.0027)  loss_hw_3_unscaled: 0.0067 (0.0066)  loss_ce_4_unscaled: 0.0391 (0.0435)  loss_bbox_4_unscaled: 0.0093 (0.0094)  loss_giou_4_unscaled: 0.1888 (0.1919)  loss_xy_4_unscaled: 0.0026 (0.0028)  loss_hw_4_unscaled: 0.0067 (0.0066)  loss_ce_interm_unscaled: 0.0665 (0.0789)  loss_bbox_interm_unscaled: 0.0091 (0.0097)  loss_giou_interm_unscaled: 0.1930 (0.1963)  loss_xy_interm_unscaled: 0.0026 (0.0029)  loss_hw_interm_unscaled: 0.0065 (0.0068)\n",
            "Input text prompt: bolt . wrong direction . 1 . 2 . 3 . 4 . 5 .\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Test:  [ 0/10]  eta: 0:00:18    time: 1.8531  data: 0.9278  max mem: 8948\n",
            "Test:  [ 9/10]  eta: 0:00:01    time: 1.0305  data: 0.1064  max mem: 8948\n",
            "Test: Total time: 0:00:10 (1.0402 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.055\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.114\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.041\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.298\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.218\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.005\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.237\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.484\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.460\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.594\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.399\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Epoch: [14]  [ 0/90]  eta: 0:06:34  lr: 0.000000  loss: 3.1756 (3.1756)  loss_ce: 0.0347 (0.0347)  loss_bbox: 0.0428 (0.0428)  loss_giou: 0.3541 (0.3541)  loss_ce_0: 0.0677 (0.0677)  loss_bbox_0: 0.0430 (0.0430)  loss_giou_0: 0.3532 (0.3532)  loss_ce_1: 0.0575 (0.0575)  loss_bbox_1: 0.0430 (0.0430)  loss_giou_1: 0.3534 (0.3534)  loss_ce_2: 0.0485 (0.0485)  loss_bbox_2: 0.0430 (0.0430)  loss_giou_2: 0.3524 (0.3524)  loss_ce_3: 0.0428 (0.0428)  loss_bbox_3: 0.0426 (0.0426)  loss_giou_3: 0.3506 (0.3506)  loss_ce_4: 0.0401 (0.0401)  loss_bbox_4: 0.0427 (0.0427)  loss_giou_4: 0.3523 (0.3523)  loss_ce_interm: 0.1275 (0.1275)  loss_bbox_interm: 0.0439 (0.0439)  loss_giou_interm: 0.3398 (0.3398)  loss_ce_unscaled: 0.0174 (0.0174)  loss_bbox_unscaled: 0.0086 (0.0086)  loss_giou_unscaled: 0.1771 (0.1771)  loss_xy_unscaled: 0.0029 (0.0029)  loss_hw_unscaled: 0.0057 (0.0057)  loss_ce_0_unscaled: 0.0338 (0.0338)  loss_bbox_0_unscaled: 0.0086 (0.0086)  loss_giou_0_unscaled: 0.1766 (0.1766)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0058 (0.0058)  loss_ce_1_unscaled: 0.0288 (0.0288)  loss_bbox_1_unscaled: 0.0086 (0.0086)  loss_giou_1_unscaled: 0.1767 (0.1767)  loss_xy_1_unscaled: 0.0029 (0.0029)  loss_hw_1_unscaled: 0.0057 (0.0057)  loss_ce_2_unscaled: 0.0243 (0.0243)  loss_bbox_2_unscaled: 0.0086 (0.0086)  loss_giou_2_unscaled: 0.1762 (0.1762)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0058 (0.0058)  loss_ce_3_unscaled: 0.0214 (0.0214)  loss_bbox_3_unscaled: 0.0085 (0.0085)  loss_giou_3_unscaled: 0.1753 (0.1753)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0057 (0.0057)  loss_ce_4_unscaled: 0.0200 (0.0200)  loss_bbox_4_unscaled: 0.0085 (0.0085)  loss_giou_4_unscaled: 0.1761 (0.1761)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0057 (0.0057)  loss_ce_interm_unscaled: 0.0637 (0.0637)  loss_bbox_interm_unscaled: 0.0088 (0.0088)  loss_giou_interm_unscaled: 0.1699 (0.1699)  loss_xy_interm_unscaled: 0.0027 (0.0027)  loss_hw_interm_unscaled: 0.0061 (0.0061)  time: 4.3789  data: 0.9091  max mem: 8948\n",
            "Epoch: [14]  [10/90]  eta: 0:03:50  lr: 0.000000  loss: 3.4644 (3.7824)  loss_ce: 0.0910 (0.0914)  loss_bbox: 0.0448 (0.0480)  loss_giou: 0.3541 (0.3732)  loss_ce_0: 0.1340 (0.1374)  loss_bbox_0: 0.0430 (0.0477)  loss_giou_0: 0.3532 (0.3733)  loss_ce_1: 0.1074 (0.1164)  loss_bbox_1: 0.0433 (0.0475)  loss_giou_1: 0.3534 (0.3739)  loss_ce_2: 0.1025 (0.1066)  loss_bbox_2: 0.0434 (0.0481)  loss_giou_2: 0.3529 (0.3750)  loss_ce_3: 0.0856 (0.0965)  loss_bbox_3: 0.0429 (0.0478)  loss_giou_3: 0.3522 (0.3756)  loss_ce_4: 0.0767 (0.0954)  loss_bbox_4: 0.0449 (0.0480)  loss_giou_4: 0.3523 (0.3742)  loss_ce_interm: 0.1621 (0.1808)  loss_bbox_interm: 0.0458 (0.0485)  loss_giou_interm: 0.3557 (0.3772)  loss_ce_unscaled: 0.0455 (0.0457)  loss_bbox_unscaled: 0.0090 (0.0096)  loss_giou_unscaled: 0.1771 (0.1866)  loss_xy_unscaled: 0.0027 (0.0029)  loss_hw_unscaled: 0.0063 (0.0067)  loss_ce_0_unscaled: 0.0670 (0.0687)  loss_bbox_0_unscaled: 0.0086 (0.0095)  loss_giou_0_unscaled: 0.1766 (0.1867)  loss_xy_0_unscaled: 0.0027 (0.0029)  loss_hw_0_unscaled: 0.0060 (0.0066)  loss_ce_1_unscaled: 0.0537 (0.0582)  loss_bbox_1_unscaled: 0.0087 (0.0095)  loss_giou_1_unscaled: 0.1767 (0.1869)  loss_xy_1_unscaled: 0.0027 (0.0029)  loss_hw_1_unscaled: 0.0059 (0.0066)  loss_ce_2_unscaled: 0.0513 (0.0533)  loss_bbox_2_unscaled: 0.0087 (0.0096)  loss_giou_2_unscaled: 0.1764 (0.1875)  loss_xy_2_unscaled: 0.0028 (0.0030)  loss_hw_2_unscaled: 0.0061 (0.0067)  loss_ce_3_unscaled: 0.0428 (0.0482)  loss_bbox_3_unscaled: 0.0086 (0.0096)  loss_giou_3_unscaled: 0.1761 (0.1878)  loss_xy_3_unscaled: 0.0028 (0.0030)  loss_hw_3_unscaled: 0.0060 (0.0066)  loss_ce_4_unscaled: 0.0384 (0.0477)  loss_bbox_4_unscaled: 0.0090 (0.0096)  loss_giou_4_unscaled: 0.1761 (0.1871)  loss_xy_4_unscaled: 0.0027 (0.0029)  loss_hw_4_unscaled: 0.0063 (0.0067)  loss_ce_interm_unscaled: 0.0810 (0.0904)  loss_bbox_interm_unscaled: 0.0092 (0.0097)  loss_giou_interm_unscaled: 0.1779 (0.1886)  loss_xy_interm_unscaled: 0.0028 (0.0030)  loss_hw_interm_unscaled: 0.0062 (0.0067)  time: 2.8776  data: 0.0938  max mem: 8948\n",
            "Epoch: [14]  [20/90]  eta: 0:03:14  lr: 0.000000  loss: 3.6139 (3.7682)  loss_ce: 0.0763 (0.0863)  loss_bbox: 0.0462 (0.0473)  loss_giou: 0.3723 (0.3798)  loss_ce_0: 0.1141 (0.1259)  loss_bbox_0: 0.0463 (0.0471)  loss_giou_0: 0.3667 (0.3789)  loss_ce_1: 0.1000 (0.1081)  loss_bbox_1: 0.0466 (0.0470)  loss_giou_1: 0.3610 (0.3814)  loss_ce_2: 0.1025 (0.1006)  loss_bbox_2: 0.0470 (0.0473)  loss_giou_2: 0.3701 (0.3811)  loss_ce_3: 0.0856 (0.0902)  loss_bbox_3: 0.0464 (0.0472)  loss_giou_3: 0.3751 (0.3825)  loss_ce_4: 0.0767 (0.0892)  loss_bbox_4: 0.0463 (0.0473)  loss_giou_4: 0.3687 (0.3801)  loss_ce_interm: 0.1468 (0.1637)  loss_bbox_interm: 0.0476 (0.0489)  loss_giou_interm: 0.3795 (0.3880)  loss_ce_unscaled: 0.0382 (0.0432)  loss_bbox_unscaled: 0.0092 (0.0095)  loss_giou_unscaled: 0.1862 (0.1899)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0063 (0.0066)  loss_ce_0_unscaled: 0.0570 (0.0630)  loss_bbox_0_unscaled: 0.0093 (0.0094)  loss_giou_0_unscaled: 0.1833 (0.1894)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0063 (0.0066)  loss_ce_1_unscaled: 0.0500 (0.0541)  loss_bbox_1_unscaled: 0.0093 (0.0094)  loss_giou_1_unscaled: 0.1805 (0.1907)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0063 (0.0066)  loss_ce_2_unscaled: 0.0513 (0.0503)  loss_bbox_2_unscaled: 0.0094 (0.0095)  loss_giou_2_unscaled: 0.1851 (0.1905)  loss_xy_2_unscaled: 0.0028 (0.0029)  loss_hw_2_unscaled: 0.0063 (0.0066)  loss_ce_3_unscaled: 0.0428 (0.0451)  loss_bbox_3_unscaled: 0.0093 (0.0094)  loss_giou_3_unscaled: 0.1876 (0.1913)  loss_xy_3_unscaled: 0.0028 (0.0029)  loss_hw_3_unscaled: 0.0063 (0.0066)  loss_ce_4_unscaled: 0.0384 (0.0446)  loss_bbox_4_unscaled: 0.0093 (0.0095)  loss_giou_4_unscaled: 0.1843 (0.1901)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0063 (0.0066)  loss_ce_interm_unscaled: 0.0734 (0.0818)  loss_bbox_interm_unscaled: 0.0095 (0.0098)  loss_giou_interm_unscaled: 0.1898 (0.1940)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0065 (0.0068)  time: 2.6998  data: 0.0126  max mem: 8948\n",
            "Epoch: [14]  [30/90]  eta: 0:02:55  lr: 0.000000  loss: 3.6440 (3.7180)  loss_ce: 0.0669 (0.0820)  loss_bbox: 0.0455 (0.0466)  loss_giou: 0.3809 (0.3782)  loss_ce_0: 0.1091 (0.1229)  loss_bbox_0: 0.0455 (0.0465)  loss_giou_0: 0.3727 (0.3763)  loss_ce_1: 0.0943 (0.1053)  loss_bbox_1: 0.0448 (0.0465)  loss_giou_1: 0.3771 (0.3778)  loss_ce_2: 0.0813 (0.0978)  loss_bbox_2: 0.0441 (0.0465)  loss_giou_2: 0.3810 (0.3777)  loss_ce_3: 0.0856 (0.0878)  loss_bbox_3: 0.0443 (0.0464)  loss_giou_3: 0.3795 (0.3787)  loss_ce_4: 0.0691 (0.0849)  loss_bbox_4: 0.0455 (0.0466)  loss_giou_4: 0.3802 (0.3784)  loss_ce_interm: 0.1439 (0.1581)  loss_bbox_interm: 0.0467 (0.0485)  loss_giou_interm: 0.3796 (0.3842)  loss_ce_unscaled: 0.0334 (0.0410)  loss_bbox_unscaled: 0.0091 (0.0093)  loss_giou_unscaled: 0.1904 (0.1891)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0062 (0.0066)  loss_ce_0_unscaled: 0.0545 (0.0614)  loss_bbox_0_unscaled: 0.0091 (0.0093)  loss_giou_0_unscaled: 0.1864 (0.1881)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0064 (0.0065)  loss_ce_1_unscaled: 0.0471 (0.0527)  loss_bbox_1_unscaled: 0.0090 (0.0093)  loss_giou_1_unscaled: 0.1886 (0.1889)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0063 (0.0065)  loss_ce_2_unscaled: 0.0407 (0.0489)  loss_bbox_2_unscaled: 0.0088 (0.0093)  loss_giou_2_unscaled: 0.1905 (0.1889)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0062 (0.0065)  loss_ce_3_unscaled: 0.0428 (0.0439)  loss_bbox_3_unscaled: 0.0089 (0.0093)  loss_giou_3_unscaled: 0.1897 (0.1894)  loss_xy_3_unscaled: 0.0027 (0.0028)  loss_hw_3_unscaled: 0.0063 (0.0065)  loss_ce_4_unscaled: 0.0346 (0.0425)  loss_bbox_4_unscaled: 0.0091 (0.0093)  loss_giou_4_unscaled: 0.1901 (0.1892)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0062 (0.0066)  loss_ce_interm_unscaled: 0.0720 (0.0791)  loss_bbox_interm_unscaled: 0.0093 (0.0097)  loss_giou_interm_unscaled: 0.1898 (0.1921)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0067 (0.0068)  time: 2.9517  data: 0.0150  max mem: 8948\n",
            "Epoch: [14]  [40/90]  eta: 0:02:25  lr: 0.000000  loss: 3.9259 (3.8433)  loss_ce: 0.0870 (0.0937)  loss_bbox: 0.0455 (0.0474)  loss_giou: 0.3859 (0.3858)  loss_ce_0: 0.1378 (0.1302)  loss_bbox_0: 0.0455 (0.0472)  loss_giou_0: 0.3863 (0.3834)  loss_ce_1: 0.1148 (0.1159)  loss_bbox_1: 0.0442 (0.0472)  loss_giou_1: 0.3771 (0.3837)  loss_ce_2: 0.1103 (0.1075)  loss_bbox_2: 0.0441 (0.0472)  loss_giou_2: 0.3810 (0.3840)  loss_ce_3: 0.1004 (0.0990)  loss_bbox_3: 0.0446 (0.0471)  loss_giou_3: 0.3795 (0.3857)  loss_ce_4: 0.0884 (0.0967)  loss_bbox_4: 0.0455 (0.0474)  loss_giou_4: 0.3919 (0.3862)  loss_ce_interm: 0.1686 (0.1701)  loss_bbox_interm: 0.0467 (0.0491)  loss_giou_interm: 0.3855 (0.3887)  loss_ce_unscaled: 0.0435 (0.0468)  loss_bbox_unscaled: 0.0091 (0.0095)  loss_giou_unscaled: 0.1930 (0.1929)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0066 (0.0067)  loss_ce_0_unscaled: 0.0689 (0.0651)  loss_bbox_0_unscaled: 0.0091 (0.0094)  loss_giou_0_unscaled: 0.1931 (0.1917)  loss_xy_0_unscaled: 0.0028 (0.0028)  loss_hw_0_unscaled: 0.0065 (0.0066)  loss_ce_1_unscaled: 0.0574 (0.0580)  loss_bbox_1_unscaled: 0.0088 (0.0094)  loss_giou_1_unscaled: 0.1886 (0.1919)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0063 (0.0066)  loss_ce_2_unscaled: 0.0551 (0.0538)  loss_bbox_2_unscaled: 0.0088 (0.0094)  loss_giou_2_unscaled: 0.1905 (0.1920)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0065 (0.0066)  loss_ce_3_unscaled: 0.0502 (0.0495)  loss_bbox_3_unscaled: 0.0089 (0.0094)  loss_giou_3_unscaled: 0.1897 (0.1928)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0064 (0.0066)  loss_ce_4_unscaled: 0.0442 (0.0484)  loss_bbox_4_unscaled: 0.0091 (0.0095)  loss_giou_4_unscaled: 0.1959 (0.1931)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0065 (0.0067)  loss_ce_interm_unscaled: 0.0843 (0.0850)  loss_bbox_interm_unscaled: 0.0093 (0.0098)  loss_giou_interm_unscaled: 0.1928 (0.1944)  loss_xy_interm_unscaled: 0.0029 (0.0029)  loss_hw_interm_unscaled: 0.0068 (0.0069)  time: 3.0358  data: 0.0160  max mem: 8948\n",
            "Epoch: [14]  [50/90]  eta: 0:01:55  lr: 0.000000  loss: 4.0722 (3.8328)  loss_ce: 0.0790 (0.0886)  loss_bbox: 0.0505 (0.0480)  loss_giou: 0.4038 (0.3892)  loss_ce_0: 0.1181 (0.1242)  loss_bbox_0: 0.0498 (0.0477)  loss_giou_0: 0.4055 (0.3876)  loss_ce_1: 0.1028 (0.1099)  loss_bbox_1: 0.0504 (0.0479)  loss_giou_1: 0.4028 (0.3874)  loss_ce_2: 0.0879 (0.1017)  loss_bbox_2: 0.0484 (0.0477)  loss_giou_2: 0.4023 (0.3875)  loss_ce_3: 0.0813 (0.0936)  loss_bbox_3: 0.0504 (0.0477)  loss_giou_3: 0.4016 (0.3893)  loss_ce_4: 0.0821 (0.0915)  loss_bbox_4: 0.0507 (0.0480)  loss_giou_4: 0.4039 (0.3895)  loss_ce_interm: 0.1511 (0.1636)  loss_bbox_interm: 0.0513 (0.0495)  loss_giou_interm: 0.4079 (0.3926)  loss_ce_unscaled: 0.0395 (0.0443)  loss_bbox_unscaled: 0.0101 (0.0096)  loss_giou_unscaled: 0.2019 (0.1946)  loss_xy_unscaled: 0.0026 (0.0028)  loss_hw_unscaled: 0.0070 (0.0068)  loss_ce_0_unscaled: 0.0591 (0.0621)  loss_bbox_0_unscaled: 0.0100 (0.0095)  loss_giou_0_unscaled: 0.2028 (0.1938)  loss_xy_0_unscaled: 0.0029 (0.0028)  loss_hw_0_unscaled: 0.0069 (0.0067)  loss_ce_1_unscaled: 0.0514 (0.0550)  loss_bbox_1_unscaled: 0.0101 (0.0096)  loss_giou_1_unscaled: 0.2014 (0.1937)  loss_xy_1_unscaled: 0.0027 (0.0028)  loss_hw_1_unscaled: 0.0071 (0.0068)  loss_ce_2_unscaled: 0.0440 (0.0509)  loss_bbox_2_unscaled: 0.0097 (0.0095)  loss_giou_2_unscaled: 0.2011 (0.1938)  loss_xy_2_unscaled: 0.0027 (0.0028)  loss_hw_2_unscaled: 0.0070 (0.0067)  loss_ce_3_unscaled: 0.0407 (0.0468)  loss_bbox_3_unscaled: 0.0101 (0.0095)  loss_giou_3_unscaled: 0.2008 (0.1947)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0070 (0.0067)  loss_ce_4_unscaled: 0.0411 (0.0457)  loss_bbox_4_unscaled: 0.0101 (0.0096)  loss_giou_4_unscaled: 0.2019 (0.1948)  loss_xy_4_unscaled: 0.0026 (0.0028)  loss_hw_4_unscaled: 0.0070 (0.0068)  loss_ce_interm_unscaled: 0.0755 (0.0818)  loss_bbox_interm_unscaled: 0.0103 (0.0099)  loss_giou_interm_unscaled: 0.2039 (0.1963)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0070 (0.0070)  time: 2.8495  data: 0.0150  max mem: 8948\n",
            "Epoch: [14]  [60/90]  eta: 0:01:26  lr: 0.000000  loss: 3.7422 (3.8083)  loss_ce: 0.0521 (0.0842)  loss_bbox: 0.0455 (0.0478)  loss_giou: 0.3954 (0.3894)  loss_ce_0: 0.0918 (0.1208)  loss_bbox_0: 0.0473 (0.0477)  loss_giou_0: 0.3994 (0.3882)  loss_ce_1: 0.0766 (0.1062)  loss_bbox_1: 0.0452 (0.0477)  loss_giou_1: 0.3962 (0.3879)  loss_ce_2: 0.0672 (0.0980)  loss_bbox_2: 0.0460 (0.0475)  loss_giou_2: 0.3961 (0.3875)  loss_ce_3: 0.0631 (0.0908)  loss_bbox_3: 0.0454 (0.0475)  loss_giou_3: 0.3991 (0.3889)  loss_ce_4: 0.0591 (0.0876)  loss_bbox_4: 0.0456 (0.0478)  loss_giou_4: 0.3965 (0.3896)  loss_ce_interm: 0.1297 (0.1609)  loss_bbox_interm: 0.0474 (0.0493)  loss_giou_interm: 0.4081 (0.3931)  loss_ce_unscaled: 0.0260 (0.0421)  loss_bbox_unscaled: 0.0091 (0.0096)  loss_giou_unscaled: 0.1977 (0.1947)  loss_xy_unscaled: 0.0025 (0.0028)  loss_hw_unscaled: 0.0066 (0.0068)  loss_ce_0_unscaled: 0.0459 (0.0604)  loss_bbox_0_unscaled: 0.0095 (0.0095)  loss_giou_0_unscaled: 0.1997 (0.1941)  loss_xy_0_unscaled: 0.0025 (0.0028)  loss_hw_0_unscaled: 0.0067 (0.0068)  loss_ce_1_unscaled: 0.0383 (0.0531)  loss_bbox_1_unscaled: 0.0090 (0.0095)  loss_giou_1_unscaled: 0.1981 (0.1940)  loss_xy_1_unscaled: 0.0025 (0.0028)  loss_hw_1_unscaled: 0.0067 (0.0068)  loss_ce_2_unscaled: 0.0336 (0.0490)  loss_bbox_2_unscaled: 0.0092 (0.0095)  loss_giou_2_unscaled: 0.1981 (0.1937)  loss_xy_2_unscaled: 0.0025 (0.0028)  loss_hw_2_unscaled: 0.0066 (0.0067)  loss_ce_3_unscaled: 0.0316 (0.0454)  loss_bbox_3_unscaled: 0.0091 (0.0095)  loss_giou_3_unscaled: 0.1995 (0.1944)  loss_xy_3_unscaled: 0.0025 (0.0028)  loss_hw_3_unscaled: 0.0063 (0.0067)  loss_ce_4_unscaled: 0.0295 (0.0438)  loss_bbox_4_unscaled: 0.0091 (0.0096)  loss_giou_4_unscaled: 0.1982 (0.1948)  loss_xy_4_unscaled: 0.0025 (0.0028)  loss_hw_4_unscaled: 0.0066 (0.0068)  loss_ce_interm_unscaled: 0.0648 (0.0805)  loss_bbox_interm_unscaled: 0.0095 (0.0099)  loss_giou_interm_unscaled: 0.2041 (0.1965)  loss_xy_interm_unscaled: 0.0026 (0.0028)  loss_hw_interm_unscaled: 0.0070 (0.0070)  time: 2.8491  data: 0.0160  max mem: 8948\n",
            "Epoch: [14]  [70/90]  eta: 0:00:57  lr: 0.000000  loss: 3.6401 (3.7835)  loss_ce: 0.0528 (0.0815)  loss_bbox: 0.0443 (0.0479)  loss_giou: 0.3769 (0.3880)  loss_ce_0: 0.0950 (0.1181)  loss_bbox_0: 0.0472 (0.0478)  loss_giou_0: 0.3831 (0.3873)  loss_ce_1: 0.0751 (0.1038)  loss_bbox_1: 0.0450 (0.0479)  loss_giou_1: 0.3791 (0.3871)  loss_ce_2: 0.0667 (0.0954)  loss_bbox_2: 0.0443 (0.0477)  loss_giou_2: 0.3785 (0.3866)  loss_ce_3: 0.0661 (0.0883)  loss_bbox_3: 0.0439 (0.0477)  loss_giou_3: 0.3772 (0.3877)  loss_ce_4: 0.0591 (0.0851)  loss_bbox_4: 0.0444 (0.0479)  loss_giou_4: 0.3783 (0.3882)  loss_ce_interm: 0.1381 (0.1580)  loss_bbox_interm: 0.0461 (0.0493)  loss_giou_interm: 0.3892 (0.3920)  loss_ce_unscaled: 0.0264 (0.0408)  loss_bbox_unscaled: 0.0089 (0.0096)  loss_giou_unscaled: 0.1885 (0.1940)  loss_xy_unscaled: 0.0027 (0.0028)  loss_hw_unscaled: 0.0063 (0.0068)  loss_ce_0_unscaled: 0.0475 (0.0591)  loss_bbox_0_unscaled: 0.0094 (0.0096)  loss_giou_0_unscaled: 0.1915 (0.1936)  loss_xy_0_unscaled: 0.0026 (0.0028)  loss_hw_0_unscaled: 0.0067 (0.0068)  loss_ce_1_unscaled: 0.0376 (0.0519)  loss_bbox_1_unscaled: 0.0090 (0.0096)  loss_giou_1_unscaled: 0.1896 (0.1935)  loss_xy_1_unscaled: 0.0026 (0.0028)  loss_hw_1_unscaled: 0.0063 (0.0068)  loss_ce_2_unscaled: 0.0333 (0.0477)  loss_bbox_2_unscaled: 0.0089 (0.0095)  loss_giou_2_unscaled: 0.1893 (0.1933)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0064 (0.0068)  loss_ce_3_unscaled: 0.0330 (0.0442)  loss_bbox_3_unscaled: 0.0088 (0.0095)  loss_giou_3_unscaled: 0.1886 (0.1938)  loss_xy_3_unscaled: 0.0026 (0.0028)  loss_hw_3_unscaled: 0.0063 (0.0068)  loss_ce_4_unscaled: 0.0295 (0.0425)  loss_bbox_4_unscaled: 0.0089 (0.0096)  loss_giou_4_unscaled: 0.1891 (0.1941)  loss_xy_4_unscaled: 0.0027 (0.0028)  loss_hw_4_unscaled: 0.0064 (0.0068)  loss_ce_interm_unscaled: 0.0691 (0.0790)  loss_bbox_interm_unscaled: 0.0092 (0.0099)  loss_giou_interm_unscaled: 0.1946 (0.1960)  loss_xy_interm_unscaled: 0.0026 (0.0029)  loss_hw_interm_unscaled: 0.0067 (0.0070)  time: 2.7807  data: 0.0166  max mem: 8948\n",
            "Epoch: [14]  [80/90]  eta: 0:00:28  lr: 0.000000  loss: 3.6935 (3.7958)  loss_ce: 0.0656 (0.0833)  loss_bbox: 0.0479 (0.0479)  loss_giou: 0.3772 (0.3880)  loss_ce_0: 0.1060 (0.1200)  loss_bbox_0: 0.0489 (0.0480)  loss_giou_0: 0.3773 (0.3872)  loss_ce_1: 0.0897 (0.1045)  loss_bbox_1: 0.0481 (0.0478)  loss_giou_1: 0.3784 (0.3880)  loss_ce_2: 0.0781 (0.0961)  loss_bbox_2: 0.0481 (0.0477)  loss_giou_2: 0.3785 (0.3868)  loss_ce_3: 0.0752 (0.0891)  loss_bbox_3: 0.0482 (0.0477)  loss_giou_3: 0.3736 (0.3876)  loss_ce_4: 0.0694 (0.0868)  loss_bbox_4: 0.0481 (0.0480)  loss_giou_4: 0.3780 (0.3881)  loss_ce_interm: 0.1507 (0.1596)  loss_bbox_interm: 0.0497 (0.0495)  loss_giou_interm: 0.3869 (0.3939)  loss_ce_unscaled: 0.0328 (0.0417)  loss_bbox_unscaled: 0.0096 (0.0096)  loss_giou_unscaled: 0.1886 (0.1940)  loss_xy_unscaled: 0.0029 (0.0028)  loss_hw_unscaled: 0.0068 (0.0068)  loss_ce_0_unscaled: 0.0530 (0.0600)  loss_bbox_0_unscaled: 0.0098 (0.0096)  loss_giou_0_unscaled: 0.1887 (0.1936)  loss_xy_0_unscaled: 0.0029 (0.0028)  loss_hw_0_unscaled: 0.0068 (0.0068)  loss_ce_1_unscaled: 0.0448 (0.0522)  loss_bbox_1_unscaled: 0.0096 (0.0096)  loss_giou_1_unscaled: 0.1892 (0.1940)  loss_xy_1_unscaled: 0.0029 (0.0028)  loss_hw_1_unscaled: 0.0069 (0.0068)  loss_ce_2_unscaled: 0.0391 (0.0481)  loss_bbox_2_unscaled: 0.0096 (0.0095)  loss_giou_2_unscaled: 0.1893 (0.1934)  loss_xy_2_unscaled: 0.0028 (0.0028)  loss_hw_2_unscaled: 0.0068 (0.0067)  loss_ce_3_unscaled: 0.0376 (0.0446)  loss_bbox_3_unscaled: 0.0096 (0.0095)  loss_giou_3_unscaled: 0.1868 (0.1938)  loss_xy_3_unscaled: 0.0029 (0.0028)  loss_hw_3_unscaled: 0.0068 (0.0067)  loss_ce_4_unscaled: 0.0347 (0.0434)  loss_bbox_4_unscaled: 0.0096 (0.0096)  loss_giou_4_unscaled: 0.1890 (0.1941)  loss_xy_4_unscaled: 0.0029 (0.0028)  loss_hw_4_unscaled: 0.0068 (0.0068)  loss_ce_interm_unscaled: 0.0753 (0.0798)  loss_bbox_interm_unscaled: 0.0099 (0.0099)  loss_giou_interm_unscaled: 0.1934 (0.1970)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0069 (0.0070)  time: 2.7172  data: 0.0159  max mem: 8948\n",
            "Epoch: [14]  [89/90]  eta: 0:00:02  lr: 0.000000  loss: 3.7116 (3.7840)  loss_ce: 0.0722 (0.0832)  loss_bbox: 0.0459 (0.0477)  loss_giou: 0.3772 (0.3864)  loss_ce_0: 0.1232 (0.1202)  loss_bbox_0: 0.0468 (0.0478)  loss_giou_0: 0.3771 (0.3858)  loss_ce_1: 0.1002 (0.1048)  loss_bbox_1: 0.0458 (0.0476)  loss_giou_1: 0.3784 (0.3865)  loss_ce_2: 0.0912 (0.0964)  loss_bbox_2: 0.0458 (0.0475)  loss_giou_2: 0.3768 (0.3854)  loss_ce_3: 0.0772 (0.0892)  loss_bbox_3: 0.0454 (0.0475)  loss_giou_3: 0.3725 (0.3862)  loss_ce_4: 0.0726 (0.0864)  loss_bbox_4: 0.0461 (0.0478)  loss_giou_4: 0.3780 (0.3866)  loss_ce_interm: 0.1562 (0.1600)  loss_bbox_interm: 0.0470 (0.0492)  loss_giou_interm: 0.3720 (0.3921)  loss_ce_unscaled: 0.0361 (0.0416)  loss_bbox_unscaled: 0.0092 (0.0095)  loss_giou_unscaled: 0.1886 (0.1932)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0068 (0.0067)  loss_ce_0_unscaled: 0.0616 (0.0601)  loss_bbox_0_unscaled: 0.0094 (0.0096)  loss_giou_0_unscaled: 0.1886 (0.1929)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0067 (0.0067)  loss_ce_1_unscaled: 0.0501 (0.0524)  loss_bbox_1_unscaled: 0.0092 (0.0095)  loss_giou_1_unscaled: 0.1892 (0.1933)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0069 (0.0067)  loss_ce_2_unscaled: 0.0456 (0.0482)  loss_bbox_2_unscaled: 0.0092 (0.0095)  loss_giou_2_unscaled: 0.1884 (0.1927)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0067 (0.0067)  loss_ce_3_unscaled: 0.0386 (0.0446)  loss_bbox_3_unscaled: 0.0091 (0.0095)  loss_giou_3_unscaled: 0.1863 (0.1931)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0068 (0.0067)  loss_ce_4_unscaled: 0.0363 (0.0432)  loss_bbox_4_unscaled: 0.0092 (0.0096)  loss_giou_4_unscaled: 0.1890 (0.1933)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0068 (0.0068)  loss_ce_interm_unscaled: 0.0781 (0.0800)  loss_bbox_interm_unscaled: 0.0094 (0.0098)  loss_giou_interm_unscaled: 0.1860 (0.1960)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0069 (0.0070)  time: 2.8123  data: 0.0150  max mem: 8948\n",
            "Epoch: [14] Total time: 0:04:16 (2.8451 s / it)\n",
            "Averaged stats: lr: 0.000000  loss: 3.7116 (3.7840)  loss_ce: 0.0722 (0.0832)  loss_bbox: 0.0459 (0.0477)  loss_giou: 0.3772 (0.3864)  loss_ce_0: 0.1232 (0.1202)  loss_bbox_0: 0.0468 (0.0478)  loss_giou_0: 0.3771 (0.3858)  loss_ce_1: 0.1002 (0.1048)  loss_bbox_1: 0.0458 (0.0476)  loss_giou_1: 0.3784 (0.3865)  loss_ce_2: 0.0912 (0.0964)  loss_bbox_2: 0.0458 (0.0475)  loss_giou_2: 0.3768 (0.3854)  loss_ce_3: 0.0772 (0.0892)  loss_bbox_3: 0.0454 (0.0475)  loss_giou_3: 0.3725 (0.3862)  loss_ce_4: 0.0726 (0.0864)  loss_bbox_4: 0.0461 (0.0478)  loss_giou_4: 0.3780 (0.3866)  loss_ce_interm: 0.1562 (0.1600)  loss_bbox_interm: 0.0470 (0.0492)  loss_giou_interm: 0.3720 (0.3921)  loss_ce_unscaled: 0.0361 (0.0416)  loss_bbox_unscaled: 0.0092 (0.0095)  loss_giou_unscaled: 0.1886 (0.1932)  loss_xy_unscaled: 0.0028 (0.0028)  loss_hw_unscaled: 0.0068 (0.0067)  loss_ce_0_unscaled: 0.0616 (0.0601)  loss_bbox_0_unscaled: 0.0094 (0.0096)  loss_giou_0_unscaled: 0.1886 (0.1929)  loss_xy_0_unscaled: 0.0027 (0.0028)  loss_hw_0_unscaled: 0.0067 (0.0067)  loss_ce_1_unscaled: 0.0501 (0.0524)  loss_bbox_1_unscaled: 0.0092 (0.0095)  loss_giou_1_unscaled: 0.1892 (0.1933)  loss_xy_1_unscaled: 0.0028 (0.0028)  loss_hw_1_unscaled: 0.0069 (0.0067)  loss_ce_2_unscaled: 0.0456 (0.0482)  loss_bbox_2_unscaled: 0.0092 (0.0095)  loss_giou_2_unscaled: 0.1884 (0.1927)  loss_xy_2_unscaled: 0.0026 (0.0028)  loss_hw_2_unscaled: 0.0067 (0.0067)  loss_ce_3_unscaled: 0.0386 (0.0446)  loss_bbox_3_unscaled: 0.0091 (0.0095)  loss_giou_3_unscaled: 0.1863 (0.1931)  loss_xy_3_unscaled: 0.0028 (0.0028)  loss_hw_3_unscaled: 0.0068 (0.0067)  loss_ce_4_unscaled: 0.0363 (0.0432)  loss_bbox_4_unscaled: 0.0092 (0.0096)  loss_giou_4_unscaled: 0.1890 (0.1933)  loss_xy_4_unscaled: 0.0028 (0.0028)  loss_hw_4_unscaled: 0.0068 (0.0068)  loss_ce_interm_unscaled: 0.0781 (0.0800)  loss_bbox_interm_unscaled: 0.0094 (0.0098)  loss_giou_interm_unscaled: 0.1860 (0.1960)  loss_xy_interm_unscaled: 0.0028 (0.0029)  loss_hw_interm_unscaled: 0.0069 (0.0070)\n",
            "Input text prompt: bolt . wrong direction . 1 . 2 . 3 . 4 . 5 .\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Test:  [ 0/10]  eta: 0:00:18    time: 1.8512  data: 0.9245  max mem: 8948\n",
            "Test:  [ 9/10]  eta: 0:00:01    time: 1.0334  data: 0.1078  max mem: 8948\n",
            "Test: Total time: 0:00:10 (1.0428 s / it)\n",
            "Averaged stats: \n",
            "Accumulating evaluation results...\n",
            "DONE (t=0.10s).\n",
            "IoU metric: bbox\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.056\n",
            " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.117\n",
            " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.040\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.054\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.294\n",
            " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.220\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.009\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.242\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.485\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.454\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.602\n",
            " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.441\n",
            "Training time 1:08:52\n"
          ]
        }
      ],
      "source": [
        "GPU_NUM=1\n",
        "CGF=\"/content/Open-GroundingDino/config/cfg_odvg.py\"\n",
        "DATASETS=\"/content/Open-GroundingDino/config/datasets_mixed_odvg.json\"\n",
        "OUTPUT_DIR=\"/content/output\"\n",
        "!chmod +x train_dist.sh\n",
        "!bash train_dist.sh {CGF} {DATASETS} {OUTPUT_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7. Inference on the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Mxf8nKcC6AS",
        "outputId": "6bb0b218-8e5f-4011-e813-facda8e30439"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'GroundingDINO'...\n",
            "remote: Enumerating objects: 443, done.\u001b[K\n",
            "remote: Counting objects: 100% (211/211), done.\u001b[K\n",
            "remote: Compressing objects: 100% (83/83), done.\u001b[K\n",
            "remote: Total 443 (delta 156), reused 137 (delta 128), pack-reused 232\u001b[K\n",
            "Receiving objects: 100% (443/443), 12.86 MiB | 19.04 MiB/s, done.\n",
            "Resolving deltas: 100% (228/228), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YynH_4FxabFk",
        "outputId": "020cb3b7-f57d-44f1-b677-926a0395441c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/Open-GroundingDino/GroundingDINO\n"
          ]
        }
      ],
      "source": [
        "%cd GroundingDINO/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmSfXVmmaet8",
        "outputId": "5c8942cf-b7f4-4d8e-c2ee-b9c8afaf4952"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///content/Open-GroundingDino/GroundingDINO\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (0.18.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (4.41.2)\n",
            "Requirement already satisfied: addict in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (2.4.0)\n",
            "Requirement already satisfied: yapf in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (0.40.1)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (1.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (1.25.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (4.8.0.76)\n",
            "Requirement already satisfied: supervision in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (0.6.0)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.10/dist-packages (from groundingdino==0.1.0) (2.0.7)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools->groundingdino==0.1.0) (3.7.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm->groundingdino==0.1.0) (6.0.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm->groundingdino==0.1.0) (0.23.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm->groundingdino==0.1.0) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->groundingdino==0.1.0) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->groundingdino==0.1.0) (12.5.40)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->groundingdino==0.1.0) (9.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->groundingdino==0.1.0) (24.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->groundingdino==0.1.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->groundingdino==0.1.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->groundingdino==0.1.0) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->groundingdino==0.1.0) (4.66.4)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->groundingdino==0.1.0) (7.1.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->groundingdino==0.1.0) (4.2.2)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->groundingdino==0.1.0) (2.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->groundingdino==0.1.0) (3.19.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->groundingdino==0.1.0) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->groundingdino==0.1.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->groundingdino==0.1.0) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->groundingdino==0.1.0) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->groundingdino==0.1.0) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools->groundingdino==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->groundingdino==0.1.0) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->groundingdino==0.1.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->groundingdino==0.1.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->groundingdino==0.1.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->groundingdino==0.1.0) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->groundingdino==0.1.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools->groundingdino==0.1.0) (1.16.0)\n",
            "Installing collected packages: groundingdino\n",
            "  Running setup.py develop for groundingdino\n",
            "Successfully installed groundingdino-0.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXcn4qi2biWd",
        "outputId": "fe3a94c4-82e1-486f-fed5-f855ff8acb5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8. Creating a folder for Saving the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YQQeX5Wrvhs",
        "outputId": "46d2e584-9504-40e4-999f-0e7d6ce8de99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/allval_images_in_folder\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.makedirs(\"/content/allval_images_in_folder\", exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ks3xT19IsZwh",
        "outputId": "6bbeba9a-3686-424c-f3d0-10a0bf732fb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/allval_images_in_folder\n"
          ]
        }
      ],
      "source": [
        "%cd /content/allval_images_in_folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IrEv1k7zr6nl",
        "outputId": "6b0df0ca-809d-405f-eeb5-b56a01688bf2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/allval_images_in_folder'"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Inference on a single Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62y3A4tYakMy",
        "outputId": "7361d918-c654-40b5-f45f-71beda0f37e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Open-GroundingDino/tools/inference_on_a_image.py\", line 188, in <module>\n",
            "    model = load_model(config_file, checkpoint_path, cpu_only=args.cpu_only)\n",
            "  File \"/content/Open-GroundingDino/tools/inference_on_a_image.py\", line 75, in load_model\n",
            "    model = build_model(args)\n",
            "  File \"/content/Open-GroundingDino/GroundingDINO/groundingdino/models/__init__.py\", line 17, in build_model\n",
            "    model = build_func(args)\n",
            "  File \"/content/Open-GroundingDino/GroundingDINO/groundingdino/models/GroundingDINO/groundingdino.py\", line 381, in build_groundingdino\n",
            "    backbone = build_backbone(args)\n",
            "  File \"/content/Open-GroundingDino/GroundingDINO/groundingdino/models/GroundingDINO/backbone/backbone.py\", line 199, in build_backbone\n",
            "    backbone = build_swin_transformer(\n",
            "  File \"/content/Open-GroundingDino/GroundingDINO/groundingdino/models/GroundingDINO/backbone/swin_transformer.py\", line 790, in build_swin_transformer\n",
            "    model = SwinTransformer(pretrain_img_size=pretrain_img_size, **kw_cgf)\n",
            "  File \"/content/Open-GroundingDino/GroundingDINO/groundingdino/models/GroundingDINO/backbone/swin_transformer.py\", line 606, in __init__\n",
            "    layer = BasicLayer(\n",
            "  File \"/content/Open-GroundingDino/GroundingDINO/groundingdino/models/GroundingDINO/backbone/swin_transformer.py\", line 385, in __init__\n",
            "    [\n",
            "  File \"/content/Open-GroundingDino/GroundingDINO/groundingdino/models/GroundingDINO/backbone/swin_transformer.py\", line 386, in <listcomp>\n",
            "    SwinTransformerBlock(\n",
            "  File \"/content/Open-GroundingDino/GroundingDINO/groundingdino/models/GroundingDINO/backbone/swin_transformer.py\", line 231, in __init__\n",
            "    self.mlp = Mlp(\n",
            "  File \"/content/Open-GroundingDino/GroundingDINO/groundingdino/models/GroundingDINO/backbone/swin_transformer.py\", line 35, in __init__\n",
            "    self.fc2 = nn.Linear(hidden_features, out_features)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 103, in __init__\n",
            "    self.reset_parameters()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\", line 109, in reset_parameters\n",
            "    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/init.py\", line 459, in kaiming_uniform_\n",
            "    return tensor.uniform_(-bound, bound, generator=generator)\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python \"/content/Open-GroundingDino/tools/inference_on_a_image.py\" \\\n",
        "  -c \"/content/Open-GroundingDino/tools/GroundingDINO_SwinT_OGC.py\" \\\n",
        "  -p \"/content/output/checkpoint_best_regular.pth\" \\\n",
        "  -i \"/content/open_img/val/Image__2024-03-13__13-12-44 - Copy.png\" \\\n",
        "  -t \"bolt . wrong direction . 1 . 2 . 3 . 4 . 5 \" \\\n",
        "  -o pred_images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Inferencing on Val images folder "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "r_cccjDYrqeA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "\n",
        "# Directory containing the images\n",
        "image_dir = \"/content/open_img/val\"\n",
        "# Get a list of all image files in the directory\n",
        "image_files = [f for f in os.listdir(image_dir) if f.endswith('.png') or f.endswith('.jpg')]\n",
        "\n",
        "# Define the other arguments for the inference script\n",
        "config_path = \"/content/Open-GroundingDino/tools/GroundingDINO_SwinT_OGC.py\"\n",
        "checkpoint_path = \"/content/output/checkpoint_best_regular.pth\"\n",
        "text_prompts = \"bolt . wrong direction . 1 . 2 . 3 . 4 . 5 \"\n",
        "output_dir = \"pred_images\"\n",
        "\n",
        "# Loop over all image files and run the inference script on each one\n",
        "for image_file in image_files:\n",
        "    image_path = os.path.join(image_dir, image_file)\n",
        "    command = [\n",
        "        \"python\", \"/content/Open-GroundingDino/tools/inference_on_a_image.py\",\n",
        "        \"-c\", config_path,\n",
        "        \"-p\", checkpoint_path,\n",
        "        \"-i\", image_path,\n",
        "        \"-t\", text_prompts,\n",
        "        \"-o\", output_dir+image_file\n",
        "    ]\n",
        "    subprocess.run(command)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Organizing Images in a single folder and copying folder to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgcG1GJ-zZsn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def is_image(file):\n",
        "    image_extensions = ['.png', '.jpg', '.jpeg', '.gif', '.bmp', '.tiff']\n",
        "    return any(file.lower().endswith(ext) for ext in image_extensions)\n",
        "\n",
        "def copy_images(src_dir, dest_dir):\n",
        "    if not os.path.exists(dest_dir):\n",
        "        os.makedirs(dest_dir)\n",
        "\n",
        "    for root, _, files in os.walk(src_dir):\n",
        "        for file in files:\n",
        "            if is_image(file):\n",
        "                src_file_path = os.path.join(root, file)\n",
        "                dest_file_path = os.path.join(dest_dir, file)\n",
        "\n",
        "                # To handle duplicate filenames\n",
        "                if os.path.exists(dest_file_path):\n",
        "                    base, ext = os.path.splitext(file)\n",
        "                    count = 1\n",
        "                    while os.path.exists(dest_file_path):\n",
        "                        dest_file_path = os.path.join(dest_dir, f\"{base}_{count}{ext}\")\n",
        "                        count += 1\n",
        "\n",
        "                shutil.copy2(src_file_path, dest_file_path)\n",
        "                print(f\"Copied {src_file_path} to {dest_file_path}\")\n",
        "\n",
        "# Example usage\n",
        "src_directory = '/content/allval_images_in_folder'\n",
        "dest_directory = '/content/final_val_images'\n",
        "copy_images(src_directory, dest_directory)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFCdzcdW0D7B"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/final_val_images.zip /content/final_val_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "ZxKmu6Nh0WUm"
      },
      "outputs": [],
      "source": [
        "!cp \"/content/final_val_images.zip\" \"/content/drive/MyDrive\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
